using Functors: @functor
using RecursiveArrayTools: VectorOfArray, convert

# ---- Aggregation (pooling) and misc functions ----

meanlastdim(X::A) where {A <: AbstractArray{T, N}} where {T, N} = mean(X, dims = N)
sumlastdim(X::A)  where {A <: AbstractArray{T, N}} where {T, N} = sum(X, dims = N)
LSElastdim(X::A)  where {A <: AbstractArray{T, N}} where {T, N} = logsumexp(X, dims = N)

function _agg(a::String)
	@assert a âˆˆ ["mean", "sum", "logsumexp"]
	if a == "mean"
		meanlastdim
	elseif a == "sum"
		sumlastdim
	elseif a == "logsumexp"
		LSElastdim
	end
end

"""
	samplesize(Z)

Computes the sample size m for a set of independent realisations `Z`, often
useful as an expert summary statistic in `DeepSetExpert` objects.

Note that this function is a simple wrapper around `numberreplicates`, but this
function returns the number of replicates as the eltype of `Z`.
"""
samplesize(Z) = eltype(Z)(numberreplicates(Z))
samplesize(Z::V) where V <: AbstractVector = samplesize.(Z)

# ---- DeepSet ----

"""
    DeepSet(Ïˆ, Ï•, a)
	DeepSet(Ïˆ, Ï•; a::String = "mean")

The Deep Set representation,

```math
Î¸Ì‚(ğ™) = Ï•(ğ“(ğ™)),	â€‚	â€‚ğ“(ğ™) = ğš(\\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\\}),
```

where ğ™ â‰¡ (ğ™â‚', â€¦, ğ™â‚˜')' are independent replicates from the model, `Ïˆ` and `Ï•`
are neural networks, and `a` is a permutation-invariant aggregation function.

To make the architecture agnostic to the sample size ``m``, the aggregation
function `a` must aggregate over the replicates. It can be specified as a
positional argument of type `Function`, or as a keyword argument with
permissible values `"mean"`, `"sum"`, and `"logsumexp"`.

`DeepSet` objects act on data stored as `Vector{A}`, where each
element of the vector is associated with one parameter vector (i.e., one set of
independent replicates), and where `A` depends on the form of the data and the
chosen architecture for `Ïˆ`. As a rule of thumb, when the data are stored as an
array, the replicates are stored in the final dimension of the array. (This is
usually the 'batch' dimension, but batching with `DeepSets` is done at the set
level, i.e., sets of replicates are batched together.) For example, with
gridded spatial data and `Ïˆ` a CNN, `A` should be
a 4-dimensional array, with the replicates stored in the 4áµ—Ê° dimension.

Note that, internally, data stored as `Vector{Arrays}` are first
concatenated along the replicates dimension before being passed into the inner
neural network `Ïˆ`; this means that `Ïˆ` is applied to a single large array
rather than many small arrays, which can substantially improve computational
efficiency, particularly on the GPU.

Set-level information, ``ğ±``, that is not a function of the data can be passed
directly into the outer network `Ï•` in the following manner,

```math
Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)', ğ±')'),	â€‚	â€‚ğ“(ğ™) = ğš(\\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\\}),
```

This is done by providing a `Tuple{Vector{A}, Vector{B}}`, where
the first element of the tuple contains the vector of data sets and the second
element contains the vector of set-level information.

# Examples
```
using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï• = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator
Zâ‚ = rand(n, 3);                  # single set of 3 realisations
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 3)]; # two sets each containing 3 realisations
Zâ‚ƒ = [rand(n, m) for m âˆˆ (3, 4)]; # two sets containing 3 and 4 realisations
Î¸Ì‚(Zâ‚)
Î¸Ì‚(Zâ‚‚)
Î¸Ì‚(Zâ‚ƒ)

# Repeat the above but with set-level information:
qâ‚“ = 2
Ï•  = Chain(Dense(w + qâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSet(Ïˆ, Ï•)
xâ‚ = rand(qâ‚“)
xâ‚‚ = [rand(qâ‚“) for _ âˆˆ eachindex(Zâ‚‚)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚‚, xâ‚‚))
Î¸Ì‚((Zâ‚ƒ, xâ‚‚))
```
"""
struct DeepSet{T, F, G}
	Ïˆ::T
	Ï•::G
	a::F
end
DeepSet(Ïˆ, Ï•; a::String = "mean") = DeepSet(Ïˆ, Ï•, _agg(a))
@functor DeepSet
Base.show(io::IO, D::DeepSet) = print(io, "\nDeepSet object with:\nInner network:  $(D.Ïˆ)\nAggregation function:  $(D.a)\nOuter network:  $(D.Ï•)")
Base.show(io::IO, m::MIME"text/plain", D::DeepSet) = print(io, D)


# Single data set
function (d::DeepSet)(Z::A) where A
	d.Ï•(d.a(d.Ïˆ(Z)))
end

# Single data set with set-level covariates
function (d::DeepSet)(tup::Tup) where {Tup <: Tuple{A, B}} where {A, B <: AbstractVector{T}} where T
	Z = tup[1]
	x = tup[2]
	t = d.a(d.Ïˆ(Z))
	d.Ï•(vcat(t, x))
end

# Multiple data sets: simple fallback method using broadcasting
function (d::DeepSet)(Z::V) where {V <: AbstractVector{A}} where A
  	stackarrays(d.(Z))
end

# Multiple data sets: optimised version for array data.
function (d::DeepSet)(Z::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa. Note that constructing
	# colons in this manner makes the function agnostic to ndims(Ïˆa).
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# Aggregate each set of transformed features: The resulting vector from the
	# list comprehension is a vector of arrays, where the last dimension of each
	# array is of size 1. Then, stack this vector of arrays into one large array,
	# where the last dimension of this large array has size equal to length(v).
	# Note that we cannot pre-allocate and fill an array, since array mutation
	# is not supported by Zygote (which is needed during training).
	t = stackarrays([d.a(Ïˆa[colons..., idx]) for idx âˆˆ indices])

	# Apply the outer network
	Î¸Ì‚ = d.Ï•(t)

	return Î¸Ì‚
end

# Multiple data sets with set-level covariates
function (d::DeepSet)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A, B <: AbstractVector{T}} where {T}
	Z = tup[1]
	x = tup[2]
	t = d.a.(d.Ïˆ.(Z))
	u = vcat.(t, x)
	stackarrays(d.Ï•.(u))
end

# Multiple data sets: optimised version for array data + vector set-level covariates.
function (d::DeepSet)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}

	Z = tup[1]
	X = tup[2]

	# Almost exactly the same code as the method defined above, but here we also
	# concatenate the covariates X before passing them into the outer network
	z = stackarrays(Z)
	Ïˆa = d.Ïˆ(z)
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		x = X[i]
		u = vcat(t, x)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	Î¸Ì‚ = d.Ï•(u)

	return Î¸Ì‚
end






# ---- DeepSetExpert: DeepSet with expert summary statistics ----

# Note that this struct is necessary because the Vector{Array} method of
# `DeepSet` concatenates the arrays into a single large array before passing
# the data into Ïˆ.
"""
	DeepSetExpert(Ïˆ, Ï•, S, a)
	DeepSetExpert(Ïˆ, Ï•, S; a::String = "mean")
	DeepSetExpert(deepset::DeepSet, Ï•, S)

Identical to `DeepSet`, but with additional expert summary statistics,

```math
Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)', ğ’(ğ™)')'),	â€‚	â€‚ğ“(ğ™) = ğš(\\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\\}),
```

where `S` is a function that returns a vector of expert summary statistics.

The constructor `DeepSetExpert(deepset::DeepSet, Ï•, S)` inherits `Ïˆ` and `a`
from `deepset`.

Similarly to `DeepSet`, set-level information can be incorporated by passing a
`Tuple`, in which case we have

```math
Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)', ğ’(ğ™)', ğ±')'),	â€‚	â€‚ğ“(ğ™) = ğš(\\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\\}).
```

# Examples
```
using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qâ‚› = 1
qâ‚œ = 32
w = 16
Ïˆ = Chain(Dense(n, w, relu), Dense(w, qâ‚œ, relu));
Ï• = Chain(Dense(qâ‚œ + qâ‚›, w), Dense(w, p));
Î¸Ì‚ = DeepSetExpert(Ïˆ, Ï•, S)

# Apply the estimator
Zâ‚ = rand(n, 3);                  # single set
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 4)]; # two sets
Î¸Ì‚(Zâ‚)
Î¸Ì‚(Zâ‚‚)

# Repeat the above but with set-level information:
qâ‚“ = 2
Ï•  = Chain(Dense(qâ‚œ + qâ‚› + qâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSetExpert(Ïˆ, Ï•, S)
xâ‚ = rand(qâ‚“)
xâ‚‚ = [rand(qâ‚“) for _ âˆˆ eachindex(Zâ‚‚)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚‚, xâ‚‚))
```
"""
struct DeepSetExpert{F, G, H, K}
	Ïˆ::G
	Ï•::F
	S::H
	a::K
end
Flux.@functor DeepSetExpert
Flux.trainable(d::DeepSetExpert) = (d.Ïˆ, d.Ï•)
DeepSetExpert(Ïˆ, Ï•, S; a::String = "mean") = DeepSetExpert(Ïˆ, Ï•, S, _agg(a))
DeepSetExpert(deepset::DeepSet, Ï•, S) = DeepSetExpert(deepset.Ïˆ, Ï•, S, deepset.a)
Base.show(io::IO, D::DeepSetExpert) = print(io, "\nDeepSetExpert object with:\nInner network:  $(D.Ïˆ)\nAggregation function:  $(D.a)\nExpert statistics: $(D.S)\nOuter network:  $(D.Ï•)")
Base.show(io::IO, m::MIME"text/plain", D::DeepSetExpert) = print(io, D)

# Single data set
function (d::DeepSetExpert)(Z::A) where {A <: AbstractArray{T, N}} where {T, N}
	t = d.a(d.Ïˆ(Z))
	s = d.S(Z)
	u = vcat(t, s)
	d.Ï•(u)
end

# Single data set with set-level covariates
function (d::DeepSetExpert)(tup::Tup) where {Tup <: Tuple{A, B}} where {A, B <: AbstractVector{T}} where T
	Z = tup[1]
	x = tup[2]
	t = d.a(d.Ïˆ(Z))
	s = d.S(Z)
	u = vcat(t, s, x)
	d.Ï•(u)
end

# Multiple data sets: simple fallback method using broadcasting
function (d::DeepSetExpert)(Z::V) where {V <: AbstractVector{A}} where A
  	stackarrays(d.(Z))
end


# Multiple data sets: optimised version for array data.
function (d::DeepSetExpert)(Z::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network to obtain the neural summary statistics
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa.
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# Construct the combined neural and expert summary statistics
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		s = d.S(Z[i])
		u = vcat(t, s)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	d.Ï•(u)
end

# Multiple data sets with set-level covariates
function (d::DeepSetExpert)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A, B <: AbstractVector{T}} where {T}
	Z = tup[1]
	x = tup[2]
	t = d.a.(d.Ïˆ.(Z))
	s = d.S.(Z)
	u = vcat.(t, s, x)
	stackarrays(d.Ï•.(u))
end


# Multiple data sets with set-level covariates: optimised version for array data.
function (d::DeepSetExpert)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}

	Z = tup[1]
	X = tup[2]

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network to obtain the neural summary statistics
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa.
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# concatenate the neural summary statistics with X
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		s = d.S(Z[i])
		x = X[i]
		u = vcat(t, s, x)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	d.Ï•(u)
end




# ---- GraphPropagatePool ----

"""
    GraphPropagatePool(propagation, globalpool)

A graph neural network (GNN) module designed to act as the inner network `Ïˆ` in
the `DeepSet`/`DeepSetExpert` architecture.

The `propagation` module transforms graphical input
data into a set of hidden feature graphs; the `globalpool` module aggregates
the feature graphs (graph-wise) into a single hidden-feature vector.
Critically, this hidden-feature vector is of fixed length irrespective of the
size and shape of the graph.

The data should be a `GNNGraph` or `AbstractVector{GNNGraph}`, where each graph
is associated with a single parameter vector. The graphs may contain sub-graphs
corresponding to independent replicates from the model.

# Examples
```
using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Create some graphs
d = 1             # dimension of the response variable
nâ‚, nâ‚‚ = 11, 27   # number of nodes
eâ‚, eâ‚‚ = 30, 50   # number of edges
gâ‚ = rand_graph(nâ‚, eâ‚, ndata = rand(d, nâ‚))
gâ‚‚ = rand_graph(nâ‚‚, eâ‚‚, ndata = rand(d, nâ‚‚))
g  = batch([gâ‚, gâ‚‚])

# propagation module and global pooling module
w = 5
o = 7
propagation = GNNChain(GraphConv(d => w), GraphConv(w => w), GraphConv(w => o))
meanpool = GlobalPool(mean)

# DeepSet-based estimator with GNN for the inner network Ïˆ
w = 32
p = 3
Ïˆ = GraphPropagatePool(propagation, meanpool)
Ï• = Chain(Dense(o, w, relu), Dense(w, p))
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator
Î¸Ì‚(gâ‚)           # single graph with a single replicate
Î¸Ì‚(g)            # single graph with sub-graphs (i.e., with replicates)
Î¸Ì‚([gâ‚, gâ‚‚, g])  # vector of graphs (each element is a different data set)

# Repeat the above but with set-level information:
qâ‚“ = 2
Ï• = Chain(Dense(o + qâ‚“, w, relu), Dense(w, p))
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)
xâ‚ = rand(qâ‚“)
xâ‚‚ = [rand(qâ‚“) for _ âˆˆ eachindex([gâ‚, gâ‚‚, g])]
Î¸Ì‚((gâ‚, xâ‚))
Î¸Ì‚((g, xâ‚))
Î¸Ì‚(([gâ‚, gâ‚‚, g], xâ‚‚))

# Repeat the above but with set-level information and expert statistics:
S = samplesize
qâ‚› = 1
Ï• = Chain(Dense(o + qâ‚“ + qâ‚›, w, relu), Dense(w, p))
Î¸Ì‚ = DeepSetExpert(Ïˆ, Ï•, S)
Î¸Ì‚((gâ‚, xâ‚))
Î¸Ì‚((g, xâ‚))
Î¸Ì‚(([gâ‚, gâ‚‚, g], xâ‚‚))
```
"""
struct GraphPropagatePool{F, G}
	propagation::F      # propagation module
	globalpool::G       # global pooling module
end
@functor GraphPropagatePool


# Single data set
function (est::GraphPropagatePool)(g::GNNGraph)

	# Apply the graph-to-graph transformation and global pooling
	gÌ„ = est.globalpool(est.propagation(g))

	# Extract the graph level data (i.e., pooled features), a matrix with:
	# 	nrows = number of feature graphs in final propagation layer * number of elements returned by the global pooling operation (one if global mean pooling is used)
	#	ncols = number of original graphs (i.e., number of independent replicates).
	h = gÌ„.gdata.u

	return h
end

# Multiple data sets
# Internally, we combine the graphs when doing mini-batching, to
# fully exploit GPU parallelism. What is slightly different here is that,
# contrary to most applications, we have a multiple graphs associated with each
# label (usually, each graph is associated with a label).
function (est::GraphPropagatePool)(v::V) where {V <: AbstractVector{G}} where {G <: GNNGraph}

	# Simple, inefficient implementation for sanity checking. Note that this is
	# much slower than the efficient approach below.
	# Î¸Ì‚ = stackarrays(est.(v))

	# Convert v to a super graph. Since each element of v is itself a super graph
	# (where each sub graph corresponds to an independent replicate), we need to
	# count the number of sub-graphs in each element of v for later use.
	# Specifically, we need to keep track of the indices to determine which
	# independent replicates are grouped together.
	m = numberreplicates(v)
	g = Flux.batch(v)
	# NB batch() causes array mutation, which means that this method
	# cannot be used for computing gradients during training. As a work around,
	# I've added a second method that takes both g and m. The user will not need
	# to use this method, it's only necessary internally during training.

	return est(g, m)
end
function (est::GraphPropagatePool)(g::GNNGraph, m::AbstractVector{I}) where {I <: Integer}

	# Apply the graph-to-graph transformation and global pooling
	gÌ„ = est.globalpool(est.propagation(g))

	# Extract the graph level features (i.e., pooled features), a matrix with:
	# 	nrows = number of features graphs in final propagation layer * number of elements returned by the global pooling operation (one if global mean pooling is used)
	#	ncols = total number of original graphs (i.e., total number of independent replicates).
	h = gÌ„.gdata.u

	# Split the features based on the original grouping.
	ng = length(m)
	cs = cumsum(m)
	indices = [(cs[i] - m[i] + 1):cs[i] for i âˆˆ 1:ng]
	hÌƒ = [h[:, idx] for idx âˆˆ indices]

	return hÌƒ
end








# ---- Deep Set pooling (dimension after pooling is greater than 1) ----

# Come back to this later; just get an example with global pooling working first
#
# w = 32
# R = 4
# Ïˆâ‚ = Chain(Dense(o, w, relu), Dense(w, w, relu), Dense(w, w, relu)) # NB should the input size just be one? I think so.
# Ï•â‚ = Chain(Dense(w, w, relu), Dense(w, R))
# deepsetpool = DeepSet(Ïˆâ‚, Ï•â‚)
#
# function (est::GraphPropagatePool)(g::GNNGraph)
#
# 	# Apply the graph-to-graph transformation, and then extract the node-level
# 	# features. This yields a matrix of size (H, N), where H is the number of
# 	# feature graphs in the final layer and N is the total number of nodes in
# 	# all graphs.
# 	xÌƒ = est.propagation(g).ndata[1] # node-level features
# 	H = size(xÌƒ, 1)
#
# 	# NB: The following is only necessary for more complicated pooling layers.
# 	# Now split xÌƒ according to which graph it belongs to.
# 	# find the number of nodes in each graph, and construct IntegerRange objects
# 	# to index xÌƒ appropriately
# 	I = graph_indicator(g)
# 	ng = g.num_graphs
# 	n = [sum(I .== i) for i âˆˆ 1:ng]
# 	cs  = cumsum(n)
# 	indices = [(cs[i] - n[i] + 1):cs[i] for i âˆˆ 1:ng]
# 	xÌƒ = [xÌƒ[:, idx] for idx âˆˆ indices] # NB maybe I can do this without creating this vector; see what I do for DeepSets (I don't think so, actually).
#
# 	# Apply an abitrary global pooling function to each feature graph
# 	# (i.e., each row of xÌƒ). The pooling function should return a vector of length
# 	# equal to the number of graphs, and where each element is a vector of length RH,
# 	# where R is the number of elements in each graph after pooling.
# 	h = est.globalpool(xÌƒ)
#
# 	# Apply the Deep Set module to map the learned feature vector to the
# 	# parameter space
# 	Î¸Ì‚ = est.deepset(h)
#
# 	return Î¸Ì‚
# end
#
# # # Assumes y is an Array{T, 2}, where the number of rows is H and the number of
# # # columns is equal to the number of nodes for the current graph
# # function DeepSetPool(deepset::DeepSet, y::M) where {M <: AbstractMatrix{T}} where {T}
# # 	y = [y[j, :] for j âˆˆ 1:size(y, 1)]
# # 	y = reshape.(y, 1, 1, :)
# # 	h = deepset(y)
# # 	vec(h)
# # end





# ---- Functions assuming that the propagation and globalpool layers have been wrapped in WithGraph() ----

# NB this is a low priority optimisation that is only useful if we are training
# with a fixed set of locations.

# function (est::GraphPropagatePool)(a::A) where {A <: AbstractArray{T, N}} where {T, N}
#
# 	# Apply the graph-to-graph transformation
# 	gÌƒ = est.propagation(a)
#
# 	# Global pooling
# 	# h is a matrix with,
# 	# 	nrows = number of features graphs in final propagation layer * number of elements returned by the global pooling operation (one if global mean pooling is used)
# 	#	ncols = number of original graphs (i.e., number of independent replicates).
# 	h = est.globalpool(gÌƒ)
#
# 	# Reshape matrix to three-dimensional arrays for compatibility with Flux
# 	o = size(h, 1)
# 	h = reshape(h, o, 1, :)
#
# 	# Apply the Deep Set module to map to the parameter space.
# 	Î¸Ì‚ = est.deepset(h)
# end
#
#
# function (est::GraphPropagatePool)(v::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}
#
# 	# Simple, less efficient implementation for sanity checking:
# 	Î¸Ì‚ = stackarrays(est.(v))
#
# 	# # Convert v to a super graph. Since each element of v is itself a super graph
# 	# # (where each sub graph corresponds to an independent replicate), we need to
# 	# # count the number of sub-graphs in each element of v for later use.
# 	# # Specifically, we need to keep track of the indices to determine which
# 	# # independent replicates are grouped together.
# 	# m = est.propagation.g.num_graphs
# 	# m = repeat([m], length(v))
# 	#
# 	# g = Flux.batch(repeat([est.propagation.g], length(v)))
# 	# g = GNNGraph(g, ndata = (Z = stackarrays(v)))
# 	#
# 	# # Apply the graph-to-graph transformation
# 	# gÌƒ = est.propagation.model(g)
# 	#
# 	# # Global pooling
# 	# gÌ„ = est.globalpool(gÌƒ)
# 	#
# 	# # Extract the graph level data (i.e., the pooled features).
# 	# # h is a matrix with,
# 	# # 	nrows = number of features graphs in final propagation layer * number of elements returned by the global pooling operation (one if global mean pooling is used)
# 	# #	ncols = total number of original graphs (i.e., total number of independent replicates).
# 	# h = gÌ„.gdata[1]
# 	#
# 	# # Split the data based on the original grouping
# 	# ng = length(v)
# 	# cs = cumsum(m)
# 	# indices = [(cs[i] - m[i] + 1):cs[i] for i âˆˆ 1:length(v)]
# 	# h = [h[:, idx] for idx âˆˆ indices]
# 	#
# 	# # Reshape matrices to three-dimensional arrays for compatibility with Flux
# 	# o = size(h[1], 1)
# 	# h = reshape.(h, o, 1, :)
# 	#
# 	# # Apply the Deep Set module to map to the parameter space.
# 	# Î¸Ì‚ = est.deepset(h)
#
# 	return Î¸Ì‚
# end

# ---- Compress ----


"""
    Compress(a, b)

Uses the scaled logistic function to compress the output of a neural network to
be between `a` and `b`.

The elements of `a` should be less than the corresponding element of `b`.

# Examples
```
using NeuralEstimators
using Flux

p = 3
a = [0.1, 4, 2]
b = [0.9, 9, 3]
l = Compress(a, b)
K = 10
Î¸ = rand(p, K)
l(Î¸)

n = 20
Z = rand(n, K)
Î¸Ì‚ = Chain(Dense(n, p), l)
Î¸Ì‚(Z)
```
"""
struct Compress{T}
  a::T
  b::T
  m::T
end
Compress(a, b) = Compress(a, b, (b + a) / 2)

(l::Compress)(Î¸) = l.a .+ (l.b - l.a) ./ (one(eltype(Î¸)) .+ exp.(-(Î¸ .- l.m)))

Flux.@functor Compress
Flux.trainable(l::Compress) =  ()


# ---- CholeskyParameters and CovarianceMatrixParameters ----

# Original discussion: https://groups.google.com/g/julia-users/c/UARlZBCNlng
"""
	vectotril(v)
	vectotriu(v)
Converts a vector `v` of length ``d(d+1)Ã·2`` into a ``d``-dimensional lower
or upper triangular matrix.

Note that the triangular matrix is constructed on the CPU, but the returned
matrix will be a GPU array if `v` is a GPU array. Note also that the
return type is not of type `Triangular` matrix (i.e., the zeros are
materialised) since `Traingular` matrices are not always compatible with other
GPU operations.

# Examples
```
d = 4
n = d*(d+1)Ã·2
v = collect(range(1, n))
vectotril(v)
vectotriu(v)
```
"""
function vectotril(v) where V
	ArrayType = containertype(v)
	T = eltype(v)
	v = cpu(v)
	n = length(v)
	d = (-1 + isqrt(1 + 8n)) Ã· 2
	d*(d+1)Ã·2 == n || error("vectotril: length of vector is not triangular")

	#TODO  get rid of k like I do in vectotriu(), for simpler code and one less allocation (k)
	k = 0
	L = [ i >= j ? (k+=1; v[k]) : zero(T) for i=1:d, j=1:d ]
	# L = [ i>=j ? v[i*(i-1)Ã·2+j] : zero(T) for i=1:d, j=1:d ]
	convert(ArrayType, L)
end

function vectotriu(v) where V
	ArrayType = containertype(v)
	T = eltype(v)
	v = cpu(v)
	n = length(v)
	d = (-1 + isqrt(1 + 8n)) Ã· 2
	d*(d+1)Ã·2 == n || error("vectotriu: length of vector is not triangular")
	U = [ i<=j ? v[j*(j-1)Ã·2+i] : zero(T) for i=1:d, j=1:d ]
	convert(ArrayType, U)
end


@doc raw"""
    CholeskyParameters(d)
	CholeskyParametersConstrained(d, determinant = 1f0)
Layer for constructing the parameters of a Cholesky factor for a `d`-dimensional
random vector.

This layer transforms an `Matrix` with `d`(`d`+1)Ã·2 rows (the number of
non-zero elements in a Cholesky factor) into a `Matrix` of the same
dimension, but with `d` rows constrained to be positive (corresponding to
the diagonal elements of the Cholesky factor) and the remaining rows
unconstrained.

The ordering of the transformed array aligns with Julia's column-major ordering,
so that a Cholesky factor with `d` = 3,

```math
\begin{bmatrix}
Lâ‚â‚ &  &  \\
Lâ‚‚â‚ & Lâ‚‚â‚‚ &  \\
Lâ‚ƒâ‚ & Lâ‚ƒâ‚‚ & Lâ‚ƒâ‚ƒ \\
\end{bmatrix},
```

will follow the ordering ``[Lâ‚â‚, Lâ‚‚â‚, Lâ‚ƒâ‚, Lâ‚‚â‚‚, Lâ‚ƒâ‚‚, Lâ‚ƒâ‚ƒ]'``. Since
the diagonal elements must be positive, in this example rows 1, 4, and 6 of the
transformed array will be constrained to be positive.

`CholeskyParametersConstrained` constrains the `determinant` of the Cholesky
factor. Since the determinant of a triangular matrix is equal to the product of
its diagonal elements, the determinant is constrained by setting the final
diagonal element equal to `determinant`/``(Î  Láµ¢áµ¢)`` where the product is over
``i < d``.

# Examples
```
using NeuralEstimators

d = 4
p = d*(d+1)Ã·2
Î¸ = randn(p, 50)
l = CholeskyParameters(d)
l(Î¸)                                       # returns matrix (used for Flux networks)
L = [vectotril(y) for y âˆˆ eachcol(l(Î¸))]   # convert matrix to Cholesky factors
```
"""
struct CholeskyParameters{T <: Integer, G}
  d::T
  diag_idx::G
end
function CholeskyParameters(d::Integer)
	diag_idx = [1]
	for i âˆˆ 1:(d-1)
		push!(diag_idx, diag_idx[i] + d-i+1)
	end
	CholeskyParameters(d, diag_idx)
end
function (l::CholeskyParameters)(x)
	p, K = size(x)
	y = [i âˆˆ l.diag_idx ? exp.(x[i, :]) : x[i, :] for i âˆˆ 1:p]

	# Original code:
	# copy(stackarrays(y, merge = false)')

	# New code (compatible with Zygote, since it advoids Transpose/Adjoint objects):
	permutedims(reshape(vcat(y...), K, p))
end

struct CholeskyParametersConstrained{T <: Integer, G}
  d::T
  determinant::G
  choleskyparameters::CholeskyParameters
end
function CholeskyParametersConstrained(d, determinant = 1f0)
	CholeskyParametersConstrained(d, determinant, CholeskyParameters(d))
end
function (l::CholeskyParametersConstrained)(x)
	y = l.choleskyparameters(x)
	u = y[l.choleskyparameters.diag_idx[1:end-1], :]
	v = l.determinant ./ prod(u, dims = 1)
	vcat(y[1:end-1, :], v)
end

@doc raw"""
    CovarianceMatrixParameters(d)
	CovarianceMatrixParametersConstrained(d, determinant = 1f0)

Layer for constructing the parameters of a covariance matrix for a
`d`-dimensional random vector.

Due to symmetry, there are `d`(`d` + 1)/2 free parameters in a covariance
matrix, so this layer transforms a `Matrix` with `d`(`d` + 1)/2 rows into a
`Matrix` of the same dimension. Internally, it uses a `CholeskyParameters` layer
to construct a valid Cholesky factor, from which a positive-definite covariance
matrix Î£ can be computed.

The ordering of the transformed array aligns with Julia's column-major ordering,
so that a covariance matrix with `d` = 3,

```math
\begin{bmatrix}
Î£â‚â‚ & Î£â‚â‚‚ & Î£â‚â‚ƒ \\
Î£â‚‚â‚ & Î£â‚‚â‚‚ & Î£â‚‚â‚ƒ \\
Î£â‚ƒâ‚ & Î£â‚ƒâ‚‚ & Î£â‚ƒâ‚ƒ \\
\end{bmatrix},
```

will follow the ordering ``[Î£â‚â‚, Î£â‚‚â‚, Î£â‚ƒâ‚, Î£â‚‚â‚‚, Î£â‚ƒâ‚‚, Î£â‚ƒâ‚ƒ]'``. Only
the lower triangle of the matrix is returned because covariance matrices are
symmetric.

`CovarianceMatrixParametersConstrained` constrains the `determinant` of the
covariance matrix to `determinant`.

# Examples
```
using NeuralEstimators
using LinearAlgebra

d = 4
p = d*(d+1)Ã·2
l = CovarianceMatrixParameters(d)
Î¸ = randn(p, 50)

# returns matrix (used for Flux networks)
l(Î¸)

# convert matrix to Cholesky factors
# (note that Symmetric behaves slightly differently on GPU and CPU matrices, so
# for consistency we first move the lower triangular matrix to the CPU)
[Symmetric(cpu(vectotril(y)), :L) for y âˆˆ eachcol(l(Î¸))]
```
"""
struct CovarianceMatrixParameters{T <: Integer, G}
  d::T
  idx::G
  choleskyparameters::CholeskyParameters
end
function CovarianceMatrixParameters(d::Integer)
	idx = tril(trues(d, d))
	idx = findall(vec(idx)) # convert to scalar indices
	return CovarianceMatrixParameters(d, idx, CholeskyParameters(d))
end

struct CovarianceMatrixParametersConstrained{T <: Integer, G}
  d::T
  idx::G
  choleskyparameters::CholeskyParametersConstrained
end
function CovarianceMatrixParametersConstrained(d::Integer, determinant = 1f0)
	idx = tril(trues(d, d))
	idx = findall(vec(idx)) # convert to scalar indices
	return CovarianceMatrixParametersConstrained(d, idx, CholeskyParametersConstrained(d, sqrt(determinant)))
end

function (l::Union{CovarianceMatrixParameters, CovarianceMatrixParametersConstrained})(x)
	L = _constructL(l.choleskyparameters, x)
	Î£ = broadcast(x -> x*permutedims(x), L) # note that I replaced x' with permutedims(x) because Transpose/Adjoints don't work well with Zygote
	Î¸ = broadcast(x -> x[l.idx], Î£)
	return hcat(Î¸...)
end

function _constructL(l::Union{CholeskyParameters, CholeskyParametersConstrained}, x)
	LÎ¸ = l(x)
	K = size(LÎ¸, 2)
	L = [vectotril(view(LÎ¸, :, i)) for i âˆˆ 1:K]
	L
end

function _constructL(l::Union{CholeskyParameters, CholeskyParametersConstrained}, x::Array)
	LÎ¸ = l(x)
	K = size(LÎ¸, 2)
	L = [vectotril(collect(view(LÎ¸, :, i))) for i âˆˆ 1:K]
	L
end


"""
	SplitApply(layers, indices)
Splits an array into multiple sub-arrays by subsetting the rows using
the collection of `indices`, and then applies each layer in `layers` to the
corresponding sub-array.

Specifically, for each `i` = 1, â€¦, ``n``, with ``n`` the number of `layers`,
`SplitApply(x)` performs `layers[i](x[indices[i], :])`, and then vertically
concatenates the resulting transformed arrays.

# Examples
```
using NeuralEstimators

d = 4
K = 50
pâ‚ = 2          # number of non-covariance matrix parameters
pâ‚‚ = d*(d+1)Ã·2  # number of covariance matrix parameters
p = pâ‚ + pâ‚‚

a = [0.1, 4]
b = [0.9, 9]
lâ‚ = Compress(a, b)
lâ‚‚ = CovarianceMatrixParameters(d)
l = SplitApply([lâ‚, lâ‚‚], [1:pâ‚, pâ‚+1:p])

Î¸ = randn(p, K)
l(Î¸)
```
"""
struct SplitApply{T,G}
  layers::T
  indices::G
end
Flux.@functor SplitApply (layers, )
Flux.trainable(l::SplitApply) = ()
function (l::SplitApply)(x::AbstractArray)
	vcat([layer(x[idx, :]) for (layer, idx) in zip(l.layers, l.indices)]...)
end


(l::CholeskyParameters)(x::AbstractVector) = l(reshape(x, :, 1))
(l::CholeskyParametersConstrained)(x::AbstractVector) = l(reshape(x, :, 1))
(l::CovarianceMatrixParameters)(x::AbstractVector) = l(reshape(x, :, 1))
(l::CovarianceMatrixParametersConstrained)(x::AbstractVector) = l(reshape(x, :, 1))
