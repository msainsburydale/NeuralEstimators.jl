# markdown code for documentation in docs/src/workflow/advancedusage.md:
# # Combining neural and expert summary statistics
#
# See [`DeepSetExpert`](@ref).

"""
	samplesize(Z)

Computes the sample size m for a set of independent realisations `Z`, often
useful as an expert summary statistic in `DeepSetExpert` objects.

Note that this function is a simple wrapper around `numberreplicates`, but this
function returns the number of replicates as the eltype of `Z`.
"""
samplesize(Z) = eltype(Z)(numberreplicates(Z))

samplesize(Z::V) where V <: AbstractVector = samplesize.(Z)


# ---- DeepSetExpert Type and constructors ----

# we use unicode characters below to preserve readability of REPL help files
"""
	DeepSetExpert(Ïˆ, Ï•, S, a)
	DeepSetExpert(Ïˆ, Ï•, S; a::String)
	DeepSetExpert(deepset::DeepSet, Ï•, S)


A neural estimator in the `DeepSet` representation with additional expert
summary statistics,

```math
Î¸Ì‚(ð™) = Ï•((ð“(ð™)', ð’(ð™)')'),	â€‚	â€‚ð“(ð™) = ðš(\\{Ïˆ(ð™áµ¢) : i = 1, â€¦, m\\}),
```

where ð™ â‰¡ (ð™â‚', â€¦, ð™â‚˜')' are independent replicates from the model,
`Ïˆ` and `Ï•` are neural networks, `S` is a function that returns a vector
of expert summary statistics, and `ðš` is a permutation-invariant
aggregation function.

The dimension of the domain of `Ï•` must be qâ‚œ + qâ‚›, where qâ‚œ and qâ‚› are the
dimensions of the ranges of `Ïˆ` and `S`, respectively.

The constructor `DeepSetExpert(deepset::DeepSet, Ï•, S)` inherits `Ïˆ` and `a`
from `deepset`.

See `?DeepSet` for discussion on the aggregation function `ðš`.

# Examples
```
using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qâ‚› = 1
qâ‚œ = 32
w = 16
Ïˆ = Chain(Dense(n, w, relu), Dense(w, qâ‚œ, relu));
Ï• = Chain(Dense(qâ‚œ + qâ‚›, w), Dense(w, p));
Î¸Ì‚ = DeepSetExpert(Ïˆ, Ï•, S)

# Apply the estimator to a single set of 3 realisations:
Z = rand(n, 3);
Î¸Ì‚(Z)

# Apply the estimator to two sets each containing 3 realisations:
Z = [rand(n, m) for m âˆˆ (3, 3)];
Î¸Ì‚(Z)

# Apply the estimator to two sets containing 3 and 4 realisations, respectively:
Z = [rand(n, m) for m âˆˆ (3, 4)];
Î¸Ì‚(Z)
```
"""
struct DeepSetExpert{F, G, H, K}
	Ïˆ::G
	Ï•::F
	S::H
	a::K
end
#TODO make this a superclass of DeepSet? Would be better to have a single class
# that dispatches to different methods depending on wether S is present or not.

Flux.@functor DeepSetExpert
Flux.trainable(d::DeepSetExpert) = (d.Ïˆ, d.Ï•)

DeepSetExpert(Ïˆ, Ï•, S; a::String = "mean") = DeepSetExpert(Ïˆ, Ï•, S, _agg(a))
DeepSetExpert(deepset::DeepSet, Ï•, S) = DeepSetExpert(deepset.Ïˆ, Ï•, S, deepset.a)

Base.show(io::IO, D::DeepSetExpert) = print(io, "\nDeepSetExpert object with:\nInner network:  $(D.Ïˆ)\nAggregation function:  $(D.a)\nExpert statistics: $(D.S)\nOuter network:  $(D.Ï•)")
Base.show(io::IO, m::MIME"text/plain", D::DeepSetExpert) = print(io, D)


# ---- Methods ----

function (d::DeepSetExpert)(Z::A) where {A <: AbstractArray{T, N}} where {T, N}
	t = d.a(d.Ïˆ(Z))
	s = d.S(Z)
	u = vcat(t, s)
	d.Ï•(u)
end

function (d::DeepSetExpert)(tup::Tup) where {Tup <: Tuple{A, B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}
	Z = tup[1]
	x = tup[2]
	t = d.a(d.Ïˆ(Z))
	s = d.S(Z)
	u = vcat(Z, s, x)
	d.Ï•(u)
end

# # Simple, intuitive (although inefficient) implementation using broadcasting:
# function (d::DeepSetExpert)(v::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}
#   stackarrays(d.(Z))
# end

# Optimised version. This approach ensures that the neural networks Ï• and Ï are
# applied to arrays that are as large as possible, improving efficiency compared
# with the intuitive method above (particularly on the GPU):
# Note I can't take the gradient of this function... Might have to open an issue with Zygote.
function (d::DeepSetExpert)(Z::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network to obtain the neural summary statistics
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa.
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# Construct the combined neural and expert summary statistics
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		s = d.S(Z[i])
		u = vcat(t, s)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	d.Ï•(u)
end

function (d::DeepSetExpert)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}

	Z = tup[1]
	X = tup[2]

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network to obtain the neural summary statistics
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa.
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# concatenate the neural summary statistics with X
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		s = d.S(Z[i])
		x = X[i]
		u = vcat(t, s, x)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	d.Ï•(u)
end
