# This is an internal function used in Flux to check the size of the
# arguments passed to a loss function
function _check_sizes(yÌ‚::AbstractArray, y::AbstractArray)
    for d = 1:max(ndims(yÌ‚), ndims(y))
        size(yÌ‚, d) == size(y, d) || throw(DimensionMismatch(
            "loss function expects size(yÌ‚) = $(size(yÌ‚)) to match size(y) = $(size(y))"
        ))
    end
end
_check_sizes(yÌ‚, y) = nothing  # pass-through, for constant label e.g. y = 1
@non_differentiable _check_sizes(yÌ‚::Any, y::Any)

# ---- surrogates for 0-1 loss ----

@doc raw"""
    tanhloss(Î¸Ì‚, Î¸, Îº; joint::Bool = true, scale_by_parameter_dim::Bool = true)
For `Îº` > 0, computes the loss function given in [Sainsbury-Dale et al. (2025; Eqn. 14)](https://arxiv.org/abs/2501.04330), namely,
```math
L(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \tanh\big(\big\|\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}\|_1/\kappa\big),
```
which yields the 0-1 loss function in the limit `Îº` â†’ 0.

If `joint = true` (default), the Lâ‚ norm is computed over each parameter vector, so that with `Îº` close to zero, the resulting Bayes estimator approximates the mode of the joint posterior distribution. Otherwise, if `joint = false`, the loss function is computed as 
```math
L(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \sum_{i=1}^d \tanh\big(|\hat{\theta}_i - \theta_i|/\kappa\big),
```
where $d$ denotes the dimension of the parameter vector $\boldsymbol{\theta}$. In this case, with `Îº` close to zero, the resulting Bayes estimator approximates the vector containing the modes of the marginal posterior distributions.

Compared with the [`kpowerloss()`](@ref), which may also be used as a continuous approximation of the 0--1 loss function, the gradient of
this loss is bounded as ``\|\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}\|_1 \to 0``, which can improve numerical stability during training. 
"""
function tanhloss(Î¸Ì‚, Î¸, Îº; joint::Bool = true, scale_by_parameter_dim::Bool = true)
    _check_sizes(Î¸Ì‚, Î¸)

    T = eltype(Î¸)
    Îº = T(Îº)
    p = size(Î¸, 1)
    scale = scale_by_parameter_dim ? sqrt(T(p)) : one(T)

    d = @. abs(Î¸Ì‚ - Î¸)

    if joint
        d = sum(d, dims = 1) ./ scale
    end

    L = tanh_fast.(d ./ Îº)
    return mean(L)
end

@doc raw"""
    kpowerloss(Î¸Ì‚, Î¸, Îº; agg = mean, safeorigin = true, Ïµ = 0.1)
For `Îº` > 0, the `Îº`-th power absolute-distance loss function,
```math
L(\hat{\boldsymbol{\theta}}, \boldsymbol{\theta}) = \|\hat{\boldsymbol{\theta}} - \boldsymbol{\theta}\|_1^\kappa,
```
contains the squared-error (`Îº` = 2), absolute-error (`Îº` = 2), and 0--1 (`Îº` â†’ 0) loss functions as special
cases. It is Lipschitz continuous if `Îº` = 1, convex if `Îº` â‰¥ 1, and strictly convex if `Îº` > 1. It is
quasiconvex for all `Îº` > 0.

If `safeorigin = true`, the loss function is modified to be piecewise, continuous, and linear in the `Ïµ`-interval surrounding the origin, to avoid pathologies around the origin. 

See also [`tanhloss()`](@ref).
"""
function kpowerloss(Î¸Ì‚, Î¸, Îº; safeorigin::Bool = true, agg = mean, Ïµ = ofeltype(Î¸Ì‚, 0.1), joint::Bool = true)

    #  If `joint = true`, the Lâ‚ norm is computed over each parameter vector, so that, with 
    # `Îº` close to zero, the resulting Bayes estimator is the mode of the joint posterior distribution;
    #  otherwise, if `joint = false`, the Bayes estimator is the vector containing the modes of the
    #  marginal posterior distributions.

    _check_sizes(Î¸Ì‚, Î¸)

    d = abs.(Î¸Ì‚ .- Î¸)
    if joint
        d = sum(d, dims = 1)
    end

    if safeorigin
        b = d .> Ïµ
        L = vcat(d[b] .^ Îº, _safefunction.(d[.!b], Îº, Ïµ))
    else
        L = d .^ Îº
    end

    return agg(L)
end

function _safefunction(d, Îº, Ïµ)
    @assert d >= 0
    Ïµ^(Îº - 1) * d
end

# ---- quantile loss ----

#TODO write the maths for when we have a vector Ï„
"""
    quantileloss(Î¸Ì‚, Î¸, Ï„; agg = mean)
    quantileloss(Î¸Ì‚, Î¸, Ï„::Vector; agg = mean)

The asymmetric quantile loss function,
```math
  L(Î¸Ì‚, Î¸; Ï„) = (Î¸Ì‚ - Î¸)(ğ•€(Î¸Ì‚ - Î¸ > 0) - Ï„),
```
where `Ï„` âˆˆ (0, 1) is a probability level and ğ•€(â‹…) is the indicator function.

The method that takes `Ï„` as a vector is useful for jointly approximating
several quantiles of the posterior distribution. In this case, the number of
rows in `Î¸Ì‚` is assumed to be ``dr``, where ``d`` is the number of parameters and
``r`` is the number probability levels in `Ï„` (i.e., the length of `Ï„`).
"""
function quantileloss(Î¸Ì‚, Î¸, Ï„; agg = mean)
    _check_sizes(Î¸Ì‚, Î¸)
    d = Î¸Ì‚ .- Î¸
    b = d .> 0
    bÌƒ = .!b
    Lâ‚ = d[b] * (1 - Ï„)
    Lâ‚‚ = -Ï„ * d[bÌƒ]
    L = vcat(Lâ‚, Lâ‚‚)
    agg(L)
end

function quantileloss(Î¸Ì‚, Î¸, Ï„::V; agg = mean) where {T, V <: AbstractVector{T}}
    Ï„ = convert(containertype(Î¸Ì‚), Ï„) # convert Ï„ to the gpu (this line means that users don't need to manually move Ï„ to the gpu)

    # Check that the sizes match
    @assert size(Î¸Ì‚, 2) == size(Î¸, 2)
    d, K = size(Î¸)

    if length(Ï„) == K # different Ï„ for each training sample => must be training continuous quantile estimator with Ï„ as input
        @ignore_derivatives Ï„ = repeat(Ï„', d) # just repeat Ï„ to match the number of parameters in the statistical model
        quantileloss(Î¸Ì‚, Î¸, Ï„; agg = agg)
    else # otherwise, we must training a discrete quantile estimator for some fixed set of probability levels
        rd = size(Î¸Ì‚, 1)
        @assert rd % d == 0
        r = rd Ã· d
        @assert length(Ï„) == r

        # repeat the arrays to facilitate broadcasting and indexing
        # note that repeat() cannot be differentiated by Zygote
        @ignore_derivatives Ï„ = repeat(Ï„, inner = (d, 1), outer = (1, K))
        @ignore_derivatives Î¸ = repeat(Î¸, r)

        quantileloss(Î¸Ì‚, Î¸, Ï„; agg = agg)
    end
end

#NB matrix method is only used internally, and therefore not documented 
function quantileloss(Î¸Ì‚, Î¸, Ï„::M; agg = mean) where {T, M <: AbstractMatrix{T}}
    d = Î¸Ì‚ .- Î¸
    b = d .> 0
    bÌƒ = .!b
    Lâ‚ = d[b] .* (1 .- Ï„[b])
    Lâ‚‚ = -Ï„[bÌƒ] .* d[bÌƒ]
    L = vcat(Lâ‚, Lâ‚‚)
    agg(L)
end

# ---- interval score ----

"""
    intervalscore(l, u, Î¸, Î±; agg = mean)
    intervalscore(Î¸Ì‚, Î¸, Î±; agg = mean)
    intervalscore(assessment::Assessment; average_over_parameters::Bool = false, average_over_sample_sizes::Bool = true)

Given an interval [`l`, `u`] with nominal coverage 100Ã—(1-`Î±`)%  and true value `Î¸`, the
interval score ([Gneiting and Raftery, 2007](https://www.tandfonline.com/doi/abs/10.1198/016214506000001437)) is defined as
```math
S(l, u, Î¸; Î±) = (u - l) + 2Î±â»Â¹(l - Î¸)ğ•€(Î¸ < l) + 2Î±â»Â¹(Î¸ - u)ğ•€(Î¸ > u),
```
where `Î±` âˆˆ (0, 1) and ğ•€(â‹…) is the indicator function.

The method that takes a single value `Î¸Ì‚` assumes that `Î¸Ì‚` is a matrix with ``2d`` rows,
where ``d`` is the dimension of the parameter vector to make inference on. The first
and second sets of ``d`` rows will be used as `l` and `u`, respectively.
"""
function intervalscore(l, u, Î¸, Î±; agg = mean)
    bâ‚ = Î¸ .< l
    bâ‚‚ = Î¸ .> u

    S = u - l
    S = S + bâ‚ .* (2 / Î±) .* (l .- Î¸)
    S = S + bâ‚‚ .* (2 / Î±) .* (Î¸ .- u)

    agg(S)
end

function intervalscore(Î¸Ì‚, Î¸, Î±; agg = mean)
    @assert size(Î¸Ì‚, 1) % 2 == 0
    d = size(Î¸Ì‚, 1) Ã· 2
    l = Î¸Ì‚[1:d, :]
    u = Î¸Ì‚[(d + 1):end, :]

    intervalscore(l, u, Î¸, Î±, agg = agg)
end
