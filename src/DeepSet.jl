using Functors: @functor
using RecursiveArrayTools: VectorOfArray, convert

# ---- Aggregation (pooling) functions ----

meanlastdim(X::A) where {A <: AbstractArray{T, N}} where {T, N} = mean(X, dims = N)
sumlastdim(X::A)  where {A <: AbstractArray{T, N}} where {T, N} = sum(X, dims = N)
LSElastdim(X::A)  where {A <: AbstractArray{T, N}} where {T, N} = logsumexp(X, dims = N)

function _agg(a::String)
	@assert a âˆˆ ["mean", "sum", "logsumexp"]
	if a == "mean"
		meanlastdim
	elseif a == "sum"
		sumlastdim
	elseif a == "logsumexp"
		LSElastdim
	end
end

# ---- DeepSet Type and constructors ----

# we use unicode characters below to preserve readability of REPL help files
"""
    DeepSet(Ïˆ, Ï•, a)
	DeepSet(Ïˆ, Ï•; a::String = "mean")

A neural estimator in the `DeepSet` representation,

```math
Î¸Ì‚(ð™) = Ï•(ð“(ð™)),	â€‚	â€‚ð“(ð™) = ðš(\\{Ïˆ(ð™áµ¢) : i = 1, â€¦, m\\}),
```

where ð™ â‰¡ (ð™â‚', â€¦, ð™â‚˜')' are independent replicates from the model, `Ïˆ` and `Ï•`
are neural networks, and `ðš` is a permutation-invariant aggregation function.

The function `ðš` must aggregate over the last dimension of an array (i.e., the
replicates dimension). It can be specified as a positional argument of
type `Function`, or as a keyword argument of type `String` with permissible
values `"mean"`, `"sum"`, and `"logsumexp"`.

# Examples
```
using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï• = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator to a single set of 3 realisations:
Zâ‚ = rand(n, 3);
Î¸Ì‚(Zâ‚)

# Apply the estimator to two sets each containing 3 realisations:
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 3)];
Î¸Ì‚(Zâ‚‚)

# Apply the estimator to two sets containing 3 and 4 realisations, respectively:
Zâ‚ƒ = [rand(n, m) for m âˆˆ (3, 4)];
Î¸Ì‚(Zâ‚ƒ)

# Repeat the above but with some covariates:
dâ‚“ = 2
Ï•â‚“ = Chain(Dense(w + dâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSet(Ïˆ, Ï•â‚“)
xâ‚ = rand(dâ‚“)
xâ‚‚ = [rand(dâ‚“), rand(dâ‚“)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚ƒ, xâ‚‚))
```
"""
struct DeepSet{T, F, G}
	Ïˆ::T
	Ï•::G
	a::F
end
# ð™â‚ â†’ Ïˆ() \n
#          â†˜ \n
# â‹®     â‹®     a() â†’ Ï•() \n
#          â†— \n
# ð™â‚˜ â†’ Ïˆ() \n


DeepSet(Ïˆ, Ï•; a::String = "mean") = DeepSet(Ïˆ, Ï•, _agg(a))

@functor DeepSet # allows Flux to optimise the parameters

# Clean printing:
Base.show(io::IO, D::DeepSet) = print(io, "\nDeepSet object with:\nInner network:  $(D.Ïˆ)\nAggregation function:  $(D.a)\nOuter network:  $(D.Ï•)")
Base.show(io::IO, m::MIME"text/plain", D::DeepSet) = print(io, D)


# ---- Methods ----

function (d::DeepSet)(Z::A) where {A <: AbstractArray{T, N}} where {T, N}
	d.Ï•(d.a(d.Ïˆ(Z)))
end

function (d::DeepSet)(tup::Tup) where {Tup <: Tuple{A, B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}
	Z = tup[1]
	x = tup[2]
	t = d.a(d.Ïˆ(Z))
	d.Ï•(vcat(t, x))
end


# Simple, intuitive (although inefficient) implementation using broadcasting:

# function (d::DeepSet)(v::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}
#   Î¸Ì‚ = d.Ï•.(d.a.(d.Ïˆ.(v)))
#   Î¸Ì‚ = stackarrays(Î¸Ì‚)
#   return Î¸Ì‚
# end

# Optimised version. This approach ensures that the neural networks Ïˆ and Ï• are
# applied to arrays that are as large as possible, improving efficiency compared
# with the intuitive method above (particularly on the GPU):
function (d::DeepSet)(Z::V) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa. Note that constructing
	# colons in this manner makes the function agnostic to ndims(Ïˆa).
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# Aggregate each set of transformed features: The resulting vector from the
	# list comprehension is a vector of arrays, where the last dimension of each
	# array is of size 1. Then, stack this vector of arrays into one large array,
	# where the last dimension of this large array has size equal to length(v).
	# Note that we cannot pre-allocate and fill an array, since array mutation
	# is not supported by Zygote (which is needed during training).
	large_aggregated_Ïˆa = [d.a(Ïˆa[colons..., idx]) for idx âˆˆ indices] |> stackarrays

	# Apply the outer network
	Î¸Ì‚ = d.Ï•(large_aggregated_Ïˆa)

	return Î¸Ì‚
end

function (d::DeepSet)(tup::Tup) where {Tup <: Tuple{Vâ‚, Vâ‚‚}} where {Vâ‚ <: AbstractVector{A}, Vâ‚‚ <: AbstractVector{B}} where {A <: AbstractArray{T, N}, B <: AbstractVector{T}} where {T, N}


	Z = tup[1]
	X = tup[2]

	# Convert to a single large Array
	z = stackarrays(Z)

	# Apply the inner neural network to obtain the neural summary statistics
	Ïˆa = d.Ïˆ(z)

	# Compute the indices needed for aggregation and construct a tuple of colons
	# used to subset all but the last dimension of Ïˆa.
	indices = _getindices(Z)
	colons  = ntuple(_ -> (:), ndims(Ïˆa) - 1)

	# concatenate the neural summary statistics with X
	u = map(eachindex(Z)) do i
		idx = indices[i]
		t = d.a(Ïˆa[colons..., idx])
		x = X[i]
		u = vcat(t, x)
		u
	end
	u = stackarrays(u)

	# Apply the outer network
	Î¸Ì‚ = d.Ï•(u)

	return Î¸Ì‚
end
