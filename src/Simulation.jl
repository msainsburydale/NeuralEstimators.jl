# TODO Finish the documentation for all of the simulateX functions..
# TODO throughout the repo, I should be consistent with the order in which Î¾ and
# parameters appear; I think it's best for Î¾ to appear before parameters.


"""
	simulate(parameters::P, Î¾, m::Integer, num_rep::Integer) where {P <: ParameterConfigurations}

Generic method that simulates `num_rep` sets of  sets of `m` independent replicates for each parameter
configuration by calling `simulate(parameters, Î¾, m)`.
"""
function simulate(parameters::P, Î¾, m::Integer, num_rep::Integer) where {P <: ParameterConfigurations}
	v = [simulate(parameters, Î¾, m) for i âˆˆ 1:num_rep]
	v = vcat(v...) # should be ok since we're only splatting num_rep vectors, which doesn't get prohibitively large even during bootstrapping. No reason not to use stack, though.
	return v
end

# Wrapper function that returns simulated data and the true parameter values
_simulate(params::P, Î¾, m) where {P <: ParameterConfigurations} = (simulate(params, Î¾, m), params.Î¸)



# ---- Gaussian process ----


"""
	simulategaussianprocess(L::AbstractArray{T, 2}, Ïƒ::T, m::Integer)
	simulategaussianprocess(L::AbstractArray{T, 2})

Simulates `m` realisations from a Gau(0, ğšº + ÏƒÂ²ğˆ) distribution, where ğšº â‰¡ LL'.

If `Ïƒ` and `m` are not provided, a single field without nugget variance is returned.
"""
function simulategaussianprocess(L::AbstractArray{T, 2}, Ïƒ::T, m::Integer) where T
	n = size(L, 1)
	y = similar(L, n, m)
	for h âˆˆ 1:m
		y[:, h] = simulategaussianprocess(L, Ïƒ)
	end
	return y
end

function simulategaussianprocess(L::AbstractArray{T, 2}, Ïƒ::T) where T
	n = size(L, 1)
	return simulategaussianprocess(L) + Ïƒ * randn(T, n)
end

function simulategaussianprocess(L::AbstractArray{T, 2}) where T
	n = size(L, 1)
	y = randn(T, n)
	return L * y
end


# ---- Schlather's max-stable model ----

"""
	simulateschlather(L::AbstractArray{T, 2}; C = 3.5)
	simulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)

Simulates from Schlather's max-stable model. Based on Algorithm 1.2.2 of Dey DK, Yan J (2016). Extreme value modeling and
risk analysis: methods and applications. CRC Press, Boca Raton, Florida.
"""
function simulateschlather(L::AbstractArray{T, 2}; C = 3.5) where T <: Number

	n = size(L, 1)  # number of spatial locations

	Z   = fill(zero(T), n) # TODO Why fill this with zeros? Just do undef.
	Î¶â»Â¹ = randexp(T)
	Î¶   = 1 / Î¶â»Â¹

	# A property of the model that must be enforced is E(max{0, Yáµ¢}) = 1. It can
	# be shown that this condition is satisfied if the marginal variance of Y(â‹…)
	# is equal to 2Ï€. Now, our simulation design embeds a marginal variance of 1
	# into fields generated from the cholesky factors, and hence
	# simulategaussianprocess(L) returns simulations from a Gaussian
	# process with marginal variance 1. To scale the marginal variance to
	# 2Ï€, we therefore need to multiply the field by âˆš(2Ï€).

	# Note that, compared with Algorithm 1.2.2 of Dey DK, Yan J (2016),
	# some simplifications have been made to the code below. This is because
	# max{Z(s), Î¶W(s)} â‰¡ max{Z(s), max{0, Î¶Y(s)}} = max{Z(s), Î¶Y(s)}, since
	# Z(s) is initialised to 0 and increases during simulation.
	while (Î¶ * C) > minimum(Z)
		Y = simulategaussianprocess(L)
		Y = âˆš(2Ï€)Y
		Z = max.(Z, Î¶ * Y)
		E = randexp(T)
		Î¶â»Â¹ += E
		Î¶ = 1 / Î¶â»Â¹
	end

	# Lenzi et al. used the log transform to stablise the variance, and this can
	# help avoid neural network collapse. Note that there is also a theoretical
	# justification for this transformation; it transforms from the data from
	# the unit FrÃ©chet scale to the Gumbel scale, which is typically better behaved.
	Z = log.(Z) # TODO decide if this is what we want to do; can add an arguement transform::Bool = true.

	return Z
end

function simulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5) where T <: Number
	n = size(L, 1)
	Z = similar(L, n, m)
	for h âˆˆ 1:m
		Z[:, h] = simulateschlather(L, C = C)
	end

	return Z
end


# ---- Conditional extremes ----

a(h, z; Î», Îº) = z * exp(-(h / Î»)^Îº)
b(h, z; Î², Î», Îº) = 1 + a(h, z, Î» = Î», Îº = Îº)^Î²
delta(h; Î´â‚) = 1 + exp(-(h / Î´â‚)^2)

CÌƒ(h, Ï, Î½) = matern(h, Ï, Î½)
ÏƒÌƒâ‚€(h, Ï, Î½) = âˆš(2 - 2 * CÌƒ(h, Ï, Î½))

Î¦(q::T) where T <: Number = cdf(Normal(zero(T), one(T)), q)
t(yÌƒâ‚€â‚, Î¼, Ï„, Î´) = Fâ‚›â»Â¹(Î¦(yÌƒâ‚€â‚), Î¼, Ï„, Î´)


"""
	simulateconditionalextremes(Î¸::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, sâ‚€_idx::Integer, u::T) where T <: Number
	simulateconditionalextremes(Î¸::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, sâ‚€_idx::Integer, u::T, m::Integer) where T <: Number

Simulates from the spatial conditional extremes model for parameters.

# Examples
S = rand(Float32, 10, 2)
D = [norm(sáµ¢ - sâ±¼) for sáµ¢ âˆˆ eachrow(S), sâ±¼ in eachrow(S)]
L = maternchols(D, 0.6f0, 0.5f0)
sâ‚€ = S[1, :]'
h = map(norm, eachslice(S .- sâ‚€, dims = 1))
sâ‚€_idx = findfirst(x -> x == 0.0, h)
u = 0.7f0
simulateconditionalextremes(Î¸, L[:, :, 1], h, sâ‚€_idx, u)
"""
function simulateconditionalextremes(
	Î¸::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, sâ‚€_idx::Integer, u::T, m::Integer
	) where T <: Number

	n = size(L, 1)
	Z = similar(L, n, m)
	for k âˆˆ 1:m
		Z[:, k] = simulateconditionalextremes(Î¸, L, h, sâ‚€_idx, u)
	end

	return Z
end


function simulateconditionalextremes(
	Î¸::AbstractVector{T}, L::AbstractArray{T, 2}, h::AbstractVector{T}, sâ‚€_idx::Integer, u::T
	) where T <: Number

	@assert length(Î¸) == 8
	@assert sâ‚€_idx > 0
	@assert sâ‚€_idx <= length(h)
	@assert size(L, 1) == size(L, 2)
	@assert size(L, 1) == length(h)

	# Parameters associated with a(.) and b(.):
	Îº = Î¸[1]
	Î» = Î¸[2]
	Î² = Î¸[3]
	# Covariance parameters associated with the Gaussian process
	Ï = Î¸[4]
	Î½ = Î¸[5]
	# Location and scale parameters for the residual process
	Î¼ = Î¸[6]
	Ï„ = Î¸[7]
	Î´â‚ = Î¸[8]

	# Construct the parameter Î´ used in the Subbotin distribution:
	Î´ = delta.(h, Î´â‚ = Î´â‚)

	# Observed datum at the conditioning site, Zâ‚€:
	Zâ‚€ = u + randexp(T)

	# Simulate a mean-zero Gaussian random field with unit marginal variance,
    # independently of Zâ‚€. Note that YÌƒ inherits the order of L. Therefore, we
	# can use sâ‚€_idx to access sâ‚€ in all subsequent vectors.
	YÌƒ  = simulategaussianprocess(L)

	# Adjust the Gaussian process so that it is 0 at sâ‚€
	YÌƒâ‚€ = YÌƒ .- YÌƒ[sâ‚€_idx]

	# Transform to unit variance:
	# ÏƒÌƒâ‚€ = sqrt.(2 .- 2 *  matern.(h, Ï, Î½))
	YÌƒâ‚€â‚ = YÌƒâ‚€ ./ ÏƒÌƒâ‚€.(h, Ï, Î½)
	YÌƒâ‚€â‚[sâ‚€_idx] = zero(T) # avoid pathology by setting YÌƒâ‚€â‚(sâ‚€) = 0.

	# Probability integral transform from the standard Gaussian scale to the
	# standard uniform scale, and then inverse probability integral transform
	# from the standard uniform scale to the Subbotin scale:
    Y = t.(YÌƒâ‚€â‚, Î¼, Ï„, Î´)

	# Apply the functions a(â‹…) and b(â‹…) to simulate data throughout the domain:
	Z = a.(h, Zâ‚€, Î» = Î», Îº = Îº) + b.(h, Zâ‚€, Î² = Î², Î» = Î», Îº = Îº) .* Y

	# Variance stabilising transform
	Z = cbrt.(Z) # TODO decide if this is what we want to do; can add an arguement transform::Bool = true.

	return Z
end




# ---- Miscellaneous functions ----

@doc raw"""
    matern(h, Ï, Î½, ÏƒÂ² = 1)
For two points separated by `h` units, compute the MatÃ©rn covariance function
with range `Ï`, smoothness `Î½`, and marginal variance `ÏƒÂ²`.

We use the parametrisation
``C(\mathbf{h}) = \sigma^2 \frac{2^{1 - \nu}}{\Gamma(\nu)} \left(\frac{\|\mathbf{h}\|}{\rho}\right) K_\nu \left(\frac{\|\mathbf{h}\|}{\rho}\right)``,
where ``\Gamma(\cdot)`` is the gamma function, and ``K_\nu(\cdot)`` is the modified Bessel
function of the second kind of order ``\nu``. This parameterisation is the same as used by the `R`
package `fields`, but differs to the parametrisation given by Wikipedia.

Note that the `Julia` functions for ``\Gamma(\cdot)`` and ``K_\nu(\cdot)``, respectively `gamma()` and
`besselk()`, do not work on the GPU and, hence, nor does `matern()`.
"""
function matern(h, Ï, Î½, ÏƒÂ² = one(typeof(h)))

	@assert h >= 0 "h should be non-negative"
	@assert Ï > 0 "Ï should be positive"
	@assert Î½ > 0 "Î½ should be positive"

	if h == 0
        C = ÏƒÂ²
    else
		d = h / Ï
        C = ÏƒÂ² * ((2^(1 - Î½)) / gamma(Î½)) * d^Î½ * besselk(Î½, d)
    end
    return C
end

matern(h, Ï) =  matern(h, Ï, 1.0)

"""
    maternchols(D, Ï, Î½)
Given a distance matrix `D`, computes the covariance matrix Î£ under the
MatÃ©rn covariance function with range `Ï` and smoothness `Î½`, and
return the Cholesky factor of this matrix.

Providing vectors for `Ï` and `Î½` will yield a three-dimensional array of
Cholesky factors.
"""
function maternchols(D, Ï, Î½)
	L = [cholesky(Symmetric(matern.(D, Ï[i], Î½[i]))).L  for i âˆˆ eachindex(Ï)]
	L = convert.(Array, L) # TODO Would be better if stackarrays() could handle other classes. Maybe it would work if I remove the type from stackarrays()
	L = stackarrays(L, merge = false)
	return L
end

"""
    _incgammalowerunregularised(a, x)
For positive `a` and `x`, computes the lower unregularised incomplete gamma
function, ``\\gamma(a, x) = \\int_{0}^x t^{a-1}e^{-t}dt``.
"""
_incgammalowerunregularised(a, x) = incgamma(a, x; upper = false, reg = false)


"""
	objectindices(objects, Î¸::AbstractMatrix{T}) where T
Returns a vector of indices giving element of `objects` associated with each
parameter configuration in `Î¸`.

The number of parameter configurations, `K = size(Î¸, 2)`, must be a multiple of
the number of objects, `N = size(objects)[end]`. Further, repeated parameters
used to generate `objects` must be stored in `Î¸` after using the `inner` keyword
argument of `repeat()` (see example below).

# Examples
```
K = 6
N = 3
Ïƒâ‚‘ = rand(K)
Ï = rand(N)
Î½ = rand(N)
S = expandgrid(1:9, 1:9)
D = pairwise(Euclidean(), S, S, dims = 1)
L = maternchols(D, Ï, Î½)
Ï = repeat(Ï, inner = K Ã· N)
Î½ = repeat(Î½, inner = K Ã· N)
Î¸ = hcat(Ïƒâ‚‘, Ï, Î½)'
objectindices(L, Î¸)
```
"""
function objectindices(objects, Î¸::AbstractMatrix{T}) where T

	K = size(Î¸, 2)
	N = size(objects)[end]
	@assert K % N == 0 "The nu"

	return repeat(1:N, inner = K Ã· N)
end
