"""
	NeuralEstimator
An abstract supertype for all neural estimators.
"""
abstract type NeuralEstimator end

"""
	BayesEstimator <: NeuralEstimator
An abstract supertype for neural Bayes estimators.
"""
abstract type BayesEstimator <: NeuralEstimator end

"""
	PointEstimator <: BayesEstimator
    PointEstimator(network)
A neural point estimator, where the neural `network` is a mapping from the sample space to the parameter space.
"""
struct PointEstimator{N} <: BayesEstimator
    network::N
end
(estimator::PointEstimator)(Z) = estimator.network(Z)

@doc raw"""
	IntervalEstimator <: BayesEstimator
	IntervalEstimator(u, v = u, c::Union{Function, Compress} = identity; probs = [0.025, 0.975], g = exp)
	IntervalEstimator(u, c::Union{Function, Compress}; probs = [0.025, 0.975], g = exp)
A neural estimator that jointly estimates marginal posterior credible intervals based on the probability levels `probs` (by default, 95% central credible intervals).

The estimator employs a representation that prevents quantile crossing. Specifically, given data ``\boldsymbol{Z}``, 
it constructs intervals for each parameter
``\theta_i``, ``i = 1, \dots, d,``  of the form,
```math
[c_i(u_i(\boldsymbol{Z})), \;\; c_i(u_i(\boldsymbol{Z})) + g(v_i(\boldsymbol{Z})))],
```
where  ``\boldsymbol{u}(â‹…) \equiv (u_1(\cdot), \dots, u_d(\cdot))'`` and
``\boldsymbol{v}(â‹…) \equiv (v_1(\cdot), \dots, v_d(\cdot))'`` are neural networks
that map from the sample space to ``\mathbb{R}^d``; $g(\cdot)$ is a
monotonically increasing function (e.g., exponential or softplus); and each
``c_i(â‹…)`` is a monotonically increasing function that maps its input to the
prior support of ``\theta_i``.

The functions ``c_i(â‹…)`` may be collectively defined by a ``d``-dimensional [`Compress`](@ref) object, which can constrain the interval estimator's output to the prior support. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted. If only a single neural-network architecture is provided, it will be used for both ``\boldsymbol{u}(â‹…)`` and ``\boldsymbol{v}(â‹…)``.

The return value when applied to data using [`estimate`()](@ref) is a matrix with ``2d`` rows, where the first and second ``d`` rows correspond to the lower and upper bounds, respectively. The function [`interval()`](@ref) can be used to format this output in a readable ``d`` Ã— 2 matrix.  

See also [`QuantileEstimator`](@ref).

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¼,Ïƒ ~ N(Î¼, ÏƒÂ²) with priors Î¼ ~ U(0, 1) and Ïƒ ~ U(0, 1)
d = 2     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 100   # number of independent replicates
sample(K) = rand32(d, K)
simulate(Î¸, m) = [Ï‘[1] .+ Ï‘[2] .* randn(n, m) for Ï‘ in eachcol(Î¸)]

# Neural network
w = 128   # width of each hidden layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu))
Ï• = Chain(Dense(w, w, relu), Dense(w, d))
u = DeepSet(Ïˆ, Ï•)

# Initialise the estimator
estimator = IntervalEstimator(u)

# Train the estimator
estimator = train(estimator, sample, simulate, m = m)

# Inference with "observed" data 
Î¸ = [0.8f0; 0.1f0]
Z = simulate(Î¸, m)
estimate(estimator, Z) 
interval(estimator, Z)
```
"""
struct IntervalEstimator{N, H, C, G} <: BayesEstimator
    u::N
    v::N
    c::C
    probs::H
    g::G
end
function IntervalEstimator(u, v = u, c::Union{Function, Compress} = identity; probs = [0.025, 0.975], g = exp)
    if !isa(probs, AbstractArray)
        probs = [probs]
    end
    @assert all(0 .< probs .< 1)
    IntervalEstimator(deepcopy(u), deepcopy(v), c, probs, g)
end
IntervalEstimator(u, c::Union{Function, Compress}; kwargs...) = IntervalEstimator(deepcopy(u), deepcopy(u), c; kwargs...)
Flux.trainable(est::IntervalEstimator) = (u = est.u, v = est.v)
function (est::IntervalEstimator)(Z)
    bâ‚— = est.u(Z)                # lower bound
    báµ¤ = bâ‚— .+ est.g.(est.v(Z))  # upper bound
    vcat(est.c(bâ‚—), est.c(báµ¤))
end

@doc raw"""
	QuantileEstimator <: BayesEstimator
	QuantileEstimator(v; probs = [0.025, 0.5, 0.975], g = Flux.softplus, i = nothing)
A neural estimator that jointly estimates a fixed set of marginal posterior
quantiles, with probability levels $\{\tau_1, \dots, \tau_T\}$ controlled by the
keyword argument `probs`. This generalises [`IntervalEstimator`](@ref) to support an arbitrary number of probability levels. 

Given data ``\boldsymbol{Z}``, by default the estimator approximates quantiles of the distributions of 
```math
\theta_i \mid \boldsymbol{Z}, \quad i = 1, \dots, d, 
```
for parameters $\boldsymbol{\theta} \equiv (\theta_1, \dots, \theta_d)'$.
Alternatively, if initialised with `i` set to a positive integer, the estimator approximates quantiles of
the full conditional distribution of  
```math
\theta_i \mid \boldsymbol{Z}, \boldsymbol{\theta}_{-i},
```
where $\boldsymbol{\theta}_{-i}$ denotes the parameter vector with its $i$th
element removed. 

The estimator employs a representation that prevents quantile crossing, namely,
```math
\begin{aligned}
\boldsymbol{q}^{(\tau_1)}(\boldsymbol{Z}) &= \boldsymbol{v}^{(\tau_1)}(\boldsymbol{Z}),\\
\boldsymbol{q}^{(\tau_t)}(\boldsymbol{Z}) &= \boldsymbol{v}^{(\tau_1)}(\boldsymbol{Z}) + \sum_{j=2}^t g(\boldsymbol{v}^{(\tau_j)}(\boldsymbol{Z})), \quad t = 2, \dots, T,
\end{aligned}
```
where $\boldsymbol{q}^{(\tau)}(\boldsymbol{Z})$ denotes the vector of $\tau$-quantiles 
for parameters $\boldsymbol{\theta} \equiv (\theta_1, \dots, \theta_d)'$; 
$\boldsymbol{v}^{(\tau_t)}(\cdot)$, $t = 1, \dots, T$, are neural networks
that map from the sample space to ``\mathbb{R}^d``; and $g(\cdot)$ is a
monotonically increasing function (e.g., exponential or softplus) applied elementwise to
its arguments. If `g = nothing`, the quantiles are estimated independently through the representation
```math
\boldsymbol{q}^{(\tau_t)}(\boldsymbol{Z}) = \boldsymbol{v}^{(\tau_t)}(\boldsymbol{Z}), \quad t = 1, \dots, T.
```

When the neural networks are [`DeepSet`](@ref) objects, two requirements must be met. 
First, the number of input neurons in the first layer of the outer network must equal the number of
neurons in the final layer of the inner network plus $\text{dim}(\boldsymbol{\theta}_{-i})$, where we define 
$\text{dim}(\boldsymbol{\theta}_{-i}) \equiv 0$ when targetting marginal posteriors of the form $\theta_i \mid \boldsymbol{Z}$ (the default behaviour). 
Second, the number of output neurons in the final layer of the outer network must equal $d - \text{dim}(\boldsymbol{\theta}_{-i})$. 

The return value is a matrix with $\{d - \text{dim}(\boldsymbol{\theta}_{-i})\} \times T$ rows, where the
first ``T`` rows correspond to the estimated quantiles for the first
parameter, the second ``T`` rows corresponds to the estimated quantiles for the second parameter, and so on.

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¼,Ïƒ ~ N(Î¼, ÏƒÂ²) with priors Î¼ ~ U(0, 1) and Ïƒ ~ U(0, 1)
d = 2     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 30    # number of independent replicates in each data set
sample(K) = rand32(d, K)
simulate(Î¸, m) = [Ï‘[1] .+ Ï‘[2] .* randn32(n, m) for Ï‘ in eachcol(Î¸)]

# ---- Quantiles of Î¸áµ¢ âˆ£ ğ™, i = 1, â€¦, d ----

# Neural network
w = 64   # width of each hidden layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu))
Ï• = Chain(Dense(w, w, relu), Dense(w, d))
v = DeepSet(Ïˆ, Ï•)

# Initialise the estimator
estimator = QuantileEstimator(v)

# Train the estimator
estimator = train(estimator, sample, simulate, m = m)

# Inference with "observed" data 
Î¸ = [0.8f0; 0.1f0]
Z = simulate(Î¸, m)
estimate(estimator, Z) 

# ---- Quantiles of Î¸áµ¢ âˆ£ ğ™, Î¸â‚‹áµ¢ ----

# Neural network
w = 64  # width of each hidden layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu))
Ï• = Chain(Dense(w + 1, w, relu), Dense(w, d - 1))
v = DeepSet(Ïˆ, Ï•)

# Initialise estimators respectively targetting quantiles of Î¼âˆ£Z,Ïƒ and Ïƒâˆ£Z,Î¼
qâ‚ = QuantileEstimator(v; i = 1)
qâ‚‚ = QuantileEstimator(v; i = 2)

# Train the estimators
qâ‚ = train(qâ‚, sample, simulate, m = m)
qâ‚‚ = train(qâ‚‚, sample, simulate, m = m)

# Estimate quantiles of Î¼âˆ£Z,Ïƒ with Ïƒ = 0.5 and for many data sets
Î¸â‚‹áµ¢ = 0.5f0
qâ‚(Z, Î¸â‚‹áµ¢)

# Estimate quantiles of Î¼âˆ£Z,Ïƒ with Ïƒ = 0.5 for a single data set
qâ‚(Z[1], Î¸â‚‹áµ¢)
```
"""
struct QuantileEstimator{V, P, G, I} <: BayesEstimator #TODO function for neat output as dxT matrix like interval() 
    v::V
    probs::P
    g::G
    i::I
end
function QuantileEstimator(v; probs = [0.025, 0.5, 0.975], g = Flux.softplus, i::Union{Integer, Nothing} = nothing)
    if !isa(probs, AbstractArray)
        probs = [probs]
    end
    @assert all(0 .< probs .< 1)
    if !isnothing(i)
        @assert i > 0
    end
    QuantileEstimator(deepcopy.(repeat([v], length(probs))), probs, g, i)
end
Flux.trainable(est::QuantileEstimator) = (v = est.v,)
function (est::QuantileEstimator)(input) # input might be Z, or a tuple (Z, Î¸â‚‹áµ¢)

    # Apply each neural network to Z
    v = map(est.v) do v
        v(input)
    end

    # If g is specified, impose monotonicity
    if isnothing(est.g)
        q = v
    else
        gv = broadcast.(est.g, v[2:end])
        q = cumsum([v[1], gv...])
    end

    # Convert to matrix
    reduce(vcat, q)
end
# user-level convenience methods (not used internally) for full conditional estimation
function (est::QuantileEstimator)(Z, Î¸â‚‹áµ¢::Vector)
    i = est.i
    @assert !isnothing(i) "slot i must be specified when approximating a full conditional"
    if isa(Z, Vector) # repeat Î¸â‚‹áµ¢ to match the number of data sets
        Î¸â‚‹áµ¢ = [Î¸â‚‹áµ¢ for _ in eachindex(Z)]
    end
    est((Z, Î¸â‚‹áµ¢))  # "Tupleise" the input and apply the estimator
end
(est::QuantileEstimator)(Z, Î¸â‚‹áµ¢::Number) = est(Z, [Î¸â‚‹áµ¢])
const QuantileEstimatorDiscrete = QuantileEstimator # alias

# function posterior(Z; Î¼â‚€ = 0, Ïƒâ‚€ = 1, ÏƒÂ² = 1)
# 	Î¼Ìƒ = (1/Ïƒâ‚€^2 + length(Z)/ÏƒÂ²)^-1 * (Î¼â‚€/Ïƒâ‚€^2 + sum(Z)/ÏƒÂ²)
# 	ÏƒÌƒ = sqrt((1/Ïƒâ‚€^2 + length(Z)/ÏƒÂ²)^-1)
# 	Normal(Î¼Ìƒ, ÏƒÌƒ)
# end

# ; and see [`QuantileEstimatorContinuous`](@ref) for estimating posterior quantiles based on a continuous probability level provided as input to the neural network.
@doc raw"""
	QuantileEstimatorContinuous <: BayesEstimator
	QuantileEstimatorContinuous(network; i = nothing, num_training_probs::Integer = 1)
A neural estimator that estimates marginal posterior quantiles, with the probability level `Ï„` given as input to the neural network.

Given data $\boldsymbol{Z}$ and the desired probability level 
$\tau âˆˆ (0, 1)$, by default the estimator approximates the $\tau$-quantile of the distributions of 
```math
\theta_i \mid \boldsymbol{Z}, \quad i = 1, \dots, d, 
```
for parameters $\boldsymbol{\theta} \equiv (\theta_1, \dots, \theta_d)'$.
Alternatively, if initialised with `i` set to a positive integer, the estimator
approximates the $\tau$-quantile of the full conditional distribution of 
```math
\theta_i \mid \boldsymbol{Z}, \boldsymbol{\theta}_{-i},
```
where $\boldsymbol{\theta}_{-i}$ denotes the parameter vector with its $i$th element removed. 

Although not a requirement, one may employ a (partially) monotonic neural
network to prevent quantile crossing (i.e., to ensure that the
$\tau_1$-quantile does not exceed the $\tau_2$-quantile for any
$\tau_2 > \tau_1$). There are several ways to construct such a neural network:
one simple yet effective approach is to ensure that all weights associated with
$\tau$ are strictly positive
(see, e.g., [Cannon, 2018](https://link.springer.com/article/10.1007/s00477-018-1573-6)),
and this can be done using the [`DensePositive`](@ref) layer as shown in the example below.

When the neural network is a [`DeepSet`](@ref), two requirements must be met. First, the number of input neurons in the first layer of the outer network must equal the number of
neurons in the final layer of the inner network plus $1 + \text{dim}(\boldsymbol{\theta}_{-i})$, where we define 
$\text{dim}(\boldsymbol{\theta}_{-i}) \equiv 0$ when targetting marginal posteriors of the form $\theta_i \mid \boldsymbol{Z}$ (the default behaviour). 
Second, the number of output neurons in the final layer of the outer network must equal $d - \text{dim}(\boldsymbol{\theta}_{-i})$. 

The return value is a matrix with $d - \text{dim}(\boldsymbol{\theta}_{-i})$ rows,
corresponding to the estimated quantile for each parameter not in $\boldsymbol{\theta}_{-i}$.

See also [`QuantileEstimator`](@ref).

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¼,Ïƒ ~ N(Î¼, ÏƒÂ²) with priors Î¼ ~ U(0, 1) and Ïƒ ~ U(0, 1)
d = 2     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 30    # number of independent replicates in each data set
sample(K) = rand32(d, K)
simulateZ(Î¸, m) = [Ï‘[1] .+ Ï‘[2] .* randn32(n, m) for Ï‘ in eachcol(Î¸)]
simulateÏ„(K)    = [rand32(10) for k in 1:K]
simulate(Î¸, m)  = simulateZ(Î¸, m), simulateÏ„(size(Î¸, 2))

# ---- Quantiles of Î¸áµ¢ âˆ£ ğ™, i = 1, â€¦, d ----

# Neural network: partially monotonic network to preclude quantile crossing
w = 64  # width of each hidden layer
Ïˆ = Chain(
	Dense(n, w, relu),
	Dense(w, w, relu),
	Dense(w, w, relu)
	)
Ï• = Chain(
	DensePositive(Dense(w + 1, w, relu); last_only = true),
	DensePositive(Dense(w, w, relu)),
	DensePositive(Dense(w, d))
	)
network = DeepSet(Ïˆ, Ï•)

# Initialise the estimator
qÌ‚ = QuantileEstimatorContinuous(network)

# Train the estimator
qÌ‚ = train(qÌ‚, sample, simulate, m = m)

# Test data 
Î¸ = sample(1000)
Z = simulateZ(Î¸, m)

# Estimate 0.1-quantile for each parameter and for many data sets
Ï„ = 0.1f0
qÌ‚(Z, Ï„)

# Estimate multiple quantiles for each parameter and for many data sets
# (note that Ï„ is given as a row vector)
Ï„ = f32([0.1, 0.25, 0.5, 0.75, 0.9])'
qÌ‚(Z, Ï„)

# Estimate multiple quantiles for a single data set 
qÌ‚(Z[1], Ï„)

# ---- Quantiles of Î¸áµ¢ âˆ£ ğ™, Î¸â‚‹áµ¢ ----

# Neural network: partially monotonic network to preclude quantile crossing
w = 64  # width of each hidden layer
Ïˆ = Chain(
	Dense(n, w, relu),
	Dense(w, w, relu),
	Dense(w, w, relu)
	)
Ï• = Chain(
	DensePositive(Dense(w + 2, w, relu); last_only = true),
	DensePositive(Dense(w, w, relu)),
	DensePositive(Dense(w, d - 1))
	)
network = DeepSet(Ïˆ, Ï•)

# Initialise the estimator targetting Î¼âˆ£Z,Ïƒ
i = 1
qÌ‚áµ¢ = QuantileEstimatorContinuous(network; i = i)

# Train the estimator
qÌ‚áµ¢ = train(qÌ‚áµ¢, prior, simulate, m = m)

# Test data 
Î¸ = sample(1000)
Z = simulateZ(Î¸, m)

# Estimate quantiles of Î¼âˆ£Z,Ïƒ with Ïƒ = 0.5 and for many data sets
# (can use Î¸[InvertedIndices.Not(i), :] to determine the order in which the conditioned parameters should be given)
Î¸â‚‹áµ¢ = 0.5f0
Ï„ = f32([0.1, 0.25, 0.5, 0.75, 0.9])
qÌ‚áµ¢(Z, Î¸â‚‹áµ¢, Ï„)

# Estimate quantiles of Î¼âˆ£Z,Ïƒ with Ïƒ = 0.5 and for a single data set
qÌ‚áµ¢(Z[1], Î¸â‚‹áµ¢, Ï„)
```
"""
struct QuantileEstimatorContinuous{N, I} <: NeuralEstimator
    network::N
    i::I
end
function QuantileEstimatorContinuous(network; i::Union{Integer, Nothing} = nothing)
    if !isnothing(i)
        @assert i > 0
    end
    QuantileEstimatorContinuous(network, i)
end
# core method (used internally)
(est::QuantileEstimatorContinuous)(tup::Tuple) = est.network(tup)
# user-level convenience functions (not used internally)
function (est::QuantileEstimatorContinuous)(Z, Ï„)
    if !isnothing(est.i)
        error("To estimate the Ï„-quantile of the full conditional Î¸áµ¢|Z,Î¸â‚‹áµ¢ the call should be of the form estimator(Z, Î¸â‚‹áµ¢, Ï„)")
    end
    est((Z, Ï„)) # "Tupleise" input and pass to Tuple method
end
function (est::QuantileEstimatorContinuous)(Z, Ï„::Number)
    est(Z, [Ï„])
end
function (est::QuantileEstimatorContinuous)(Z::V, Ï„::Number) where {V <: AbstractVector{A}} where {A}
    est(Z, repeat([[Ï„]], length(Z)))
end
# user-level convenience functions (not used internally) for full conditional estimation
function (est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Matrix, Ï„::Matrix)
    i = est.i
    @assert !isnothing(i) "slot i must be specified when approximating a full conditional"
    if size(Î¸â‚‹áµ¢, 2) != size(Ï„, 2)
        @assert size(Î¸â‚‹áµ¢, 2) == 1 "size(Î¸â‚‹áµ¢, 2)=$(size(Î¸â‚‹áµ¢, 2)) and size(Ï„, 2)=$(size(Ï„, 2)) do not match"
        Î¸â‚‹áµ¢ = repeat(Î¸â‚‹áµ¢, outer = (1, size(Ï„, 2)))
    end
    Î¸â‚‹áµ¢Ï„ = vcat(Î¸â‚‹áµ¢, Ï„) # combine parameters and probability level into single pxK matrix
    q = est((Z, Î¸â‚‹áµ¢Ï„))  # "Tupleise" the input and pass to tuple method
    if !isa(q, Vector)
        q = [q]
    end
    reduce(hcat, permutedims.(q))
end
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Matrix, Ï„::Vector) = est(Z, Î¸â‚‹áµ¢, permutedims(reduce(vcat, Ï„)))
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Matrix, Ï„::Number) = est(Z, Î¸â‚‹áµ¢, repeat([Ï„], size(Î¸â‚‹áµ¢, 2)))
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Vector, Ï„::Vector) = est(Z, reshape(Î¸â‚‹áµ¢, :, 1), permutedims(Ï„))
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Vector, Ï„::Number) = est(Z, Î¸â‚‹áµ¢, [Ï„])
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Number, Ï„::Number) = est(Z, [Î¸â‚‹áµ¢], Ï„)
(est::QuantileEstimatorContinuous)(Z, Î¸â‚‹áµ¢::Number, Ï„::Vector) = est(Z, [Î¸â‚‹áµ¢], Ï„)

@doc raw"""
	PosteriorEstimator <: NeuralEstimator
	PosteriorEstimator(network, q::ApproximateDistribution)
    PosteriorEstimator(network, d::Integer, dstar::Integer = d; q::ApproximateDistribution = NormalisingFlow, kwargs...)
A neural estimator that approximates the posterior distribution $p(\boldsymbol{\theta} \mid \boldsymbol{Z})$, based on a neural `network` and an approximate distribution `q` (see the available in-built [Approximate distributions](@ref)). 

The neural `network` is a mapping from the sample space to a space determined by the chosen approximate distribution `q`. Often, the output space is the space $\mathcal{K}$ of the approximate-distribution parameters $\boldsymbol{\kappa}$. However, for certain distributions (notably, [`NormalisingFlow`](@ref)), the neural network outputs summary statistics of suitable dimension (e.g., the dimension $d$ of the parameter vector), which are then transformed into parameters of the approximate distribution using conventional multilayer perceptrons (see [`NormalisingFlow`](@ref)).

The convenience constructor `PosteriorEstimator(network, d::Integer, dstar::Integer = d)` builds the approximate distribution automatically, with the keyword arguments passed onto the approximate-distribution constructor.  

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¼,Ïƒ ~ N(Î¼, ÏƒÂ²) with priors Î¼ ~ U(0, 1) and Ïƒ ~ U(0, 1)
d = 2     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 30    # number of independent replicates in each data set
sample(K) = rand32(d, K)
simulate(Î¸, m) = [Ï‘[1] .+ Ï‘[2] .* randn32(n, m) for Ï‘ in eachcol(Î¸)]

# Distribution used to approximate the posterior 
q = NormalisingFlow(d, d) 

# Neural network (outputs d summary statistics)
w = 128   
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))
Ï• = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, d))
network = DeepSet(Ïˆ, Ï•)

# Initialise the estimator
estimator = PosteriorEstimator(network, q)

# Train the estimator
estimator = train(estimator, sample, simulate, m = m)

# Inference with observed data 
Î¸ = [0.8f0 0.1f0]'
Z = simulate(Î¸, m)
sampleposterior(estimator, Z) # posterior draws 
posteriormean(estimator, Z)   # point estimate
```
"""
struct PosteriorEstimator{Q, N} <: NeuralEstimator
    q::Q
    network::N
end
numdistributionalparams(estimator::PosteriorEstimator) = numdistributionalparams(estimator.q)
logdensity(estimator::PosteriorEstimator, Î¸, Z) = logdensity(estimator.q, f32(Î¸), estimator.network(f32(Z)))
(estimator::PosteriorEstimator)(ZÎ¸::Tuple) = logdensity(estimator, ZÎ¸[2], ZÎ¸[1]) # internal method only used during training # TODO not ideal that we assume an ordering here

# Convenience constructor
function PosteriorEstimator(network, d::Integer, dstar::Integer = d; q = NormalisingFlow, kwargs...)

    # Convert string to type if needed
    q = if q isa String
        # Get the type from the string name
        getfield(@__MODULE__, Symbol(q))
    else
        q
    end

    # Distribution used to approximate the posterior 
    q = q(d, dstar; kwargs...) 

    # Initialise the estimator
    return PosteriorEstimator(q, network)
end

# Constructor for consistent argument ordering
function PosteriorEstimator(network, q::A) where A <: ApproximateDistribution
    return PosteriorEstimator(q, network)
end




#TODO maybe its better to not have a tuple, and just allow the arguments to be passed as normal... Just have to change DeepSet definition to allow two arguments in some places (this is more natural). Can easily allow backwards compat in this case too. 
@doc raw"""
	RatioEstimator <: NeuralEstimator
	RatioEstimator(network)
A neural estimator that estimates the likelihood-to-evidence ratio,
```math
r(\boldsymbol{Z}, \boldsymbol{\theta}) \equiv p(\boldsymbol{Z} \mid \boldsymbol{\theta})/p(\boldsymbol{Z}),
```
where $p(\boldsymbol{Z} \mid \boldsymbol{\theta})$ is the likelihood and $p(\boldsymbol{Z})$
is the marginal likelihood, also known as the model evidence.

For numerical stability, training is done on the log-scale using the relation 
$\log r(\boldsymbol{Z}, \boldsymbol{\theta}) = \text{logit}(c^*(\boldsymbol{Z}, \boldsymbol{\theta}))$, 
where $c^*(\cdot, \cdot)$ denotes the Bayes classifier as described in the [methodology](@ref "Neural ratio estimators") section. 
Hence, the neural network should be a mapping from $\mathcal{Z} \times \Theta$ to $\mathbb{R}$, 
where $\mathcal{Z}$ and $\Theta$ denote the sample and parameter spaces, respectively. 

!!! note "Network input"
    The neural network must implement a method `network(::Tuple)`, where the first element of the tuple contains the data sets and the second element contains the parameter matrices.  

When the neural network is a [`DeepSet`](@ref) (which implements the above method), two requirements must be met. First, the number of input neurons in the first layer of the outer network must equal $d$ plus the number of output neurons in the final layer of the inner network. Second, the number of output neurons in the final layer of the outer network must be one.

When applying the estimator to data, the log of the likelihood-to-evidence ratio is returned. 
The estimated ratio can then be used in various Bayesian
(e.g., [Hermans et al., 2020](https://proceedings.mlr.press/v119/hermans20a.html))
or frequentist
(e.g., [Walchessen et al., 2024](https://doi.org/10.1016/j.spasta.2024.100848))
inferential algorithms.

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¼,Ïƒ ~ N(Î¼, ÏƒÂ²) with priors Î¼ ~ U(0, 1) and Ïƒ ~ U(0, 1)
d = 2     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 30    # number of independent replicates in each data set
sample(K) = rand32(d, K)
simulate(Î¸, m) = [Ï‘[1] .+ Ï‘[2] .* randn32(n, m) for Ï‘ in eachcol(Î¸)]

# Neural network
w = 128 
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))
Ï• = Chain(Dense(w + d, w, relu), Dense(w, w, relu), Dense(w, 1))
network = DeepSet(Ïˆ, Ï•)

# Initialise the estimator
rÌ‚ = RatioEstimator(network)

# Train the estimator
rÌ‚ = train(rÌ‚, sample, simulate, m = m)

# Generate "observed" data 
Î¸ = sample(1)
z = simulate(Î¸, 200)[1]

# Grid-based optimization and sampling
Î¸_grid = f32(expandgrid(0:0.01:1, 0:0.01:1))'  # fine gridding of the parameter space
estimate(rÌ‚, z, Î¸_grid)                         # log of likelihood-to-evidence ratios
posteriormode(rÌ‚, z; Î¸_grid = Î¸_grid)           # posterior mode 
sampleposterior(rÌ‚, z; Î¸_grid = Î¸_grid)         # posterior samples

# Gradient-based optimization
using Optim
Î¸â‚€ = [0.5, 0.5]                                # initial estimate
posteriormode(rÌ‚, z; Î¸â‚€ = Î¸â‚€)                   # posterior mode 
```
"""
struct RatioEstimator{N} <: NeuralEstimator
    network::N
end
function (estimator::RatioEstimator)(Z, Î¸; kwargs...)
    estimator((Z, Î¸); kwargs...) # "Tupleise" the input and pass to Tuple method
end
function (estimator::RatioEstimator)(ZÎ¸::Tuple)
    logr = estimator.network(ZÎ¸)
    if typeof(logr) <: AbstractVector
        logr = reduce(vcat, logr)
    end
    return logr
end

# function (estimator::RatioEstimator)(ZÎ¸::Tuple; classifier::Bool = false)
#     c = Ïƒ(estimator.network(ZÎ¸))
#     if typeof(c) <: AbstractVector
#         c = reduce(vcat, c)
#     end
#     classifier ? c : c ./ (1 .- c)
# end

# function (estimator::RatioEstimator)(ZÎ¸::Tuple; return_log_ratio::Bool = true, return_classifier::Bool = false)
#     log_ratio = estimator.network(ZÎ¸)
#     if return_log_ratio
#         return log_ratio
#     end

#     c = Ïƒ(log_ratio)
#     if typeof(c) <: AbstractVector
#         c = reduce(vcat, c)
#     end

#     return_classifier ? c : c ./ (1 .- c)
# end

# # Estimate ratio for many data sets and parameter vectors
# Î¸ = sample(1000)
# Z = simulate(Î¸, m)
# rÌ‚(Z, Î¸)                                   # log of the likelihood-to-evidence ratios



@doc raw"""
	PiecewiseEstimator <: NeuralEstimator
	PiecewiseEstimator(estimators::Vector{BayesEstimator}, changepoints::Vector{Integer})
Creates a piecewise estimator
([Sainsbury-Dale et al., 2024](https://www.tandfonline.com/doi/full/10.1080/00031305.2023.2249522), Sec. 2.2.2)
from a collection of neural Bayes `estimators` and sample-size `changepoints`.

This allows different estimators to be applied to different ranges of sample sizes. For instance, you may wish to use one estimator for small samples and another for larger ones. Given changepoints $m_1 < m_2 < \dots < m_{l-1}$, the piecewise estimator selects from $l$ trained estimators based on the observed sample size $m$ as follows:
```math
\hat{\boldsymbol{\theta}}(\boldsymbol{Z})
=
\begin{cases}
\hat{\boldsymbol{\theta}}_1(\boldsymbol{Z}) & m \leq m_1,\\
\hat{\boldsymbol{\theta}}_2(\boldsymbol{Z}) & m_1 < m \leq m_2,\\
\quad \vdots \\
\hat{\boldsymbol{\theta}}_l(\boldsymbol{Z}) & m > m_{l-1}.
\end{cases}
```
where $\hat{\boldsymbol{\theta}}_1(\cdot)$ is a neural Bayes estimator trained to be near-optimal over the range of sample sizes in which it is applied. 

Although this strategy requires training multiple neural networks, it is computationally efficient in practice when combined with pre-training (see [Sainsbury-Dale at al., 2024](https://www.tandfonline.com/doi/full/10.1080/00031305.2023.2249522), Sec 2.3.3), which can be automated using [`trainmultiple()`](@ref). 

# Examples
```julia
using NeuralEstimators, Flux

n = 2    # bivariate data
d = 3    # dimension of parameter vector 
w = 128  # width of each hidden layer

# Small-sample estimator
Ïˆâ‚ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï•â‚ = Chain(Dense(w, w, relu), Dense(w, d));
Î¸Ì‚â‚ = PointEstimator(DeepSet(Ïˆâ‚, Ï•â‚))

# Large-sample estimator
Ïˆâ‚‚ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï•â‚‚ = Chain(Dense(w, w, relu), Dense(w, d));
Î¸Ì‚â‚‚ = PointEstimator(DeepSet(Ïˆâ‚‚, Ï•â‚‚))

# Piecewise estimator with changepoint m=30
Î¸Ì‚ = PiecewiseEstimator([Î¸Ì‚â‚, Î¸Ì‚â‚‚], 30)

# Apply the (untrained) piecewise estimator to data
Z = [rand(n, m) for m âˆˆ (10, 50)]
estimate(Î¸Ì‚, Z)
```
"""
struct PiecewiseEstimator{E, C} <: NeuralEstimator
    estimators::E
    changepoints::C
    function PiecewiseEstimator(estimators, changepoints)
        if isa(changepoints, Number)
            changepoints = [changepoints]
        end
        @assert all(isinteger.(changepoints)) "`changepoints` should contain integers"
        if length(changepoints) != length(estimators) - 1
            error("The length of `changepoints` should be one fewer than the number of `estimators`")
        elseif !issorted(changepoints)
            error("`changepoints` should be in ascending order")
        else
            E = typeof(estimators)
            C = typeof(changepoints)
            new{E, C}(estimators, changepoints)
        end
    end
end
function (estimator::PiecewiseEstimator)(Z)
    changepoints = [estimator.changepoints..., Inf]
    m = numberreplicates(Z)
    Î¸Ì‚ = map(eachindex(Z)) do i
        # find which estimator to use and then apply it
        máµ¢ = m[i]
        j = findfirst(máµ¢ .<= changepoints)
        estimator.estimators[j](Z[[i]])
    end
    return stackarrays(Î¸Ì‚)
end
Base.show(io::IO, estimator::PiecewiseEstimator) = print(io, "\nPiecewise estimator with $(length(estimator.estimators)) estimators and sample size change-points: $(estimator.changepoints)")

"""
	Ensemble <: NeuralEstimator
	Ensemble(estimators)
	Ensemble(architecture::Function, J::Integer)
	(ensemble::Ensemble)(Z; aggr = mean)
Defines an ensembleÂ of `estimators` which, when applied to data `Z`, returns the mean (or another summary defined by `aggr`) of the individual estimates (see, e.g., [Sainsbury-Dale et al., 2025, Sec. S5](https://doi.org/10.48550/arXiv.2501.04330)).

The ensemble can be initialised with a collection of trained `estimators` and then
applied immediately to observed data. Alternatively, the ensemble can be
initialised with a collection of untrained `estimators`
(or a function defining the architecture of each estimator, and the number of estimators in the ensemble),
trained with `train()`, and then applied to observed data. In the latter case, where the ensemble is trained directly,
if `savepath` is specified both the ensemble and component estimators will be saved.

Note that `train()` currently acts sequentially on the component estimators, using the `Adam` optimiser.

The ensemble components can be accessed by indexing the ensemble; the number of component estimators can be obtained using `length()`.

See also [`Parallel`](https://fluxml.ai/Flux.jl/stable/reference/models/layers/#Flux.Parallel), which can be used to mimic ensemble methods with an appropriately chosen `connection`. 

# Examples
```julia
using NeuralEstimators, Flux

# Data Z|Î¸ ~ N(Î¸, 1) with Î¸ ~ N(0, 1)
d = 1     # dimension of the parameter vector Î¸
n = 1     # dimension of each independent replicate of Z
m = 30    # number of independent replicates in each data set
sampler(K) = randn32(d, K)
simulator(Î¸, m) = [Î¼ .+ randn32(n, m) for Î¼ âˆˆ eachcol(Î¸)]

# Neural-network architecture of each ensemble component
function architecture()
	Ïˆ = Chain(Dense(n, 64, relu), Dense(64, 64, relu))
	Ï• = Chain(Dense(64, 64, relu), Dense(64, d))
	network = DeepSet(Ïˆ, Ï•)
	PointEstimator(network)
end

# Initialise ensemble with three component estimators 
ensemble = Ensemble(architecture, 3)
ensemble[1]      # access component estimators by indexing
ensemble[1:2]    # indexing with an iterable collection returns the corresponding ensemble 
length(ensemble) # number of component estimators

# Training
ensemble = train(ensemble, sampler, simulator, m = m, epochs = 5)

# Assessment
Î¸ = sampler(1000)
Z = simulator(Î¸, m)
assessment = assess(ensemble, Î¸, Z)
rmse(assessment)

# Apply to data
ensemble(Z)
```
"""
struct Ensemble{T <: NeuralEstimator} <: NeuralEstimator
    estimators::Vector{T}
end
Ensemble(architecture::Function, J::Integer) = Ensemble([architecture() for j = 1:J])

function (ensemble::Ensemble)(Z; aggr = mean)
    # Collect each estimatorâ€™s output
    Î¸Ì‚s = [estimator(Z) for estimator in ensemble.estimators]

    # Stack into 3D array (d Ã— n Ã— m) where m = number of estimators
    Î¸Ì‚ = stackarrays(Î¸Ì‚s, merge = false)

    # Aggregate elementwise
    if aggr === mean
        Î¸Ì‚ = mean(Î¸Ì‚; dims = 3)
    else
        #NB mapslices doesn't work with Zygote, so use mean as the default
        Î¸Ì‚ = mapslices(aggr, cpu(Î¸Ì‚); dims = 3)
    end

    return dropdims(Î¸Ì‚; dims = 3)
end

Base.getindex(e::Ensemble, i::Integer) = e.estimators[i]
Base.getindex(e::Ensemble, indices::AbstractVector{<:Integer}) = Ensemble(e.estimators[indices])
Base.getindex(e::Ensemble, indices::UnitRange{<:Integer}) = Ensemble(e.estimators[indices])
Base.length(e::Ensemble) = length(e.estimators)
Base.eachindex(e::Ensemble) = eachindex(e.estimators)
Base.show(io::IO, ensemble::Ensemble) = print(io, "\nEnsemble with $(length(ensemble.estimators)) component estimators")
