var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for any model for which simulation is feasible by allowing users to define their model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code illustrates how a user could formulate code for their own model.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"See also Distributions.jl for a range of distributions implemented in Julia, and the package RCall for calling R functions within Julia. ","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussian\n\nsimulatepotts\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussian","page":"Model-specific functions","title":"NeuralEstimators.simulategaussian","text":"simulategaussian(L::AbstractMatrix, m = 1)\n\nSimulates m independent and identically distributed realisations from a mean-zero multivariate Gaussian random vector with associated lower Cholesky  factor L. \n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n\nn = 500\nρ = 0.6\nν = 1.0\nS = rand(n, 2)\nD = pairwise(Euclidean(), S, dims = 1)\nΣ = Symmetric(matern.(D, ρ, ν))\nL = cholesky(Σ).L\nsimulategaussian(L)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulatepotts","page":"Model-specific functions","title":"NeuralEstimators.simulatepotts","text":"simulatepotts(grid::Matrix{Int}, β)\nsimulatepotts(grid::Matrix{Union{Int, Nothing}}, β)\nsimulatepotts(nrows::Int, ncols::Int, num_states::Int, β)\n\nChequerboard Gibbs sampling from a spatial Potts model with parameter β>0 (see, e.g., Sainsbury-Dale et al., 2025, Sec. 3.3, and the references therein).\n\nApproximately independent simulations can be obtained by setting  nsims > 1 or num_iterations > burn. The degree to which the  resulting simulations can be considered independent depends on the  thinning factor (thin) and the burn-in (burn).\n\nKeyword arguments\n\nnsims = 1: number of approximately independent replicates. \nnum_iterations = 2000: number of MCMC iterations.\nburn = num_iterations: burn-in.\nthin = 10: thinning factor.\n\nExamples\n\nusing NeuralEstimators \n\n## Marginal simulation \nβ = 0.8\nsimulatepotts(10, 10, 5, β)\n\n## Marginal simulation: approximately independent samples \nsimulatepotts(10, 10, 5, β; nsims = 100, thin = 10)\n\n## Conditional simulation \nβ = 0.8\ncomplete_grid   = simulatepotts(50, 50, 2, β)        # simulate marginally from the Ising model \nincomplete_grid = removedata(complete_grid, 0.1)     # remove 10% of the pixels at random  \nimputed_grid    = simulatepotts(incomplete_grid, β)  # conditionally simulate over missing pixels\n\n## Multiple conditional simulations \nimputed_grids   = simulatepotts(incomplete_grid, β; num_iterations = 2000, burn = 1000, thin = 10)\n\n## Recreate Fig. 8.8 of Marin & Robert (2007) “Bayesian Core”\nusing Plots \ngrids = [simulatepotts(100, 100, 2, β) for β ∈ 0.3:0.1:1.2]\nheatmaps = heatmap.(grids, legend = false, aspect_ratio=1)\nPlots.plot(heatmaps...)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::Matrix, m = 1; C = 3.5, Gumbel::Bool = false)\n\nSimulates m independent and identically distributed realisations from Schlather's (2002) max-stable model given the lower Cholesky factor L of the covariance matrix of the underlying Gaussian process. \n\nThe function uses the algorithm for approximate simulation given by Schlather (2002).\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are  returned as an nxm matrix.\n\nKeyword arguments\n\nC = 3.5: a tuning parameter that controls the accuracy of the algorithm. Small C favours computational efficiency, while large C favours accuracy. \nGumbel = true: flag indicating whether the data should be log-transformed from the unit Fréchet scale to the Gumbel scale.\n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n\nn = 500\nρ = 0.6\nν = 1.0\nS = rand(n, 2)\nD = pairwise(Euclidean(), S, dims = 1)\nΣ = Symmetric(matern.(D, ρ, ν))\nL = cholesky(Σ).L\nsimulateschlather(L)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Spatial-point-processes","page":"Model-specific functions","title":"Spatial point processes","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"maternclusterprocess","category":"page"},{"location":"API/simulation/#NeuralEstimators.maternclusterprocess","page":"Model-specific functions","title":"NeuralEstimators.maternclusterprocess","text":"maternclusterprocess(; λ=10, μ=10, r=0.1, xmin=0, xmax=1, ymin=0, ymax=1, unit_bounding_box=false)\n\nGenerates a realisation from a Matérn cluster process (e.g., Baddeley et al., 2015, Ch. 12). \n\nThe process is defined by a parent homogenous Poisson point process with intensity λ > 0, a mean number of daughter points μ > 0, and a cluster radius r > 0. The simulation is performed over a rectangular window defined by [xmin, xmax] × [ymin, ymax].\n\nIf unit_bounding_box = true, the simulated points will be scaled so that the longest side of their bounding box is equal to one (this may change the simulation window). \n\nSee also the R package spatstat, which provides functions for simulating from a range of point processes and which can be interfaced from Julia using RCall.\n\nExamples\n\nusing NeuralEstimators\n\n# Simulate a realisation from a Matérn cluster process\nS = maternclusterprocess()\n\n# Visualise realisation (requires UnicodePlots)\nusing UnicodePlots\nscatterplot(S[:, 1], S[:, 2])\n\n# Visualise realisations from the cluster process with varying parameters\nn = 250\nλ = [10, 25, 50, 90]\nμ = n ./ λ\nplots = map(eachindex(λ)) do i\n\tS = maternclusterprocess(λ = λ[i], μ = μ[i])\n\tscatterplot(S[:, 1], S[:, 2])\nend\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Covariance-functions","page":"Model-specific functions","title":"Covariance functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These covariance functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\npaciorek","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, ρ, ν, σ² = 1)\n\nGiven distance boldsymbolh (h), computes the Matérn covariance function\n\nC(boldsymbolh) = sigma^2 frac2^1 - nuGamma(nu) left(fracboldsymbolhrhoright)^nu K_nu left(fracboldsymbolhrhoright)\n\nwhere ρ is a range parameter, ν is a smoothness parameter, σ² is the marginal variance,  Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.paciorek","page":"Model-specific functions","title":"NeuralEstimators.paciorek","text":"paciorek(s, r, ω₁, ω₂, ρ, β)\n\nGiven spatial locations s and r, computes the nonstationary covariance function \n\nC(boldsymbols boldsymbolr) = \nboldsymbolSigma(boldsymbols)^14\nboldsymbolSigma(boldsymbolr)^14\nleftfracboldsymbolSigma(boldsymbols) + boldsymbolSigma(boldsymbolr)2right^-12\nC^0big(sqrtQ(boldsymbols boldsymbolr)big) \n\nwhere C^0(h) = exp-(hrho)^32 for range parameter rho  0,  the matrix boldsymbolSigma(boldsymbols) = exp(betaboldsymbols - boldsymbolomega)boldsymbolI  is a kernel matrix (Paciorek and Schervish, 2006)  with scale parameter beta  0 and reference point boldsymbolomega equiv (omega_1 omega_2) in mathbbR^2, and \n\nQ(boldsymbols boldsymbolr) = \n(boldsymbols - boldsymbolr)\nleft(fracboldsymbolSigma(boldsymbols) + boldsymbolSigma(boldsymbolr)2right)^-1\n(boldsymbols - boldsymbolr)\n\nis the squared Mahalanobis distance between boldsymbols and boldsymbolr. \n\nNote that, in practical applications, the reference point boldsymbolomega is often taken to be an estimable parameter rather than fixed and known. \n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in various paper, we have developed the following functions for evaluating the density function for several popular distributions. We include these in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(Z::V, L::LT) where {V <: AbstractVector, LT <: LowerTriangular}\ngaussiandensity(Z::A, L::LT) where {A <: AbstractArray, LT <: LowerTriangular}\ngaussiandensity(Z::A, Σ::M) where {A <: AbstractArray, M <: AbstractMatrix}\n\nEfficiently computes the density function for Z ~ 𝑁(0, Σ), namely,  \n\n2piboldsymbolSigma^-12 exp-frac12boldsymbolZ^top boldsymbolSigma^-1boldsymbolZ\n\nfor covariance matrix Σ, and where L is lower Cholesky factor of Σ.\n\nThe method gaussiandensity(Z::A, L::LT) assumes that the last dimension of Z contains independent and identically distributed replicates.\n\nIf logdensity = true (default), the log-density is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z₁, z₂, ψ₁₂; logdensity = true)\n\nThe bivariate density function (see, e.g., Sainsbury-Dale et al., 2024, Sec. S6.2) for Schlather's (2002) max-stable model, where ψ₁₂ denotes the spatial correlation function evaluated at the locations of observations z₁ and z₂.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For a very simple example that can be \"copy-paste\", see Quick start on the index page. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Before proceeding to the main examples, we first load the required packages, the following of which are used throughout these examples:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux                                  # Julia's deep-learning library\nusing Distributions: InverseGamma, Uniform  # sampling from probability distributions\nusing AlgebraOfGraphics, CairoMakie         # visualisation","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages will be used in the examples with Gridded data and Irregular spatial data:  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using Distances                             # distance matrices \nusing Folds                                 # parallel simulation (start Julia with --threads=auto)\nusing LinearAlgebra                         # Cholesky factorisation","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages are used only in the example with Irregular spatial data: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using GraphNeuralNetworks                   # GNN architecture\nusing Statistics: mean                            ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, various GPU backends can be used (see the Flux documentation for details). For instance, to use an NVIDIA GPU in the following examples, simply load CUDA.jl:  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using CUDA","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once a GPU package is loaded and a compatible GPU is available, the functions in NeuralEstimators will automatically leverage it to improve computational efficiency while ensuring memory safety via batched operations (GPU usage can be disabled by setting use_gpu = false).","category":"page"},{"location":"workflow/examples/#Univariate-data","page":"Examples","title":"Univariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we develop a neural Bayes estimator for boldsymboltheta equiv (mu sigma) from data boldsymbolZ equiv (Z_1 dots Z_m), where each Z_i oversetmathrmiidsim N(mu sigma^2). See Estimators for a list of other classes of estimators available in the package.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We begin by defining a function to sample parameters from the prior distribution. Assuming prior independence, we adopt the marginal priors mu sim N(0 1) and sigma sim IG(3 1):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tμ = randn(K)\n\tσ = rand(InverseGamma(3, 1), K)\n\tθ = vcat(μ', σ')\n\treturn θ\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define the statistical model implicitly through data simulation. Since our data are replicated, the simulated data are stored as a Vector{A}, where each element corresponds to one parameter vector. The type A reflects the multivariate structure of the data. In this example, each replicate Z_1 dots Z_m is univariate, so A is a Matrix with n = 1 row and m columns:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(θ, m)\n    [ϑ[1] .+ ϑ[2] .* randn(1, m) for ϑ in eachcol(θ)]\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design our neural network. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As we are constructing a neural Bayes estimator, the neural network is a mapping mathcalZtoTheta, and the dimensionality of the neural-network output is therefore d equiv textrmdim(Theta) = 2. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Since our data are replicated, we adopt the DeepSets framework, implemented via the type DeepSet. DeepSets consist of two neural networks: an inner network and an outer network. The inner network extracts summary statistics from the data, and its architecture depends on the multivariate structure of the data. For unstructured data (i.e., data without spatial or temporal correlation within each replicate), we use a multilayer perceptron (MLP). The input dimension matches the dimensionality of each data replicate, while the output dimension corresponds to the number of summary statistics appropriate for the model (a common choice is d). The outer network maps the learned summary statistics to the output space (here, the parameter space, Theta). The outer network is always an MLP. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Below is an example of a DeepSets architecture for neural Bayes estimation in this example. Note that many models have parameter constraints (e.g., variance and range parameters that must be strictly positive). These constraints can be incorporated in the final layer of the neural network by choosing appropriate activation functions for each parameter. Here, we enforce the constraint sigma  0 by applying the softplus activation function in the final layer of the outer network, ensuring that all parameter estimates are valid. For some additional ways to constrain parameter estimates, see Output layers. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"n = 1    # dimension of each data replicate (univariate)\nd = 2    # dimension of the parameter vector θ\nw = 128  # width of each hidden layer \n\n# Final layer has output dimension d and enforces parameter constraints\nfinal_layer = Parallel(\n    vcat,\n    Dense(w, 1, identity),     # μ ∈ ℝ\n    Dense(w, 1, softplus)      # σ > 0\n)\n\n# Inner and outer networks\nψ = Chain(Dense(n, w, relu), Dense(w, d, relu))    \nϕ = Chain(Dense(d, w, relu), final_layer)          \n\n# Combine into a DeepSet\nnetwork = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We then initialise the neural Bayes estimator by wrapping the neural network in a PointEstimator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"estimator = PointEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator using train(), here using the default mean-absolute-error loss, so that the resulting neural Bayes estimator approximates the marginal posterior medians. We'll train the estimator using m=50 independent replicates per parameter configuration. Below, we pass our user-defined functions for sampling parameters and simulating data, but one may also pass parameter or data instances, which will be held fixed during training:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 50\nestimator = train(estimator, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"One may wish to save a trained estimator and load it in a later session: see Saving and loading neural estimators for details on how this can be done. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The function assess() can be used to assess the trained estimator. Parametric and non-parametric bootstrap estimates can be obtained via bootstrap(), with corresponding confidence intervals computed using interval(). Additionally, non-parametric bootstrap-based uncertainty quantification can be included in the assessment stage through the keyword argument probs:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test, m)\nassessment = assess(estimator, θ_test, Z_test, probs = [0.025, 0.975])","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting Assessment object contains the sampled parameters, the corresponding point estimates, and the corresponding lower and upper bounds of the bootstrap intervals. This object can be used to compute various diagnostics and to visualise the neural point estimates and bootstrap intervals vs the true parameter value:  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"bias(assessment)      # μ = 0.002, σ = 0.017\nrmse(assessment)      # μ = 0.086, σ = 0.078\nrisk(assessment)      # μ = 0.055, σ = 0.056\nplot(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Univariate Gaussian example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As an alternative form of uncertainty quantification with neural Bayes estimators, one may approximate a set of marginal posterior quantiles by training a neural Bayes estimator under the quantile loss function, which allows one to generate approximate marginal posterior credible intervals. This is facilitated with IntervalEstimator which, by default, targets 95% central credible intervals:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"q̂ = IntervalEstimator(network)\nq̂ = train(q̂, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting posterior credible-interval estimator can also be assessed using assess(). Often, these intervals have better coverage than bootstrap-based intervals.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once an estimator is deemed to be well calibrated, it may be applied to observed data (below, we use simulated data as a substitute for observed data):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ = sample(1)                       # true parameters\nZ = simulate(θ, m)                  # \"observed\" data\nestimate(estimator, Z)              # point estimate\ninterval(bootstrap(estimator, Z))   # 95% non-parametric bootstrap intervals\ninterval(q̂, Z)                      # 95% marginal posterior credible intervals","category":"page"},{"location":"workflow/examples/#Gridded-data","page":"Examples","title":"Gridded data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For data collected over a regular grid, neural estimators are typically based on a convolutional neural network (CNN; see, e.g., Dumoulin and Visin, 2016). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"When using CNNs with NeuralEstimators, each data set must be stored as a multi-dimensional array. The penultimate dimension stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes), while the final dimension stores independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10times15 grid, one would construct an array of dimension 10times15times2times50.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, here we develop a neural Bayes estimator for the (univariate) spatial Gaussian process model with exponential covariance function and unknown range parameter theta  0. The spatial domain is taken to be the unit square, we simulate data on a regular square grid of size n = 16^2 = 256, and we adopt the prior theta sim U(0 05). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Simulation from Gaussian processes typically involves the computation of an expensive intermediate object, namely, the Cholesky factor of a covariance matrix. Storing intermediate objects can enable the fast simulation of new data sets when the parameters are held fixed. Hence, in this example, we define a custom type Parameters subtyping ParameterConfigurations for storing the matrix of parameters and the corresponding Cholesky factors: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters{T} <: ParameterConfigurations\n\tθ::Matrix{T}\n\tL\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Further, we define two constructors for our custom type: one that accepts an integer K, and another that accepts a dtimes K matrix of parameters. The former constructor will be useful during the training stage for sampling from the prior distribution, while the latter constructor will be useful for parametric bootstrap (since this involves repeated simulation from the fitted model):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\t# Sample parameters from the prior \n\tθ = 0.5 * rand(1, K)\n\n\t# Pass to matrix constructor\n\tParameters(θ)\nend\n\nfunction Parameters(θ::Matrix)\n\t# Spatial locations, a 16x16 grid over the unit square\n\tpts = range(0, 1, length = 16)\n\tS = expandgrid(pts, pts)\n\n\t# Distance matrix, covariance matrices, and Cholesky factors\n\tD = pairwise(Euclidean(), S, dims = 1)\n\tK = size(θ, 2)\n\tL = Folds.map(1:K) do k\n\t\tΣ = exp.(-D ./ θ[k])\n\t\tcholesky(Symmetric(Σ)).L\n\tend\n\n\tParameters(θ, L)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define the model simulator, which returns simulated data as a four-dimensional array (see Simulating data for an overview of common data formats): ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m = 1) \n\tZ = Folds.map(parameters.L) do L\n\t\tn = size(L, 1)\n\t\tz = L * randn(n, m)\n\t\tz = reshape(z, 16, 16, 1, m) \n\t\tz\n\tend\n\tZ\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"A possible neural-network architecture is as follows. Note that deeper architectures that employ residual connections (see ResidualBlock) often lead to improved performance, and certain pooling layers (e.g., GlobalMeanPool) allow the neural network to accommodate grids of varying dimension; for further discussion and an illustration, see Sainsbury-Dale et al. (2025, Sec. S3, S4). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Inner network \nψ = Chain(\n      Conv((3, 3), 1 => 32, relu),   # 3x3 convolutional filter, 1 input channel to 32 output channels\n      MaxPool((2, 2)),               # 2x2 max pooling for dimension reduction\n      Conv((3, 3), 32 => 64, relu),  # 3x3 convolutional filter, 32 input channels to 64 output channels\n      MaxPool((2, 2)),               # 2x2 max pooling for dimension reduction\n      Flux.flatten                   # flatten output to feed into a fully connected layer\n  )\n\n# Outer network \nϕ = Chain(Dense(256, 64, relu), Dense(64, 1))\n\n# DeepSet object\nnetwork = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Above, we embedded our CNN within the DeepSets framework to accommodate scenarios involving replicated spatial data (e.g., when fitting models for spatial extremes). However, the package allows users to define the neural network using any Flux model. Since this example does not include independent replicates, one could instead store each simulated data set in the final dimension of a four-dimensional array, and then use a generic CNN architecture.  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we initialise a point estimator and a posterior credible-interval estimator:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ̂ = PointEstimator(network)\nq̂ = IntervalEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Now we train the estimators, here using fixed parameter instances to avoid repeated Cholesky factorisations (see Storing expensive intermediate objects for data simulation and On-the-fly and just-in-time simulation for further discussion):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"K = 10000 # number of training parameter vectors\nθ_train = sample(K)\nθ_val   = sample(K ÷ 10)\nθ̂ = train(θ̂, θ_train, θ_val, simulate)\nq̂ = train(q̂, θ_train, θ_val, simulate)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the estimators have been trained, we assess them using empirical simulation-based methods:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test)\nassessment = assess([θ̂, q̂], θ_test, Z_test)\n\nbias(assessment)       # 0.005\nrmse(assessment)       # 0.032\ncoverage(assessment)   # 0.953\nplot(assessment)       ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Gridded spatial Gaussian process example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, we can apply our estimators to observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ = Parameters(Matrix([0.1]'))         # true parameter\nZ = simulate(θ)                        # \"observed\" data\nestimate(θ̂, Z)                         # point estimate: 0.11\ninterval(q̂, Z)                         # 95% marginal posterior credible interval: [0.08, 0.16]","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that missing data (e.g., due to cloud cover) can be accommodated using the missing-data methods implemented in the package.","category":"page"},{"location":"workflow/examples/#Irregular-spatial-data","page":"Examples","title":"Irregular spatial data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To cater for spatial data collected over arbitrary spatial locations, one may construct a neural estimator with a graph neural network (GNN; see Sainsbury-Dale, Zammit-Mangion, Richards, and Huser, 2025). The overall workflow remains as given in previous examples, with two key additional considerations.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, if inference is to be made from a single spatial data set collected before constructing estimator, training data can be simulated using the observed spatial locations, which can be treated as fixed and known. However, if the estimator is intended for application to multiple spatial data sets with varying spatial configurations, it should be trained on a diverse set of spatial configurations. These configurations can be sampled during training, possibly using a spatial point process such as maternclusterprocess(). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Second, the spatial data should be stored as a graph, which can be achieved using spatialgraph().","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, we again consider a spatial Gaussian process model with exponential covariance function, and we define a type for storing expensive intermediate objects needed for data simulation. In this example, these objects include Cholesky factors, and spatial graphs which store the adjacency matrices needed to perform graph convolutions: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters <: ParameterConfigurations\n\tθ::Matrix      # true parameters  \n\tL              # Cholesky factors\n\tg              # spatial graphs\n\tS              # spatial locations \nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Again, we define two constructors, which will be convenient for sampling parameters from the prior during training and assessment, and for parametric bootstrap sampling when making inferences from observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\t# Sample parameters from the prior \n\tθ = 0.5 * rand(1, K)\n\n\t# Sample spatial configurations from Matern cluster process on [0, 1]²\n\tn = rand(200:300, K)\n\tλ = rand(Uniform(10, 50), K)\n\tS = [maternclusterprocess(λ = λ[k], μ = n[k]/λ[k]) for k ∈ 1:K]\n\n\t# Pass to constructor\n\tParameters(θ, S)\nend\n\nfunction Parameters(θ::Matrix, S)\n\t# Compute covariance matrices and Cholesky factors \n\tL = Folds.map(axes(θ, 2)) do k\n\t\tD = pairwise(Euclidean(), S[k], dims = 1)\n\t\tΣ = Symmetric(exp.(-D ./ θ[k]))\n\t\tcholesky(Σ).L\n\tend\n\n\t# Construct spatial graphs\n\tg = spatialgraph.(S)\n\n\t# Store in Parameters object\n\tParameters(θ, L, g, S)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define a function for simulating from the model given an object of type Parameters. The code below enables simulation of an arbitrary number of independent replicates m, and one may provide a single integer for m, or any object that can be sampled using rand(m, K) (e.g., an integer range or some distribution over the possible sample sizes):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m)\n\tK = size(parameters, 2)\n\tm = rand(m, K)\n\tmap(1:K) do k\n\t\tL = parameters.L[k]\n\t\tg = parameters.g[k]\n\t\tn = size(L, 1)\n\t\tZ = L * randn(n, m[k])      \n\t\tspatialgraph(g, Z)            \n\tend\nend\nsimulate(parameters::Parameters, m::Integer = 1) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we construct our GNN architecture. Here, we use an architecture tailored to isotropic spatial dependence models; for further details, see Sainsbury-Dale et al. (2025, Sec. 2.2). We also employ a sparse approximation of the empirical variogram as an expert summary statistic (Gerber and Nychka, 2021).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In this example our goal is to construct a point estimator, however any other kind of estimator (see Estimators) can be constructed by simply substituting the appropriate estimator class in the final line below:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Spatial weight functions: continuous surrogates for 0-1 basis functions \nh_max = 0.15 # maximum distance to consider \nq = 10       # output dimension of the spatial weights\nw = KernelWeights(h_max, q)\n\n# Propagation module\npropagation = GNNChain(\n\tSpatialGraphConv(1 => q, relu, w = w, w_out = q),\n\tSpatialGraphConv(q => q, relu, w = w, w_out = q)\n)\n\n# Readout module\nreadout = GlobalPool(mean)\n\n# Inner network\nψ = GNNSummary(propagation, readout)\n\n# Expert summary statistics, the empirical variogram\nS = NeighbourhoodVariogram(h_max, q)\n\n# Outer network\nϕ = Chain(\n\tDense(2q => 128, relu), \n\tDense(128 => 128, relu), \n\tDense(128 => 1, identity)\n)\n\n# DeepSet object\nnetwork = DeepSet(ψ, ϕ; S = S)\n\n# Point estimator\nestimator = PointEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 1\nK = 5000\nθ_train = sample(K)\nθ_val   = sample(K÷5)\nestimator = train(estimator, θ_train, θ_val, simulate, m = m, epochs = 10)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that the computations in GNNs are performed in parallel, making them particularly well-suited for GPUs, which typically contain thousands of cores. If you have access to an NVIDIA GPU, you can utilise it by simply loading the Julia package CUDA. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we assess our trained estimator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test, m)\nassessment = assess(estimator, θ_test, Z_test)\nbias(assessment)   \nrmse(assessment)    \nrisk(assessment)   \nplot(assessment)   ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Estimates from a graph neural network (GNN) based neural Bayes estimator)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, once the estimator has been assessed, it may be applied to observed data, with bootstrap-based uncertainty quantification facilitated by bootstrap() and interval(). Note that, since the estimator was trained using spatial configurations in the unit square 0 1 times 0 1, the spatial coordinates of observed data should be scaled by a common factor such that they are also contained within this unit square; estimates of any range parameters are then scaled back accordingly. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"parameters = sample(1)             # sample a parameter vector and spatial locations              \nθ = parameters.θ                   # true parameters\nS = parameters.S                   # \"observed\" locations\nZ = simulate(parameters)           # \"observed\" data    \nθ̂ = estimate(estimator, Z)         # point estimate\nps = Parameters(θ̂, S)              # construct Parameters object from point estimate\nbs = bootstrap(estimator, ps, simulate, m)  # parametric bootstrap estimates\ninterval(bs)                       # parametric bootstrap confidence interval              ","category":"page"},{"location":"methodology/#Methodology","page":"Methodology","title":"Methodology","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Here, we provide an overview of the amortised neural inferential methods supported by the package, which include neural Bayes estimators, neural posterior estimators, and neural ratio estimators. For further details on each of these methods and amortised neural inference more broadly, see the review paper by Zammit-Mangion et al. (2025) and the references therein.","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Notation: We denote model parameters of interest by boldsymboltheta equiv (theta_1 dots theta_d) in Theta, where Theta subseteq mathbbR^d is the parameter space. We denote data by boldsymbolZ equiv (Z_1 dots Z_n) in mathcalZ, where mathcalZ subseteq mathbbR^n is the sample space. We denote neural-network parameters by boldsymbolgamma. For simplicity, we assume that all measures admit densities with respect to the Lebesgue measure. We use pi(cdot) to denote the prior density function of the parameters. The input argument to a generic density function p(cdot) serves to specify both the random variable associated with the density and its evaluation point.","category":"page"},{"location":"methodology/#Neural-Bayes-estimators","page":"Methodology","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The goal of parametric point estimation is to estimate boldsymboltheta from data boldsymbolZ using an estimator, hatboldsymboltheta  mathcalZtoTheta. Estimators can be constructed intuitively within a decision-theoretic framework based on average-risk optimality. Specifically, consider a loss function L Theta times Theta to 0 infty). Then the Bayes risk of the estimator hatboldsymboltheta(cdot) is  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"int_Theta int_mathcalZ  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ))p(boldsymbolZ mid boldsymboltheta) pi(boldsymboltheta) textrmd boldsymbolZ textrmdboldsymboltheta  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Any minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdot cdot) and pi(cdot). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Bayes estimators are functionals of the posterior distribution (e.g., the Bayes estimator under quadratic loss is the posterior mean), and are therefore often unavailable in closed form. A way forward is to assume a flexible parametric function for hatboldsymboltheta(cdot), and to optimise the parameters within that function in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are fast to evaluate. Let hatboldsymboltheta_boldsymbolgamma  mathcalZtoTheta denote a neural network parameterised by boldsymbolgamma. Then a Bayes estimator may be approximated by hatboldsymboltheta_boldsymbolgamma^*(cdot), where ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin frac1K sum_k = 1^K L(boldsymboltheta hatboldsymboltheta_boldsymbolgamma(boldsymbolZ^(k)))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"with boldsymboltheta^(k) sim pi(boldsymboltheta) and, independently for each k, boldsymbolZ^(k) sim p(boldsymbolZ mid  boldsymboltheta^(k)). The process of obtaining boldsymbolgamma^* is referred to as \"training the network\", and this can be performed efficiently using back-propagation and stochastic gradient descent. The trained neural network hatboldsymboltheta_boldsymbolgamma^*(cdot) approximately minimises the Bayes risk, and therefore it is called a neural Bayes estimator (Sainsbury-Dale at al., 2024). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Once trained, a neural Bayes estimator can be applied repeatedly to observed data (whose structure conforms with the chosen neural-network architecture) at a fraction of the computational cost of conventional inferential methods. It is therefore ideal to use a neural Bayes estimator in settings where inference needs to be made repeatedly; in this case, the initial training cost is said to be amortised over time. ","category":"page"},{"location":"methodology/#Uncertainty-quantification-with-neural-Bayes-estimators","page":"Methodology","title":"Uncertainty quantification with neural Bayes estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Uncertainty quantification with neural Bayes estimators often proceeds through the bootstrap distribution (e.g., Lenzi et al., 2023; Richards et al., 2024; Sainsbury-Dale et al., 2024). Bootstrap-based approaches are particularly attractive when nonparametric bootstrap is possible (e.g., when the data are independent replicates), or when simulation from the fitted model is fast, in which case parametric bootstrap is also computationally efficient. However, these conditions are not always met and, although bootstrap-based approaches are often considered to be fairly accurate and favourable to methods based on asymptotic normality, there are situations where bootstrap procedures are not reliable (see, e.g., Canty et al., 2006, pg. 6). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Alternatively, by leveraging ideas from (Bayesian) quantile regression, one may construct a neural Bayes estimator that approximates a set of marginal posterior quantiles (Fisher et al., 2023; Sainsbury-Dale et al., 2025), which can then be used to construct credible intervals for each parameter. Inference then remains fully amortised since, once the estimators are trained, both point estimates and credible intervals can be obtained with virtually zero computational cost. Specifically, posterior quantiles can be targeted by training a neural Bayes estimator under the loss function","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"L(boldsymboltheta hatboldsymboltheta tau) equiv sum_j=1^d (hattheta_j - theta_j)mathbbI(hattheta_j - theta_j) - tau quad 0  tau  1","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where mathbbI(cdot) denotes the indicator function, since the Bayes estimator under this loss function is the vector of marginal posterior tau-quantiles (Sainsbury-Dale et al., 2025, Sec. 2.2.4). ","category":"page"},{"location":"methodology/#Neural-posterior-estimators","page":"Methodology","title":"Neural posterior estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"We now describe amortised approximate posterior inference through the minimisation of an expected Kullback–Leibler (KL) divergence. Throughout, we let q(boldsymboltheta boldsymbolkappa) denote a parametric approximation to the posterior distribution p(boldsymboltheta mid boldsymbolZ), where the approximate-distribution parameters boldsymbolkappa belong to a space mathcalK. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"We first consider the non-amortised case, where the optimal parameters boldsymbolkappa^* for a single data set boldsymbolZ are found by minimising the KL divergence between p(boldsymboltheta mid boldsymbolZ) and q(boldsymboltheta boldsymbolkappa): ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"  boldsymbolkappa^* \n  equiv undersetboldsymbolkappamathrmargmin textrmKLp(boldsymboltheta mid boldsymbolZ)    q(boldsymboltheta boldsymbolkappa)\n  = undersetboldsymbolkappamathrmargmin -int_Theta log q(boldsymboltheta boldsymbolkappa) p(boldsymboltheta mid boldsymbolZ) textrmdboldsymboltheta\n  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The resulting approximate posterior q(boldsymboltheta boldsymbolkappa^*) targets the true posterior in the sense that the KL divergence is zero if and only if q(boldsymboltheta boldsymbolkappa^*) = p(boldsymboltheta mid boldsymbolZ) for all boldsymboltheta in Theta. However, solving this optimisation problem is often computationally demanding even for a single data set boldsymbolZ, and solving it for many different data sets can be computationally prohibitive. The optimisation problem can be amortised by treating the parameters boldsymbolkappa as a function boldsymbolkappa  mathcalZ to mathcalK, and then choosing the function boldsymbolkappa^*(cdot) that minimises an expected KL divergence: ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolkappa^*(cdot) equiv undersetboldsymbolkappa(cdot)mathrmargmin mathbbE_boldsymbolZtextrmKLp(boldsymboltheta mid boldsymbolZ)    q(boldsymboltheta boldsymbolkappa(boldsymbolZ))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"In practice, we approximate boldsymbolkappa^*(cdot) using a neural network, boldsymbolkappa_boldsymbolgamma  mathcalZ to mathcalK, which is parameterised by boldsymbolgamma and trained by minimising a Monte Carlo approximation of the expected KL divergence above: ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin -sum_k=1^K log q(boldsymboltheta^(k) boldsymbolkappa_boldsymbolgamma(boldsymbolZ^(k)))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Once trained, the neural network boldsymbolkappa_boldsymbolgamma^*(cdot) may be used to estimate the optimal approximate-distribution parameters boldsymbolkappa^* given data boldsymbolZ at almost no computational cost. The neural network boldsymbolkappa_boldsymbolgamma^*(cdot), together with the corresponding approximate distribution q(cdot boldsymbolkappa), is collectively referred to as a neural posterior estimator. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"There are numerous options for the approximate distribution q(cdot boldsymbolkappa). For instance, q(cdotboldsymbolkappa) can be modelled as a Gaussian distribution (e.g., Chan et al., 2018; see GaussianDistribution), where the parameters boldsymbolkappa = (boldsymbolmu textrmvech(boldsymbolL)) consist of a d-dimensional mean parameter boldsymbolmu and the d(d+1)2 non-zero elements of the lower Cholesky factor boldsymbolL of a covariance matrix, and the half-vectorisation operator textrmvech(cdot) vectorises the lower triangle of its matrix argument. One may also consider Gaussian mixtures (e.g., Papamakarios & Murray, 2016) or trans-Gaussian distributions (e.g., Maceda et al., 2024). However, the most widely adopted approach is to model q(cdot boldsymbolkappa) using a normalising flow (e.g., Ardizzone et al., 2019; Radev et al., 2022), excellent reviews for which are given by Kobyzev et al. (2020) and Papamakarios (2021). A particularly popular class of normalising flow is the affine coupling flow (e.g., Dinh et al., 2016; Kingma & Dhariwal, 2018; Ardizzone et al., 2019). Since affine coupling flows are universal density approximators (Teshima et al., 2020), they serve as the default and recommended choice for approximate distributions in this package; for further details, see NormalisingFlow. ","category":"page"},{"location":"methodology/#Neural-ratio-estimators","page":"Methodology","title":"Neural ratio estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Finally, we describe amortised inference by approximation of the likelihood-to-evidence ratio, ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"r(boldsymbolZ boldsymboltheta) equiv p(boldsymbolZ mid boldsymboltheta)p(boldsymbolZ)","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where p(boldsymbolZ mid boldsymboltheta) is the likelihood and p(boldsymbolZ) is the marginal likelihood (also known as the model evidence). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The likelihood-to-evidence ratio is ubiquitous in statistical inference. For example, likelihood ratios of the form p(boldsymbolZmid boldsymboltheta_0)p(boldsymbolZmid boldsymboltheta_1)=r(boldsymbolZ boldsymboltheta_0)r(boldsymbolZ boldsymboltheta_1) are central to hypothesis testing and model comparison, and naturally appear in the transition probabilities of most standard MCMC algorithms used for Bayesian inference. Further, since the likelihood-to-evidence ratio is a prior-free quantity, its approximation facilitates Bayesian inference in applications where one requires multiple fits of the model under different prior distributions. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Unlike the methods discussed earlier, the likelihood-to-evidence ratio might not immediately seem like a quantity well-suited for approximation by neural networks, which are trained by minimising empirical risk functions. However, this ratio emerges naturally as a simple transformation of the optimal solution to a standard binary classification problem, derived through the minimisation of an average risk. Specifically, consider a binary classifier c(boldsymbolZ boldsymboltheta) that distinguishes dependent data-parameter pairs (boldsymbolZ boldsymboltheta) sim p(boldsymbolZ boldsymboltheta) with class labels Y=1 from independent data-parameter pairs (tildeboldsymbolZ tildeboldsymboltheta) sim p(boldsymbolZ)p(boldsymboltheta) with class labels Y=0, and where the classes are balanced. Here, p(boldsymboltheta) denotes an arbitrary \"proposal\" distribution for boldsymboltheta that does not, in general, coincide with the prior distribution (see below). Then, the Bayes classifier under binary cross-entropy loss is defined as ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"beginaligned\nc^*(cdot cdot) \nequiv\nundersetc(cdot cdot)mathrmargmin sum_yin0 1 textrmPr(Y = y)  int_Thetaint_mathcalZL_textrmBCEy c(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta mid Y = y)textrmd boldsymbolZ textrmd boldsymboltheta\n=\nundersetc(cdot cdot)mathrmargmin - int_Thetaint_mathcalZBiglogc(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta)  + log1 - c(boldsymbolZ boldsymboltheta)p(boldsymbolZ)p(boldsymboltheta) Bigtextrmd boldsymbolZ textrmd boldsymboltheta\nendaligned","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where L_textrmBCE(y c) equiv -ylog(c) - (1 - y) log(1 - c). It can be shown (e.g., Hermans et al., 2020, App. B)  that the Bayes classifier is given by ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"c^*(boldsymbolZ boldsymboltheta) = fracp(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta) + p(boldsymboltheta)p(boldsymbolZ) quad boldsymbolZ in mathcalZ boldsymboltheta in Theta","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"and, hence,","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"r(boldsymbolZ boldsymboltheta) = fracc^*(boldsymbolZ boldsymboltheta)1 - c^*(boldsymbolZ boldsymboltheta) quad boldsymbolZ in mathcalZ boldsymboltheta in Theta","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"This connection links the likelihood-to-evidence ratio to the average-risk-optimal solution of a standard binary classification problem, and consequently provides a foundation for approximating the ratio using neural networks. Specifically, let c_boldsymbolgamma mathcalZ times Theta to (0 1) denote a neural network parametrised by boldsymbolgamma. Then the Bayes classifier may be approximated by c_boldsymbolgamma^*(cdot cdot), where ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":" boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin -sum_k=1^K Biglogc_boldsymbolgamma(boldsymbolZ^(k) boldsymboltheta^(k)) +  log1 - c_boldsymbolgamma(boldsymbolZ^(sigma(k)) boldsymboltheta^(k)) Big","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"with each boldsymboltheta^(k) sampled independently from a \"proposal\" distribution p(boldsymboltheta), boldsymbolZ^(k) sim p(boldsymbolZ mid boldsymboltheta^(k)), and sigma(cdot) a random permutation of 1 dots K. The proposal distribution p(boldsymboltheta) does not necessarily correspond to the prior distribution pi(boldsymboltheta), which is specified in the downstream inference algorithm (see below). In theory, any p(boldsymboltheta) with support over Theta can be used. However, with finite training data, the choice of p(boldsymboltheta) is important, as it determines where the parameters boldsymboltheta^(k) are most densely sampled and, hence, where the neural network c_boldsymbolgamma^*(cdot cdot) best approximates the Bayes classifier. Further, since neural networks are only reliable within the support of their training samples, a p(boldsymboltheta) lacking full support over Theta essentially acts as a \"soft prior\". ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Once the neural network is trained, r_boldsymbolgamma^*(boldsymbolZ boldsymboltheta) equiv c_boldsymbolgamma^*(boldsymbolZ boldsymboltheta)1 - c_boldsymbolgamma^*(boldsymbolZ boldsymboltheta)^-1, boldsymbolZ in mathcalZ boldsymboltheta in Theta, may be used to quickly approximate the likelihood-to-evidence ratio, and therefore it is called a neural ratio estimator. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Inference based on a neural ratio estimator may proceed in a frequentist setting via maximum likelihood and likelihood ratios (e.g., Walchessen et al., 2024), and in a Bayesian setting by facilitating the computation of transition probabilities in Hamiltonian Monte Carlo and MCMC algorithms (e.g., Hermans et al., 2020). Further, an approximate posterior distribution can be obtained via the identity p(boldsymboltheta mid boldsymbolZ) = pi(boldsymboltheta) r(boldsymboltheta boldsymbolZ), and sampled from using standard sampling techniques (e.g., Thomas et al., 2022).","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Saving-and-loading-neural-estimators","page":"Advanced usage","title":"Saving and loading neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In regards to saving and loading, neural estimators behave in the same manner as regular Flux models. Therefore, the examples and recommendations outlined in the Flux documentation also apply directly to neural estimators. For example, to save the model state of the neural estimator estimator, run:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using Flux\nusing BSON: @save, @load\nmodel_state = Flux.state(estimator)\n@save \"estimator.bson\" model_state","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, to load it in a new session, one may initialise a neural estimator with the same architecture used previously, and load the saved model state as follows:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"@load \"estimator.bson\" model_state\nFlux.loadmodel!(estimator, model_state)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"It is also straightforward to save the entire neural estimator, including its architecture (see here). However, the first approach outlined above is recommended for long-term storage.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For convenience, the function train() allows for the automatic saving of the model state during the training stage, via the argument savepath.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution may be stored in two ways. Most simply, they can be stored as a d times K matrix, where d is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. Alternatively, they can be stored in a user-defined subtype of ParameterConfigurations, whose only requirement is a field θ that stores the d times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, the training data set, mathcalZ_texttrain, can be simulated continuously during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Further, this technique allows for data to be simulated \"just-in-time\", in the sense that they can be simulated in small batches, used to train the neural estimator, and then removed from memory. This can substantially reduce pressure on memory resources, particularly when working with large data sets.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh the set vartheta_texttrain of parameter vectors used during training, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models. Hybrid approaches are also possible, whereby the parameters (and possibly the data) are held fixed for several epochs (i.e., several passes through the training set when performing stochastic gradient descent) before being refreshed.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with various methods of train().","category":"page"},{"location":"workflow/advancedusage/#Input-scaling","page":"Advanced usage","title":"Input scaling","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"It is important to ensure that the data passed through the neural network are on a reasonable numerical scale, since values with very large absolute value can lead to numerical instability during training (e.g., exploding gradients). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A relatively simply way to achieve this is by including a transformation in the first layer of the neural network. For example, if the data have positive support, one could define the neural network with the first layer applying a log transformation:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"network = Chain(z -> log.(1 + z), ...)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Or, if the data are not strictly positive, one may consider the following signed transformation:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"network = Chain(z -> sign.(z) .* log.(1 .+ abs.(z)), ...)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A simple preprocessing layer or transformation pipeline such as this can make a significant difference in performance and stability. See feature scaling for further discussion and possible approaches. ","category":"page"},{"location":"workflow/advancedusage/#Regularisation","page":"Advanced usage","title":"Regularisation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The term regularisation refers to a variety of techniques aimed to reduce overfitting when training a neural network, primarily by discouraging complex models.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A popular regularisation technique is known as dropout, implemented in Flux's Dropout layer. Dropout involves temporarily dropping (\"turning off\") a randomly selected set of neurons (along with their connections) at each iteration of the training stage, which results in a computationally-efficient form of model (neural-network) averaging (Srivastava et al., 2014).","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Another class of regularisation techniques involve modifying the loss function. For instance, L₁ regularisation (sometimes called lasso regression) adds to the loss a penalty based on the absolute value of the neural-network parameters. Similarly, L₂ regularisation (sometimes called ridge regression) adds to the loss a penalty based on the square of the neural-network parameters. Note that these penalty terms are not functions of the data or of the statistical-model parameters that we are trying to infer. These regularisation techniques can be implemented straightforwardly by providing a custom optimiser to train() that includes a SignDecay object for L₁ regularisation, or a WeightDecay object for L₂ regularisation. See the Flux documentation for further details. Note that, when the training data and parameters are simulated dynamically (i.e., \"on the fly\"; see On-the-fly and just-in-time simulation), overfitting is generally not a concern, making this form of regularisation unnecessary.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For illustration, the following code constructs a neural Bayes estimator using dropout and L₁ regularisation with penalty coefficient lambda = 10^-4:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators, Flux\n\n# Data Z|θ ~ N(θ, 1) with θ ~ N(0, 1)\nd = 1     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 5     # number of independent replicates in each data set\nsampler(K) = randn32(d, K)\nsimulator(θ, m) = [μ .+ randn32(n, m) for μ ∈ eachcol(θ)]\nK = 3000  # number of training samples\nθ_train = sampler(K)\nθ_val   = sampler(K)\nZ_train = simulator(θ_train, m)\nZ_val   = simulator(θ_val, m)\n\n# Neural network with dropout layers\nw = 128\nψ = Chain(Dense(1, w, relu), Dropout(0.1), Dense(w, w, relu), Dropout(0.5))     \nϕ = Chain(Dense(w, w, relu), Dropout(0.5), Dense(w, 1))           \nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise estimator\nestimator = PointEstimator(network)\n\n# Optimiser with L₁ regularisation\noptimiser = Flux.setup(OptimiserChain(SignDecay(1e-4), Adam()), estimator)\n\n# Train the estimator\ntrain(estimator, θ_train, θ_val, Z_train, Z_val; optimiser = optimiser)","category":"page"},{"location":"workflow/advancedusage/#Expert-summary-statistics","page":"Advanced usage","title":"Expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Implicitly, neural estimators involve the learning of summary statistics. However, some summary statistics are available in closed form, simple to compute, and highly informative (e.g., sample quantiles, the empirical variogram). Often, explicitly incorporating these expert summary statistics in a neural estimator can simplify the optimisation problem, and lead to a better estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The fusion of learned and expert summary statistics is facilitated by our implementation of the DeepSet framework. Note that this implementation also allows the user to construct a neural estimator using only expert summary statistics, following, for example, Gerber and Nychka (2021) and Rai et al. (2024). Note also that the user may specify arbitrary expert summary statistics, however, for convenience several standard User-defined summary statistics are provided with the package, including a fast, sparse approximation of the empirical variogram.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For an example of incorporating expert summary statistics, see Irregular spatial data, where the empirical variogram is used alongside learned graph-neural-network-based summary statistics.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as boldsymbolZ^(m) equiv (boldsymbolZ_1 dots boldsymbolZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator). For example, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*)\n=\nbegincases\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_1)  m leq m_1\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where boldsymbolgamma^* equiv (boldsymbolgamma^*_tildem_1 dots boldsymbolgamma^*_tildem_l-1), and boldsymbolgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatboldsymboltheta(cdot boldsymbolgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice and is less computationally burdensome than it first appears when used in conjunction with the technique known as pre-training (see Sainsbury-Dale at al., 2024, Sec 2.3.3), which is facilitated with trainmultiple(). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise estimators are implemented using the type PiecewiseEstimator. ","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case the Bayes risk becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"sum_m in mathcalM\ntextrmPr(M=m)left(\nint_Theta int_mathcalZ^m  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ^(m)))p(boldsymbolZ^(m) mid boldsymboltheta)pi(boldsymboltheta) textrmdboldsymbolZ^(m) textrmd boldsymboltheta\nright)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data during the training phase.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The following pseudocode illustrates how one may modify a general data simulator to train under a range of sample sizes, with the distribution of M defined by passing any object that can be sampled using rand(m, K) (e.g., an integer range like 1:30, or an integer-valued distribution from Distributions.jl):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Method that allows m to be an object that can be sampled from\nfunction simulate(parameters, m)\n\t# Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t# Generate K sample sizes from the prior distribution for M\n\tm̃ = rand(m, K)\n\n\t# Pseudocode for data simulation\n\tZ = [<simulate m̃[k] realisations from the model> for k ∈ 1:K]\n\n\treturn Z\nend\n\n# Method that allows an integer to be passed for m\nsimulate(parameters, m::Integer) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/advancedusage/#Missing-data","page":"Advanced usage","title":"Missing data","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Neural networks do not naturally handle missing data, and this property can preclude their use in a broad range of applications. Here, we describe two techniques that alleviate this challenge in the context of parameter point estimation: the masking approach and the expectation-maximisation (EM) approach. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As a running example, we consider a Gaussian process model where the data are collected over a regular grid, but where some elements of the grid are unobserved. This situation often arises in, for example, remote-sensing applications, where the presence of cloud cover prevents measurement in some places. Below, we load the packages needed in this example, and define some aspects of the model that will remain constant throughout (e.g., the prior, the spatial domain). We also define types and functions for sampling from the prior distribution and for simulating marginally from the data model.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators, Flux\nusing Distributions: Uniform\nusing Distances, LinearAlgebra\nusing Statistics: mean\n\n# Prior and dimension of parameter vector\nΠ = (τ = Uniform(0, 1.0), ρ = Uniform(0, 0.4))\nd = length(Π)\n\n# Define the grid and compute the distance matrix\npoints = range(0, 1, 16)\nS = expandgrid(points, points)\nD = pairwise(Euclidean(), S, dims = 1)\n\n# Collect model information for later use\nξ = (Π = Π, S = S, D = D)\n\n# Struct for storing parameters and Cholesky factors\nstruct Parameters <: ParameterConfigurations\n\tθ\n\tL\nend\n\n# Constructor for above struct\nfunction sample(K::Integer, ξ)\n\n\t# Sample parameters from the prior\n\tΠ = ξ.Π\n\tτ = rand(Π.τ, K)\n\tρ = rand(Π.ρ, K)\n\tν = 1 # fixed smoothness\n\n\t# Compute Cholesky factors  \n\tL = maternchols(ξ.D, ρ, ν)\n\n\t# Concatenate into matrix\n\tθ = permutedims(hcat(τ, ρ))\n\n\tParameters(θ, L)\nend\n\n# Marginal simulation from the data model\nfunction simulate(parameters::Parameters, m::Integer)\n\n\tK = size(parameters, 2)\n\tτ = parameters.θ[1, :]\n\tL = parameters.L\n\tG = isqrt(size(L, 1)) # side-length of grid\n\n\tZ = map(1:K) do k\n\t\tz = simulategaussian(L[:, :, k], m)\n\t\tz = z + τ[k] * randn(size(z)...)\n\t\tz = reshape(z, G, G, 1, :)\n\t\tz\n\tend\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/#The-masking-approach","page":"Advanced usage","title":"The masking approach","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The first missing-data technique that we consider is the so-called masking approach of Wang et al. (2024); see also the discussion by Sainsbury-Dale et al. (2025, Sec. 2.2). The strategy involves completing the data by replacing missing values with zeros, and using auxiliary variables to encode the missingness pattern, which are also passed into the network.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Let boldsymbolZ denote the complete-data vector. Then, the masking approach considers inference based on boldsymbolW, a vector of indicator variables that encode the missingness pattern (with elements equal to one or zero if the corresponding element of boldsymbolZ is observed or missing, respectively), and","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymbolU equiv boldsymbolZ odot boldsymbolW","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where odot denotes elementwise multiplication and the product of a missing element and zero is defined to be zero. Irrespective of the missingness pattern, boldsymbolU and boldsymbolW have the same fixed dimensions and hence may be processed easily using a single neural network. A neural estimator is then trained on realisations of boldsymbolU boldsymbolW which, by construction, do not contain any missing elements. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The manner in which boldsymbolU and boldsymbolW are combined depends on the multivariate structure of the data and the chosen architecture. For example, when the data are gridded and the neural network is a CNN, then boldsymbolU and boldsymbolW can be concatenated along the channels dimension (i.e., the penultimate dimension of the array).  The construction of augmented data sets boldsymbolU boldsymbolW from incomplete data is facilitated by the helper function encodedata().","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Since the missingness pattern boldsymbolW is now an input to the neural network, it must be incorporated during the training phase. When interest lies only in making inference from a single already-observed data set, boldsymbolW is fixed and known, and the Bayes risk remains unchanged. However, amortised inference, whereby one trains a single neural network that will be used to make inference with many data sets, requires a joint model for the data boldsymbolZ and the missingness pattern boldsymbolW, which is here defined as follows (see the helper function removedata()):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Marginal simulation from the data model and a MCAR missingness model\nfunction simulatemissing(parameters::Parameters, m::Integer)\n\n\tZ = simulate(parameters, m)   # complete data\n\n\tUW = map(Z) do z\n\t\tprop = rand()             # sample a missingness proportion\n\t\tz = removedata(z, prop)   # randomly remove a proportion of the data\n\t\tuw = encodedata(z)        # replace missing entries with zero and encode missingness pattern\n\t\tuw\n\tend\n\n\treturn UW\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Next, we construct and train a masked neural Bayes estimator using a CNN architecture. Here, the first convolutional layer takes two input channels, since we store the augmented data boldsymbolU in the first channel and the missingness pattern boldsymbolW in the second. We construct a point estimator, but the masking approach is applicable with any other kind of estimator (see Estimators):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Construct DeepSet object\nψ = Chain(\n\tConv((10, 10), 2 => 16,  relu),\n\tConv((5, 5),  16 => 32,  relu),\n\tConv((3, 3),  32 => 64, relu),\n\tFlux.flatten\n\t)\nϕ = Chain(Dense(64, 256, relu), Dense(256, d, exp))\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise point estimator\nθ̂ = PointEstimator(network)\n\n# Train the masked neural Bayes estimator\nθ̂ = train(θ̂, sample, simulatemissing, m = 1, ξ = ξ, K = 1000, epochs = 10)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Once trained, we can apply our masked neural Bayes estimator to (incomplete) observed data. The data must be encoded in the same manner as during training. Below, we use simulated data as a surrogate for real data, with a missingness proportion of 0.25:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"θ = sample(1, ξ)     # true parameters\nZ = simulate(θ, 1)[1]    # complete data\nZ = removedata(Z, 0.25)  # \"observed\" incomplete data (i.e., with missing values)\nUW = encodedata(Z)       # augmented data {U, W}\nθ̂(UW)                    # point estimate","category":"page"},{"location":"workflow/advancedusage/#The-EM-approach","page":"Advanced usage","title":"The EM approach","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Let boldsymbolZ_1 and boldsymbolZ_2 denote the observed and unobserved (i.e., missing) data, respectively, and let boldsymbolZ equiv (boldsymbolZ_1 boldsymbolZ_2) denote the complete data. A classical approach to facilitating inference when data are missing is the expectation-maximisation (EM) algorithm. The neural EM algorithm (Sainsbury-Dale et al., 2025) is an approximate version of the conventional (Bayesian) Monte Carlo EM algorithm which, at the lth iteration, updates the parameter vector through","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymboltheta^(l) = undersetboldsymbolthetamathrmargmax sum_h = 1^H ell(boldsymboltheta  boldsymbolZ_1  boldsymbolZ_2^(lh)) + log pi_H(boldsymboltheta)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where realisations of the missing-data component, boldsymbolZ_2^(lh)  h = 1 dots H, are sampled from the probability distribution of boldsymbolZ_2 given boldsymbolZ_1 and boldsymboltheta^(l-1), and where pi_H(boldsymboltheta) propto pi(boldsymboltheta)^H is a concentrated version of the original prior density. Given the conditionally simulated data, the neural EM algorithm performs the above EM update using a neural network that returns the MAP estimate (i.e., the posterior mode) using (complete) conditionally simulated data. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"First, we construct a neural approximation of the MAP estimator. In this example, we will take H=50. When H is taken to be reasonably large, one may lean on the Bernstein-von Mises theorem to train the neural Bayes estimator under linear or quadratic loss; otherwise, one should train the estimator under a continuous relaxation of the 0–1 loss (e.g., the tanhloss() in the limit kappa to 0). This is done as follows:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Construct DeepSet object\nψ = Chain(\n\tConv((10, 10), 1 => 16,  relu),\n\tConv((5, 5),  16 => 32,  relu),\n\tConv((3, 3),  32 => 64, relu),\n\tFlux.flatten\n\t)\nϕ = Chain(\n\tDense(64, 256, relu),\n\tDense(256, d, exp)\n\t)\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise point estimator\nθ̂ = PointEstimator(network)\n\n# Train neural Bayes estimator\nH = 50\nθ̂ = train(θ̂, sample, simulate, m = H, ξ = ξ, K = 1000, epochs = 10)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Next, we define a function for conditional simulation (see EM for details on the required format of this function):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulateconditional(Z::M, θ, ξ; nsims::Integer = 1) where {M <: AbstractMatrix{Union{Missing, T}}} where T\n\n\t# Save the original dimensions\n\tdims = size(Z)\n\n\t# Convert to vector\n\tZ = vec(Z)\n\n\t# Compute the indices of the observed and missing data\n\tI₁ = findall(z -> !ismissing(z), Z) # indices of observed data\n\tI₂ = findall(z -> ismissing(z), Z)  # indices of missing data\n\tn₁ = length(I₁)\n\tn₂ = length(I₂)\n\n\t# Extract the observed data and drop Missing from the eltype of the container\n\tZ₁ = Z[I₁]\n\tZ₁ = [Z₁...]\n\n\t# Distance matrices needed for covariance matrices\n\tD   = ξ.D # distance matrix for all locations in the grid\n\tD₂₂ = D[I₂, I₂]\n\tD₁₁ = D[I₁, I₁]\n\tD₁₂ = D[I₁, I₂]\n\n\t# Extract the parameters from θ\n\tτ = θ[1]\n\tρ = θ[2]\n\n\t# Compute covariance matrices\n\tν = 1 # fixed smoothness\n\tΣ₂₂ = matern.(UpperTriangular(D₂₂), ρ, ν); Σ₂₂[diagind(Σ₂₂)] .+= τ^2\n\tΣ₁₁ = matern.(UpperTriangular(D₁₁), ρ, ν); Σ₁₁[diagind(Σ₁₁)] .+= τ^2\n\tΣ₁₂ = matern.(D₁₂, ρ, ν)\n\n\t# Compute the Cholesky factor of Σ₁₁ and solve the lower triangular system\n\tL₁₁ = cholesky(Symmetric(Σ₁₁)).L\n\tx = L₁₁ \\ Σ₁₂\n\n\t# Conditional covariance matrix, cov(Z₂ ∣ Z₁, θ),  and its Cholesky factor\n\tΣ = Σ₂₂ - x'x\n\tL = cholesky(Symmetric(Σ)).L\n\n\t# Conditonal mean, E(Z₂ ∣ Z₁, θ)\n\ty = L₁₁ \\ Z₁\n\tμ = x'y\n\n\t# Simulate from the distribution Z₂ ∣ Z₁, θ ∼ N(μ, Σ)\n\tz = randn(n₂, nsims)\n\tZ₂ = μ .+ L * z\n\n\t# Combine the observed and missing data to form the complete data\n\tZ = map(1:nsims) do l\n\t\tz = Vector{T}(undef, n₁ + n₂)\n\t\tz[I₁] = Z₁\n\t\tz[I₂] = Z₂[:, l]\n\t\tz\n\tend\n\tZ = stackarrays(Z, merge = false)\n\n\t# Convert Z to an array with appropriate dimensions\n\tZ = reshape(Z, dims..., 1, nsims)\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Now we can use the neural EM algorithm to get parameter point estimates from data containing missing values. The algorithm is implemented with the type EM. Again, here we use simulated data as a surrogate for real data:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"θ = sample(1, ξ)            # true parameters\nZ = simulate(θ, 1)[1][:, :]     # complete data\nZ = removedata(Z, 0.25)         # \"observed\" incomplete data (i.e., with missing values)\nθ₀ = mean.([Π...])              # initial estimate, the prior mean\n\nneuralem = EM(simulateconditional, θ̂)\nneuralem(Z, θ₀, ξ = ξ, nsims = H, use_ξ_in_simulateconditional = true)","category":"page"},{"location":"workflow/advancedusage/#Censored-data","page":"Advanced usage","title":"Censored data","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Neural estimators can be constructed to handle censored data as input, by exploiting the masking approach described above in the context of missing data. For simplicity, here we describe inference with left censored data (i.e., where we observe only those data that exceed some threshold), but extensions to right or interval censoring are possible. We first present the framework for General censoring, where data are considered censored based on an arbitrary, user-defined censoring scheme. We then consider Peaks-over-threshold censoring, a special case in which the data are treated as censored if they do not exceed their corresponding marginal tau-quantile for tau in (01) close to one (Richards et al., 2024).","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As a running example, we consider a bivariate random scale Gaussian mixture copula; see Engelke, Opitz, and Wadsworth (2019) and Huser and Wadsworth (2019). We consider the task of estimating boldsymboltheta=(rhodelta), for correlation parameter rho in (-11) and shape parameter delta in 01. Variables boldsymbolY_1dotsboldsymbolY_m are independent and identically distributed according to the random scale construction","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymbolY_i = delta R_i + (1-delta)  boldsymbolX_i","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where R_i sim textExp(1) and boldsymbolX_i is a bivariate random vector following a Gaussian copula with correlation rho and unit exponential margins. We note that the vector boldsymbolY_i does not itself have unit exponential margins. Instead, its marginal distribution function, F(ydelta) is dependent on delta; this has a closed form expression, see Huser and Wadsworth (2019). In practice, the parameter delta is unknown, and so the random scale construction is treated as a copula and fitted to standardised uniform data. That is, the data used for inference are boldsymbolZ_i = F(boldsymbolY_i delta) which have been transformed to a uniform scale via the delta-dependent marginal dsitribution function.   ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Simulation of the random scale mixture (on uniform margins) and its marginal ditribution function are provided below. Transforming the data to exponential margins can, in some cases, enhance training efficiency (Richards et al., 2024). However, for simplicity, we do not apply this transformation here.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Libraries used throughout this example\nusing NeuralEstimators, Flux\nusing Folds\nusing CUDA # GPU if it is available\nusing LinearAlgebra: Symmetric, cholesky\nusing Distributions: cdf, Uniform, Normal, quantile\nusing AlgebraOfGraphics, CairoMakie   \n\n# Sampling θ from the prior distribution\nfunction sample(K)\n\tρ = rand(Uniform(-0.99, 0.99), K)\n\tδ = rand(Uniform(0.0, 1.0), K)\n\tθ = vcat(ρ', δ')\n\treturn θ \nend\n\n# Marginal simulation of Z | θ\nfunction simulate(θ, m) \n\tZ = Folds.map(1:size(θ, 2)) do k\n\t\tρ = θ[1, k]\n\t\tδ = θ[2, k]\n\t\tΣ = [1 ρ; ρ 1]\n\t\tL = cholesky(Symmetric(Σ)).L\n\t\tX = L * randn(2, m)                 # Standard Gaussian margins\n\t\tX = -log.(1 .- cdf.(Normal(), X))   # Transform to unit exponential margins\n\t\tR = -log.(1 .- rand(1, m))         \n\t\tY = δ .* R .+ (1 - δ) .* X          \n\t\tZ = F.(Y; δ = δ)                     # Transform to uniform margins\n\tend\n\treturn Z\nend\n\n# Marginal distribution function; see Huser and Wadsworth (2019)\nfunction F(y; δ)\n\tif δ == 0.5 \n        u = 1 .- exp.(- 2 .* y) .* (1 .+ 2 .* y) \n    else \n        u = 1 .- (δ ./ (2 .* δ .- 1)) .* exp.(- y ./ δ) .+ ((1 .- δ) ./ (2 * δ .- 1)) .* exp.( - y ./ (1 - δ)) \n    end\n\treturn u\nend","category":"page"},{"location":"workflow/advancedusage/#General-censoring","page":"Advanced usage","title":"General censoring","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Inference with censored data can proceed in an analogous manner to the The masking approach for missing data. First, consider a vector boldsymbolW of indicator variables that encode the censoring pattern, with elements equal to one or zero if the corresponding element of the data boldsymbolZ equiv (Z_1 dots Z_n) is censored or observed, respectively. That is, boldsymbolW equiv (mathbbI(Z_j  leq c_j)  j = 1 dots n) where c_j, j = 1 dots n, is a censoring threshold. Second, consider an augmented data vector ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymbolU equiv boldsymbolZ odot boldsymbolW + boldsymbolv odot ( boldsymbol1 - boldsymbolW)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where boldsymbol1 is a vector of ones of appropriate dimension, boldsymbolv in mathbbR^n is user-defined, and odot denotes elementwise multiplication. A neural estimator for censored data is then trained on realisations of the augmented data set, boldsymbolU boldsymbolW. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The manner in which boldsymbolU and boldsymbolW are combined depends on the multivariate structure of the data and the chosen architecture. For example, when the data are gridded and the neural network is a CNN, then boldsymbolU and boldsymbolW can be concatenated along the channels dimension (i.e., the penultimate dimension of the array). In this example, we have replicated, unstructured bivariate data stored as matrices of dimension 2times m, where m denotes the number of independent replicates, and so the neural network is based on dense multilayer perceptrons (MLPs). In these settings, a simple way to combine boldsymbolU and boldsymbolW so that they can be passed through the neural network is to concatenate boldsymbolU and boldsymbolW along their first dimension, so that the resulting input is a matrix of dimension 4 times m. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The following helper function implements a simple version of the general censoring framework described above, based on a vector of censoring levels boldsymbolc and with boldsymbolv fixed to a constant such that the censoring mechanism and augmentation values do not vary with the model parameter values or with the replicate index.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Constructing augmented data from Z and the censoring threshold c\nfunction censorandaugment(Z; c, v = -1.0)\n    W = 1 * (Z .<= c)\n    U = ifelse.(Z .<= c, v, Z)\n    return vcat(U, W)\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above censoring function can then be incorporated into the data simulator as follows. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Marginal simulation of censored data\nfunction simulatecensored(θ, m; kwargs...) \n\tZ = simulate(θ, m)\n\tUW = Folds.map(Z) do Zₖ\n\t\tmapslices(Z -> censorandaugment(Z; kwargs...), Zₖ, dims = 1)\n\tend\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below, we construct a neural point estimator for censored data, based on a DeepSet architecture.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"n = 2    # dimension of each data replicate (bivariate)\nw = 128  # width of each hidden layer\n\n# Final layer has output dimension d=2 and enforces parameter constraints\nfinal_layer = Parallel(\n    vcat,\n    Dense(w, 1, tanh),    # ρ ∈ (-1,1)\n    Dense(w, 1, sigmoid)  # δ ∈ (0,1)\n)\nψ = Chain(Dense(n * 2, w, relu), Dense(w, w, relu))    \nϕ = Chain(Dense(w, w, relu), final_layer)           \nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = PointEstimator(network)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"We now train and assess two estimators for censored data; one with c = [0, 0] and one with c = [0.5, 0.5]. When the data boldsymbolZ are on uniform margins, the components of c can be interpreted as the expected number of censored values in each component; thus, c = [0, 0] corresonds to no censoring of the data, and c = [0.5, 0.5] corresponds to a situation where, on average, 50% of each dimension Z_j is censored. As expected, the neural estimator that uses non-censored data has lower RMSE, as the data it uses contain more information.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Number of independent replicates in each data set\nm = 200 \n\n# Train an estimator with no censoring\nsimulator1(θ, m) = simulatecensored(θ, m; c = [0, 0]) \nestimator1 = train(estimator, sample, simulator1, m = m) \n\n# Train an estimator with mild censoring\nsimulator2(θ, m) = simulatecensored(θ, m; c = [0.5, 0.5]) \nestimator2 = train(estimator, sample, simulator2, m = m)\n\n# Assessment\nθ_test = sample(1000) \nUW_test1 = simulator1(θ_test, m)\nUW_test2 = simulator2(θ_test, m)\nassessment1 = assess(estimator1, θ_test, UW_test1, parameter_names = [\"ρ\", \"δ\"], estimator_name = \"No censoring\") \nassessment2 = assess(estimator2, θ_test, UW_test2, parameter_names = [\"ρ\", \"δ\"], estimator_name = \"Mild censoring\")   \nassessment  = merge(assessment1, assessment2)\nrmse(assessment)\nplot(assessment)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Estimator Parameter RMSE\nNo censoring ρ 0.238167\nNo censoring δ 0.100688\nMild censoring ρ 0.394838\nMild censoring δ 0.135169","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"(Image: General censoring)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Here we have trained two separate neural estimators to handle two different censoring threshold vectors. However, one could train a single neural estimator that caters for a range of censoring thresholds, c, by allowing it to vary with the data samples and using it as an input to the neural network. In the next section, we illustrate this in the context of peaks-over-threshold modelling, whereby a single censoring threshold is defined to be the marginal tau-quantile of the data, and we amortise the estimator with respect to the probability level tau. In a peaks-over-threshold setting, variation in the censoring thresholds can be created by placing a prior on tau, which induces a prior on c.","category":"page"},{"location":"workflow/advancedusage/#Peaks-over-threshold-censoring","page":"Advanced usage","title":"Peaks-over-threshold censoring","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Richards et al. (2024) discuss neural Bayes estimation from censored data in the context of peaks-over-threshold extremal dependence modelling, where deliberate censoring of data is imposed to reduce estimation bias in the presence of marginally non-extreme events. In these settings, data are treated as censored if they do not exceed their corresponding marginal tau-quantile, for tau in (01) close to one. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Peaks-over-threshold censoring, with tau fixed, can be easily implemented using the General censoring framework by setting the censoring threshold equal to the tau-th quantile of the data boldsymbolZ. Further, Richards et al. (2024) show that one can amortise a neural estimator with respect to the choice of tau by treating it as an input to the neural network. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below, we sample a fixed set of K parameter-data pairs for training the neural network. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Sampling values of τ to use during training \nfunction sampleτ(K)\n\tτ = rand(Uniform(0.0, 0.9), 1, K) \nend\n\n# Adapt the censored data simulation to allow for τ as an input\nfunction simulatecensored(θ, τ, m; kwargs...) \n    Z = simulate(θ, m)\n\tK = size(θ, 2)\n\tUW = Folds.map(1:K) do k\n        Zₖ = Z[k]\n        τₖ = τ[k]\n        cₖ = τₖ # data are on uniform margins: censoring threshold equals τ\n        mapslices(Z -> censorandaugment(Z; c = cₖ, kwargs...), Zₖ, dims = 1)\n\tend\nend\n\n# Generate the data used for training and validation\nK = 50000  # number of training samples\nm = 500    # number of independent replicates in each data set\nθ_train  = sample(K)\nθ_val    = sample(K ÷ 5)\nτ_train  = sampleτ(K)\nτ_val    = sampleτ(K ÷ 5)\nUW_train = simulatecensored(θ_train, τ_train, m)\nUW_val   = simulatecensored(θ_val, τ_val, m)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In this example, the probability level tau can be incorporated as an input to the neural network by treating it as an input to the outer neural network of the DeepSet architecture. To do this, we increase the input dimension of the outer network by one, and then combine the data boldsymbolU boldsymbolW and tau as a tuple (see DeepSet for details). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Construct neural network based on DeepSet architecture\nψ = Chain(Dense(n * 2, w, relu),Dense(w, w, relu))    \nϕ = Chain(Dense(w + 1, w, relu), final_layer)\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = PointEstimator(network)\n\n# Train the estimator\nestimator = train(estimator, θ_train, θ_val, (UW_train, τ_train), (UW_val, τ_val))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Our trained estimator can now be used for any value of tau within the range used during training (tau in 009). Since the estimator is amortised with respect to tau, there is no need to retrain it for different degrees of censoring.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below, we assess the estimator for different values of tau. As expected, RMSE increases with tau (larger tau corresponds to more censoring). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Test parameters\nθ_test = sample(1000)\n\n# Assessment with τ fixed to 0 (no censoring)\nτ_test1  = fill(0.0, 1000)'\nUW_test1 = simulatecensored(θ_test, τ_test1, m)\nassessment1 = assess(estimator, θ_test, (UW_test1, τ_test1), parameter_names = [\"ρ\", \"δ\"], estimator_name = \"τ = 0\")   \n\n# Assessment with τ fixed to 0.8\nτ_test2  = fill(0.8, 1000)'\nUW_test2 = simulatecensored(θ_test, τ_test2, m)\nassessment2 = assess(estimator, θ_test, (UW_test2, τ_test2), parameter_names = [\"ρ\", \"δ\"], estimator_name = \"τ = 0.8\")   \n\n# Compare results between the two censoring probability levels\nassessment = merge(assessment1, assessment2)\nrmse(assessment)\nplot(assessment)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Estimator Parameter RMSE\nτ = 0 ρ 0.241780\nτ = 0 δ 0.096779\nτ = 0.80 ρ 0.476348\nτ = 0.80 δ 0.124913","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"(Image: Peaks-over-threshold censoring)","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Neural inferential methods have marked practical appeal, as their implementation is only loosely connected to the statistical or physical model being considered. The workflow when using the package NeuralEstimators is as follows:","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior, pi(boldsymboltheta), to form training/validation/test parameter sets. Alternatively, define a function to sample parameters dynamically during training. Parameters are stored as d times K matrices, with d the dimensionality of the parameter vector and K the number of parameter vectors in the given parameter set. \nSimulate data from the model conditional on the above parameter sets, to form training/validation/test data sets. Alternatively, define a function to simulate data dynamically during training. Simulated data sets are stored as mini-batches in a format amenable to the chosen neural-network architecture. For example, when constructing an estimator from data collected over a grid, one may use a generic CNN, with each data set stored in the final dimension of a four-dimensional array. When performing inference from replicated data, a DeepSet architecture may be used, where simulated data sets are stored in a vector, and conditionally independent replicates are stored as mini-batches within each element of the vector.\nIf constructing a neural posterior estimator, choose an approximate posterior distribution q(boldsymboltheta boldsymbolkappa). \nDesign and initialise a suitable neural network. The architecture class (e.g., MLP, CNN, GNN) should align with the multivariate structure of the data (e.g., unstructured, grid, graph). The specific input and output spaces depend on the chosen inferential method: \nFor neural Bayes estimators, the neural network is a mapping mathcalZtoTheta, where mathcalZ denotes the sample space and Theta denotes the parameter space.\nFor neural posterior estimators, the neural network is a mapping mathcalZtomathcalK, where mathcalK denotes the space of the approximate-distribution parameters boldsymbolkappa. \nFor neural ratio estimators, the neural network is a mapping mathcalZtimesThetatomathbbR. \nAny Flux model can be used to construct the neural network. Given K data sets stored appropriately (see Step 2 above), the neural network should output a matrix with K columns, where the number of rows corresponds to the dimensionality of the output spaces listed above.  \nWrap the neural network (and possibly the approximate distribution) in a subtype of NeuralEstimator corresponding to the intended inferential method:\nFor neural Bayes estimators under general, user-defined loss functions, use PointEstimator; \nFor neural posterior estimators, use PosteriorEstimator;\nFor neural ratio estimators, use RatioEstimator. \nTrain the NeuralEstimator using train() and the training set, monitoring performance and convergence using the validation set. For generic neural Bayes estimators, specify a loss function. \nAssess the NeuralEstimator using assess() and the test set. ","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Once the NeuralEstimator has passed our assessments and is deemed to be well calibrated, it may be used to make inference with observed data. ","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Next, see the Examples and, once familiar with the basic workflow, see Advanced usage for further practical considerations on how to most effectively construct neural estimators.","category":"page"},{"location":"API/core/#Core","page":"Core","title":"Core","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"This page documents the classes and functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these classes and functions appear in a standard implementation: from sampling parameters from the prior distribution, to making inference with observed data.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Parameters sampled from the prior distribution are stored as a d times K matrix, where d is the dimension of the parameter vector to make inference on and K is the number of sampled parameter vectors. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). The user-defined type should be a subtype of ParameterConfigurations, whose only requirement is a field θ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.\n\nThe user-defined type must have a field θ that stores the d × K matrix of parameters, where d is the dimension of the parameter vector to make  inference on and K is the number of sampled parameter vectors. There are no other requirements.\n\nSee subsetparameters() for the generic function for subsetting these objects. \n\nExamples\n\nstruct P <: ParameterConfigurations\n\tθ\n\t# other expensive intermediate objects...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The package accommodates any model for which simulation is feasible by allowing users to define their model implicitly through simulated data.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The data are stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and the subtype A depends on the multivariate structure of the data. Common formats include:","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Unstructured data: A is typically an n times m matrix, where:\nn is the dimension of each replicate (e.g., n=1 for univariate data, n=2 for bivariate data).  \nm is the number of independent replicates in each data set (m is allowed to vary between data sets). \nData collected over a regular grid: A is typically an (N + 2)-dimensional array, where: \nThe first N dimensions correspond to the dimensions of the grid (e.g., N = 1 for time series, N = 2 for two-dimensional spatial grids). \nThe penultimate dimension stores the so-called \"channels\" (e.g., singleton for univariate processes, two for bivariate processes). \nThe final dimension stores the m independent replicates. \nSpatial data collected over irregular locations: A is typically a GNNGraph, where independent replicates (possibly with differing spatial locations) are stored as subgraphs. See the helper function spatialgraph() for constructing these graphs from matrices of spatial locations and data. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"While the formats above cover many applications, the package is flexible: the data structure simply needs to align with the chosen neural-network architecture. ","category":"page"},{"location":"API/core/#Estimators","page":"Core","title":"Estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The package provides several classes of neural estimators, organised within a type hierarchy. At the top-level of the hierarchy is NeuralEstimator, an abstract supertype for all neural estimators in the package. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Neural Bayes estimators are implemented as subtypes of the abstract supertype BayesEstimator. The simple type PointEstimator is used for constructing neural Bayes estimators under general, user-defined loss functions. Several specialised types cater for the estimation of posterior quantiles based on the quantile loss function: see IntervalEstimator and its generalisation QuantileEstimator for estimating posterior quantiles for a fixed set of probability levels. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The type PosteriorEstimator can be used to approximate the posterior distribution, and RatioEstimator can be used to approximate the likelihood-to-evidence ratio.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Several types serve as wrappers around the aforementioned estimators, enhancing their functionality. PiecewiseEstimator applies different estimators based on the sample size of the data (see the discussion on Variable sample sizes). Finally, Ensemble combines multiple estimators, aggregating their individual estimates to improve accuracy.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimator\n\nBayesEstimator\n\nPointEstimator\n\nPosteriorEstimator\n\nRatioEstimator\n\nIntervalEstimator\n\nQuantileEstimator\n\nPiecewiseEstimator\n\nEnsemble","category":"page"},{"location":"API/core/#NeuralEstimators.NeuralEstimator","page":"Core","title":"NeuralEstimators.NeuralEstimator","text":"NeuralEstimator\n\nAn abstract supertype for all neural estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.BayesEstimator","page":"Core","title":"NeuralEstimators.BayesEstimator","text":"BayesEstimator <: NeuralEstimator\n\nAn abstract supertype for neural Bayes estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PointEstimator","page":"Core","title":"NeuralEstimators.PointEstimator","text":"PointEstimator <: BayesEstimator\nPointEstimator(network)\n\nA neural point estimator, where the neural network is a mapping from the sample space to the parameter space.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PosteriorEstimator","page":"Core","title":"NeuralEstimators.PosteriorEstimator","text":"PosteriorEstimator <: NeuralEstimator\nPosteriorEstimator(q::ApproximateDistribution, network)\n\nA neural estimator that approximates the posterior distribution p(boldsymboltheta mid boldsymbolZ), based on a neural network and an approximate distribution q (see the available in-built Approximate distributions). \n\nThe neural network is a mapping from the sample space to a space determined by the chosen approximate distribution q. Often, the output space is the space mathcalK of the approximate-distribution parameters boldsymbolkappa. However, for certain distributions (notably, NormalisingFlow), the neural network outputs summary statistics of suitable dimension (e.g., the dimension d of the parameter vector), which are then transformed into parameters of the approximate distribution using conventional multilayer perceptrons (see NormalisingFlow). \n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|μ,σ ~ N(μ, σ²) with priors μ ~ U(0, 1) and σ ~ U(0, 1)\nd = 2     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(θ, m) = [ϑ[1] .+ ϑ[2] .* randn32(n, m) for ϑ in eachcol(θ)]\n\n# Distribution used to approximate the posterior \nq = NormalisingFlow(d, d) \n\n# Neural network (outputs d summary statistics)\nw = 128   \nψ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, d))\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = PosteriorEstimator(q, network)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with observed data \nθ = [0.8f0 0.1f0]'\nZ = simulate(θ, m)\nsampleposterior(estimator, Z) # posterior draws \nposteriormean(estimator, Z)   # point estimate\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.RatioEstimator","page":"Core","title":"NeuralEstimators.RatioEstimator","text":"RatioEstimator <: NeuralEstimator\nRatioEstimator(network)\n\nA neural estimator that estimates the likelihood-to-evidence ratio,\n\nr(boldsymbolZ boldsymboltheta) equiv p(boldsymbolZ mid boldsymboltheta)p(boldsymbolZ)\n\nwhere p(boldsymbolZ mid boldsymboltheta) is the likelihood and p(boldsymbolZ) is the marginal likelihood, also known as the model evidence.\n\nFor numerical stability, training is done on the log-scale using the relation  log r(boldsymbolZ boldsymboltheta) = textlogit(c^*(boldsymbolZ boldsymboltheta)),  where c^*(cdot cdot) denotes the Bayes classifier as described in the Methodology section.  Hence, the neural network should be a mapping from mathcalZ times Theta to mathbbR, where mathcalZ and Theta denote the sample and parameter spaces, respectively. \n\nWhen the neural network is a DeepSet, two requirements must be met. First, the number of input neurons in the first layer of the outer network must equal d plus the number of output neurons in the final layer of the inner network.  Second, the number of output neurons in the final layer of the outer network must be one.\n\nWhen applying the estimator to data Z, by default the likelihood-to-evidence ratio r(boldsymbolZ boldsymboltheta) is returned (setting the keyword argument classifier = true will yield class probability estimates). The estimated ratio can then be used in various Bayesian (e.g., Hermans et al., 2020) or frequentist (e.g., Walchessen et al., 2024) inferential algorithms.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|μ,σ ~ N(μ, σ²) with priors μ ~ U(0, 1) and σ ~ U(0, 1)\nd = 2     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(θ, m) = [ϑ[1] .+ ϑ[2] .* randn32(n, m) for ϑ in eachcol(θ)]\n\n# Neural network\nw = 128 \nψ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w + d, w, relu), Dense(w, w, relu), Dense(w, 1))\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nr̂ = RatioEstimator(network)\n\n# Train the estimator\nr̂ = train(r̂, sample, simulate, m = m)\n\n# Inference with \"observed\" data (grid-based optimisation and sampling)\nθ = sample(1)\nz = simulate(θ, m)[1]\nθ_grid = f32(expandgrid(0:0.01:1, 0:0.01:1))'  # fine gridding of the parameter space\nr̂(z, θ_grid)                                   # likelihood-to-evidence ratios over grid\nmlestimate(r̂, z; θ_grid = θ_grid)              # maximum-likelihood estimate\nposteriormode(r̂, z; θ_grid = θ_grid)           # posterior mode \nsampleposterior(r̂, z; θ_grid = θ_grid)         # posterior samples\n\n# Inference with \"observed\" data (gradient-based optimisation using Optim.jl)\nusing Optim\nθ₀ = [0.5, 0.5]                                # initial estimate\nmlestimate(r̂, z; θ₀ = θ₀)                      # maximum-likelihood estimate\nposteriormode(r̂, z; θ₀ = θ₀)                   # posterior mode \n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.IntervalEstimator","page":"Core","title":"NeuralEstimators.IntervalEstimator","text":"IntervalEstimator <: BayesEstimator\nIntervalEstimator(u, v = u, c::Union{Function, Compress} = identity; probs = [0.025, 0.975], g = exp)\nIntervalEstimator(u, c::Union{Function, Compress}; probs = [0.025, 0.975], g = exp)\n\nA neural estimator that jointly estimates marginal posterior credible intervals based on the probability levels probs (by default, 95% central credible intervals).\n\nThe estimator employs a representation that prevents quantile crossing. Specifically, given data boldsymbolZ,  it constructs intervals for each parameter theta_i, i = 1 dots d  of the form,\n\nc_i(u_i(boldsymbolZ))  c_i(u_i(boldsymbolZ)) + g(v_i(boldsymbolZ)))\n\nwhere  boldsymbolu() equiv (u_1(cdot) dots u_d(cdot)) and boldsymbolv() equiv (v_1(cdot) dots v_d(cdot)) are neural networks that map from the sample space to mathbbR^d; g(cdot) is a monotonically increasing function (e.g., exponential or softplus); and each c_i() is a monotonically increasing function that maps its input to the prior support of theta_i.\n\nThe functions c_i() may be collectively defined by a d-dimensional Compress object, which can constrain the interval estimator's output to the prior support. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted. If only a single neural-network architecture is provided, it will be used for both boldsymbolu() and boldsymbolv().\n\nThe return value when applied to data using estimate() is a matrix with 2d rows, where the first and second d rows correspond to the lower and upper bounds, respectively. The function interval() can be used to format this output in a readable d × 2 matrix.  \n\nSee also QuantileEstimator.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|μ,σ ~ N(μ, σ²) with priors μ ~ U(0, 1) and σ ~ U(0, 1)\nd = 2     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 100   # number of independent replicates\nsample(K) = rand32(d, K)\nsimulate(θ, m) = [ϑ[1] .+ ϑ[2] .* randn(n, m) for ϑ in eachcol(θ)]\n\n# Neural network\nw = 128   # width of each hidden layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w, w, relu), Dense(w, d))\nu = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = IntervalEstimator(u)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with \"observed\" data \nθ = [0.8f0; 0.1f0]\nZ = simulate(θ, m)\nestimate(estimator, Z) \ninterval(estimator, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimator","page":"Core","title":"NeuralEstimators.QuantileEstimator","text":"QuantileEstimator <: BayesEstimator\nQuantileEstimator(v; probs = [0.025, 0.5, 0.975], g = Flux.softplus, i = nothing)\n\nA neural estimator that jointly estimates a fixed set of marginal posterior quantiles, with probability levels tau_1 dots tau_T controlled by the keyword argument probs. This generalises IntervalEstimator to support an arbitrary number of probability levels. \n\nGiven data boldsymbolZ, by default the estimator approximates quantiles of the distributions of \n\ntheta_i mid boldsymbolZ quad i = 1 dots d \n\nfor parameters boldsymboltheta equiv (theta_1 dots theta_d). Alternatively, if initialised with i set to a positive integer, the estimator approximates quantiles of the full conditional distribution of  \n\ntheta_i mid boldsymbolZ boldsymboltheta_-i\n\nwhere boldsymboltheta_-i denotes the parameter vector with its ith element removed. \n\nThe estimator employs a representation that prevents quantile crossing, namely,\n\nbeginaligned\nboldsymbolq^(tau_1)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ)\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ) + sum_j=2^t g(boldsymbolv^(tau_j)(boldsymbolZ)) quad t = 2 dots T\nendaligned\n\nwhere boldsymbolq^(tau)(boldsymbolZ) denotes the vector of tau-quantiles  for parameters boldsymboltheta equiv (theta_1 dots theta_d);  boldsymbolv^(tau_t)(cdot), t = 1 dots T, are neural networks that map from the sample space to mathbbR^d; and g(cdot) is a monotonically increasing function (e.g., exponential or softplus) applied elementwise to its arguments. If g = nothing, the quantiles are estimated independently through the representation\n\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_t)(boldsymbolZ) quad t = 1 dots T\n\nWhen the neural networks are DeepSet objects, two requirements must be met.  First, the number of input neurons in the first layer of the outer network must equal the number of neurons in the final layer of the inner network plus textdim(boldsymboltheta_-i), where we define  textdim(boldsymboltheta_-i) equiv 0 when targetting marginal posteriors of the form theta_i mid boldsymbolZ (the default behaviour).  Second, the number of output neurons in the final layer of the outer network must equal d - textdim(boldsymboltheta_-i). \n\nThe return value is a matrix with d - textdim(boldsymboltheta_-i) times T rows, where the first T rows correspond to the estimated quantiles for the first parameter, the second T rows corresponds to the estimated quantiles for the second parameter, and so on.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|μ,σ ~ N(μ, σ²) with priors μ ~ U(0, 1) and σ ~ U(0, 1)\nd = 2     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(θ, m) = [ϑ[1] .+ ϑ[2] .* randn32(n, m) for ϑ in eachcol(θ)]\n\n# ---- Quantiles of θᵢ ∣ 𝐙, i = 1, …, d ----\n\n# Neural network\nw = 64   # width of each hidden layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w, w, relu), Dense(w, d))\nv = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = QuantileEstimator(v)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with \"observed\" data \nθ = [0.8f0; 0.1f0]\nZ = simulate(θ, m)\nestimate(estimator, Z) \n\n# ---- Quantiles of θᵢ ∣ 𝐙, θ₋ᵢ ----\n\n# Neural network\nw = 64  # width of each hidden layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w + 1, w, relu), Dense(w, d - 1))\nv = DeepSet(ψ, ϕ)\n\n# Initialise estimators respectively targetting quantiles of μ∣Z,σ and σ∣Z,μ\nq₁ = QuantileEstimator(v; i = 1)\nq₂ = QuantileEstimator(v; i = 2)\n\n# Train the estimators\nq₁ = train(q₁, sample, simulate, m = m)\nq₂ = train(q₂, sample, simulate, m = m)\n\n# Estimate quantiles of μ∣Z,σ with σ = 0.5 and for many data sets\nθ₋ᵢ = 0.5f0\nq₁(Z, θ₋ᵢ)\n\n# Estimate quantiles of μ∣Z,σ with σ = 0.5 for a single data set\nq₁(Z[1], θ₋ᵢ)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator <: NeuralEstimator\nPiecewiseEstimator(estimators, changepoints)\n\nCreates a piecewise estimator (Sainsbury-Dale et al., 2024, Sec. 2.2.2) from a collection of estimators and sample-size changepoints.\n\nSpecifically, with l estimators and sample-size changepoints m_1  m_2  dots  m_l-1, the piecewise etimator takes the form,\n\nhatboldsymboltheta(boldsymbolZ)\n=\nbegincases\nhatboldsymboltheta_1(boldsymbolZ)  m leq m_1\nhatboldsymboltheta_2(boldsymbolZ)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta_l(boldsymbolZ)  m  m_l-1\nendcases\n\nFor example, given an estimator hatboldsymboltheta_1(cdot) trained for small sample sizes (e.g., m leq 30) and an estimator hatboldsymboltheta_2(cdot) trained for moderate-to-large sample sizes (e.g., m  30), one may construct a PiecewiseEstimator that dispatches hatboldsymboltheta_1(cdot) if m leq 30 and hatboldsymboltheta_2(cdot) otherwise.\n\nSee also trainmultiple().\n\nExamples\n\nusing NeuralEstimators, Flux\n\nn = 2    # bivariate data\nd = 3    # dimension of parameter vector \nw = 128  # width of each hidden layer\n\n# Small-sample estimator\nψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ₁ = Chain(Dense(w, w, relu), Dense(w, d));\nθ̂₁ = PointEstimator(DeepSet(ψ₁, ϕ₁))\n\n# Large-sample estimator\nψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ₂ = Chain(Dense(w, w, relu), Dense(w, d));\nθ̂₂ = PointEstimator(DeepSet(ψ₂, ϕ₂))\n\n# Piecewise estimator with changepoint m=30\nθ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], 30)\n\n# Apply the (untrained) piecewise estimator to data\nZ = [rand(n, m) for m ∈ (10, 50)]\nestimate(θ̂, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.Ensemble","page":"Core","title":"NeuralEstimators.Ensemble","text":"Ensemble <: NeuralEstimator\nEnsemble(estimators)\nEnsemble(architecture::Function, J::Integer)\n(ensemble::Ensemble)(Z; aggr = median)\n\nDefines an ensemble of estimators which, when applied to data Z, returns the median (or another summary defined by aggr) of the individual estimates (see, e.g., Sainsbury-Dale et al., 2025, Sec. S3).\n\nThe ensemble can be initialised with a collection of trained estimators and then applied immediately to observed data. Alternatively, the ensemble can be initialised with a collection of untrained estimators (or a function defining the architecture of each estimator, and the number of estimators in the ensemble), trained with train(), and then applied to observed data. In the latter case, where the ensemble is trained directly, if savepath is specified both the ensemble and component estimators will be saved.\n\nNote that train() currently acts sequentially on the component estimators.\n\nThe ensemble components can be accessed by indexing the ensemble; the number of component estimators can be obtained using length().\n\nSee also Parallel, which can be used to mimic ensemble methods with an appropriately chosen connection. \n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|θ ~ N(θ, 1) with θ ~ N(0, 1)\nd = 1     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsampler(K) = randn32(d, K)\nsimulator(θ, m) = [μ .+ randn32(n, m) for μ ∈ eachcol(θ)]\n\n# Neural-network architecture of each ensemble component\nfunction architecture()\n\tψ = Chain(Dense(n, 64, relu), Dense(64, 64, relu))\n\tϕ = Chain(Dense(64, 64, relu), Dense(64, d))\n\tnetwork = DeepSet(ψ, ϕ)\n\tPointEstimator(network)\nend\n\n# Initialise ensemble with three component estimators \nensemble = Ensemble(architecture, 3)\nensemble[1]      # access component estimators by indexing\nensemble[1:2]    # indexing with an iterable collection returns the corresponding ensemble \nlength(ensemble) # number of component estimators\n\n# Training\nensemble = train(ensemble, sampler, simulator, m = m, epochs = 5)\n\n# Assessment\nθ = sampler(1000)\nZ = simulator(θ, m)\nassessment = assess(ensemble, θ, Z)\nrmse(assessment)\n\n# Apply to data\nensemble(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The function train() is used to train a single neural estimator, while the wrapper function trainmultiple() is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"train\n\ntrainmultiple","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core","title":"NeuralEstimators.train","text":"train(estimator, sampler::Function, simulator::Function; ...)\ntrain(estimator, θ_train::P, θ_val::P, simulator::Function; ...) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\ntrain(estimator, θ_train::P, θ_val::P, Z_train::T, Z_val::T; ...) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\nTrains a neural estimator.\n\nThe methods cater for different variants of \"on-the-fly\" simulation. Specifically, a sampler can be provided to continuously sample new parameter vectors from the prior, and a simulator can be provided to continuously simulate new data conditional on the parameters. If provided with specific sets of parameters (θ_train and θ_val) and/or data (Z_train and Z_val), they will be held fixed during training.\n\nIn all methods, the validation parameters and data are held fixed to reduce noise when evaluating the validation risk.\n\nWhen the data are held fixed and the number of replicates in each element of Z_train is a multiple of the number of replicates in each element of Z_val, the training data will be recycled across epochs. For instance, if each element of Z_train contains 50 replicates and each element of Z_val contains 10 replicates, the first epoch will use the first 10 replicates of Z_train, the second epoch will use the next 10 replicates, and so on. Note that this recycling mechanism requires the data to be subsettable using subsetdata().\n\nThe estimator is returned on the CPU so that it can be easily saved post training. \n\nKeyword arguments common to all methods:\n\nloss = mae (applicable only to PointEstimator): loss function used to train the neural network. In addition to the standard loss functions provided by Flux (e.g., mae, mse), see Loss functions for further options. \nepochs = 100: number of epochs to train the neural network. An epoch is one complete pass through the entire training data set when doing stochastic gradient descent.\nstopping_epochs = 5: cease training if the risk does not improve in this number of epochs.\nbatchsize = 32: the batchsize to use when performing stochastic gradient descent, that is, the number of training samples processed between each update of the neural-network parameters.\noptimiser = Flux.setup(Adam(5e-4), estimator): any Optimisers.jl optimisation rule for updating the neural-network parameters. When the training data and/or parameters are held fixed, one may wish to employ regularisation to prevent overfitting; for example, optimiser = Flux.setup(OptimiserChain(WeightDecay(1e-4), Adam()), estimator), which corresponds to L₂ regularisation with penalty coefficient λ=10⁻⁴. \nlr_schedule::Union{Nothing, ParameterSchedulers.AbstractSchedule}: defines the learning-rate schedule for adaptively changing the learning rate during training. Accepts either a ParameterSchedulers.jl object or nothing for a fixed learning rate. By default, it uses CosAnneal with a maximum set to the initial learning rate from optimiser, a minimum of zero, and a period equal to the number of epochs. The learning rate is updated at the end of each epoch. \nuse_gpu = true: flag indicating whether to use a GPU if one is available.\nsavepath::Union{Nothing, String} = nothing: path to save the trained estimator and other information; if nothing (default), nothing is saved. Otherwise, the neural-network parameters (i.e., the weights and biases) will be saved during training as bson files; the risk function evaluated over the training and validation sets will also be saved, in the first and second columns of loss_per_epoch.csv, respectively; the best parameters (as measured by validation risk) will be saved as best_network.bson.\nverbose = true: flag indicating whether information, including empirical risk values and timings, should be printed to the console during training.\n\nKeyword arguments common to train(estimator, sampler, simulator) and train(estimator, θ_train, θ_val, simulator):\n\nm = nothing: arguments to the simulator (typically the number of replicates in each data set as an Integer or an Integer collection). The simulator is called as simulator(θ, m) if m is given and as simulator(θ) otherwise. \nepochs_per_Z_refresh = 1: the number of passes to make through the training set before the training data are refreshed.\nsimulate_just_in_time = false: flag indicating whether we should simulate just-in-time, in the sense that only a batchsize number of parameter vectors and corresponding data are in memory at a given time.\n\nKeyword arguments unique to train(estimator, sampler, simulator):\n\nK = 10000: number of parameter vectors in the training set.\nK_val = K ÷ 5 number of parameter vectors in the validation set.\nξ = nothing: an arbitrary collection of objects that, if provided, will be passed to the parameter sampler as sampler(K, ξ); otherwise, the parameter sampler will be called as sampler(K). Can also be provided as xi.\nepochs_per_θ_refresh = 1: the number of passes to make through the training set before the training parameters are refreshed. Must be a multiple of epochs_per_Z_refresh. Can also be provided as epochs_per_theta_refresh.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|μ,σ ~ N(μ, σ²) with priors μ ~ N(0, 1) and σ ~ U(0, 1)\nfunction sampler(K)\n\tμ = randn(K) # Gaussian prior\n\tσ = rand(K)  # Uniform prior\n\tθ = vcat(μ', σ')\n\treturn θ\nend\nfunction simulator(θ, m)\n\t[ϑ[1] .+ ϑ[2] * randn(1, m) for ϑ ∈ eachcol(θ)]\nend\n\n# Neural network \nd = 2     # dimension of the parameter vector θ\nn = 1     # dimension of each independent replicate of Z\nw = 128   # width of each hidden layer \nψ = Chain(Dense(n, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w, w, relu), Dense(w, d))\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nestimator = PointEstimator(network)\n\n# Number of independent replicates to use during training\nm = 15\n\n# Training: simulation on-the-fly\nestimator  = train(estimator, sampler, simulator, m = m)\n\n# Training: simulation on-the-fly with fixed parameters\nK = 10000\nθ_train = sampler(K)\nθ_val   = sampler(K)\nestimator = train(estimator, θ_train, θ_val, simulator, m = m)\n\n# Training: fixed parameters and fixed data\nZ_train   = simulator(θ_train, m)\nZ_val     = simulator(θ_val, m)\nestimator = train(estimator, θ_train, θ_val, Z_train, Z_val)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.trainmultiple","page":"Core","title":"NeuralEstimators.trainmultiple","text":"trainmultiple(estimator, sampler::Function, simulator::Function, m::Vector{Integer}; ...)\ntrainmultiple(estimator, θ_train, θ_val, simulator::Function, m::Vector{Integer}; ...)\ntrainmultiple(estimator, θ_train, θ_val, Z_train, Z_val, m::Vector{Integer}; ...)\ntrainmultiple(estimator, θ_train, θ_val, Z_train::V, Z_val::V; ...) where {V <: AbstractVector{AbstractVector{Any}}}\n\nA wrapper around train() to construct multiple neural estimators for different sample sizes m.\n\nThe positional argument m specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size (see Sainsbury-Dale at al., 2024, Sec 2.3.3). For example, if m = [m₁, m₂], the estimator for sample size m₂ is pre-trained with the estimator for sample size m₁.\n\nThe method for Z_train and Z_val subsets the data using subsetdata(Z, 1:mᵢ) for each mᵢ ∈ m. The method for Z_train::V and Z_val::V trains an estimator for each element of Z_train::V and Z_val::V and, hence, it does not need to invoke subsetdata(), which can be slow or difficult to define in some cases (e.g., for graphical data). Note that, in this case, m is inferred from the data.\n\nThe keyword arguments inherit from train(). The keyword arguments epochs, batchsize, stopping_epochs, and optimiser can each be given as vectors. For example, if training two estimators, one may use a different number of epochs for each estimator by providing epochs = [epoch₁, epoch₂].\n\nThe function returns a vector of neural estimators, each corresponding to a sample size in m.\n\nSee also PiecewiseEstimator.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessment/calibration","page":"Core","title":"Assessment/calibration","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"assess\n\nAssessment\n\nrisk\n\nbias\n\nrmse\n\ncoverage","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core","title":"NeuralEstimators.assess","text":"assess(estimator, θ, Z; ...)\nassess(estimators::Vector, θ, Z; ...)\n\nAssesses an estimator (or a collection of estimators) based on true parameters θ and corresponding simulated data Z.\n\nThe parameters θ should be given as a d × K matrix, where d is the parameter dimension and K is the number of sampled parameter vectors. \n\nThe function is currently only designed for the case that a DeepSet neural network is used, in which case the data Z should be a Vector, with each element representing a simulated data set (possibly containing independent replicates). If length(Z) is greater than K, θ will be recycled via horizontal concatenation: θ = repeat(θ, outer = (1, J)), where J = length(Z) ÷ K is the number of simulated data sets per parameter vector. This allows assessment of the estimator's sampling distribution under fixed parameters.\n\nThe return value is of type Assessment. \n\nKeyword arguments\n\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided). \nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nuse_gpu = true: Bool or collection of Bool objects with length equal to the number of estimators.\nprobs = nothing (applicable only to PointEstimator): probability levels taking values between 0 and 1. By default, no bootstrap uncertainty quantification is done; if probs is provided, it must be a two-element vector specifying the lower and upper probability levels for the non-parametric bootstrap intervals (note that parametric bootstrap is not currently supported with assess()).  \nB::Integer = 400 (applicable only to PointEstimator): number of bootstrap samples. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core","title":"NeuralEstimators.Assessment","text":"Assessment(df::DataFrame, runtime::DataFrame)\n\nA type for storing the output of assess(). The field runtime contains the total time taken for each estimator. The field df is a long-form DataFrame with columns:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size (number of exchangeable replicates) for the given data set\nk:         the index of the parameter vector\nj:         the index of the data set (in the case that multiple data sets are associated with each parameter vector)\n\nIf the estimator is an IntervalEstimator, the column estimate will be replaced by the columns lower and upper, containing the lower and upper bounds of the interval, respectively.\n\nIf the estimator is a QuantileEstimator, there will also be a column prob indicating the probability level of the corresponding quantile estimate.\n\nUse merge() to combine assessments from multiple estimators of the same type or join() to combine assessments from a PointEstimator and an IntervalEstimator.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.risk","page":"Core","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's Bayes risk,\n\nr(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K L(boldsymboltheta^(k) hatboldsymboltheta(boldsymbolZ^(k)))\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nKeyword arguments\n\nloss = (x, y) -> abs(x - y): a binary operator defining the loss function (default absolute-error loss).\naverage_over_parameters::Bool = false: if true, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the loss is averaged over all sample sizes m; otherwise, the loss is averaged over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bias","page":"Core","title":"NeuralEstimators.bias","text":"bias(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's bias,\n\ntextrmbias(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k)\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.rmse","page":"Core","title":"NeuralEstimators.rmse","text":"rmse(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's root-mean-squared error,\n\ntextrmrmse(hatboldsymboltheta(cdot))\napprox\nsqrtfrac1K sum_k=1^K hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k)^2\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.coverage","page":"Core","title":"NeuralEstimators.coverage","text":"coverage(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an interval estimator's expected coverage, as defined in Hermans et al. (2022, Definition 2.1), and the proportion of parameters below and above the lower and upper bounds, respectively.\n\nKeyword arguments\n\naverage_over_parameters::Bool = false: if true, the coverage is averaged over all parameters; otherwise (default), it is computed over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the coverage is averaged over all sample sizes m; otherwise, it is computed over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-with-observed-data","page":"Core","title":"Inference with observed data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The following functions facilitate the use of a trained neural estimator with observed data. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"estimate\n\nbootstrap\n\ninterval\n\nsampleposterior\n\nposteriormean \n\nposteriormedian\n\nposteriorquantile\n\nposteriormode","category":"page"},{"location":"API/core/#NeuralEstimators.estimate","page":"Core","title":"NeuralEstimators.estimate","text":"estimate(estimator, Z; batchsize::Integer = 32, use_gpu::Bool = true, kwargs...)\n\nApplies estimator to batches of Z of size batchsize, which can prevent memory issues that can occur with large data sets. \n\nBatching will only be done if there are multiple data sets in Z, which will be inferred by Z being a vector, or a tuple whose first element is a vector.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core","title":"NeuralEstimators.bootstrap","text":"bootstrap(estimator::PointEstimator, parameters::P, Z; use_gpu = true) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(estimator::PointEstimator, parameters::P, simulator, m::Integer; B = 400, use_gpu = true) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(estimator::PointEstimator, Z; B = 400, blocks = nothing, trim = true, use_gpu = true)\n\nGenerates B bootstrap estimates using estimator.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, Z, whose length implicitly defines B. Alternatively, one may provide a simulator and the desired sample size, in which case the data will be simulated using simulator(parameters, m).\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm generates resampled data sets by sampling blocks with replacement. If trim = true, the final block is trimmed as needed to ensure that the resampled data set matches the original size of Z. \n\nThe return type is a d × B matrix, where d is the dimension of the parameter vector. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.interval","page":"Core","title":"NeuralEstimators.interval","text":"interval(θ::Matrix; probs = [0.05, 0.95], parameter_names = nothing)\ninterval(estimator::IntervalEstimator, Z; parameter_names = nothing, use_gpu = true)\n\nComputes a confidence/credible interval based either on a d × B matrix θ of parameters (typically containing bootstrap estimates or posterior draws), where d denotes the number of parameters to make inference on, or from an IntervalEstimator and data Z.\n\nWhen given θ, the intervals are constructed by computing quantiles with probability levels controlled by the keyword argument probs.\n\nThe return type is a d × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval. The rows of this matrix can be named by passing a vector of strings to the keyword argument parameter_names. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.sampleposterior","page":"Core","title":"NeuralEstimators.sampleposterior","text":"sampleposterior(estimator::PosteriorEstimator, Z, N::Integer = 1000)\nsampleposterior(estimator::RatioEstimator, Z, N::Integer = 1000; θ_grid, prior::Function = θ -> 1f0)\n\nSamples from the approximate posterior distribution implied by estimator.\n\nThe positional argument N controls the size of the posterior sample.\n\nReturns d × N matrix of posterior samples, where d is the dimension of the parameter vector. If Z is a vector containing multiple data sets, a vector of matrices will be returned \n\nWhen sampling based on a RatioEstimator, the sampling algorithm is based on a fine-gridding of the parameter space, specified through the keyword argument θ_grid (or theta_grid).  The approximate posterior density is evaluated over this grid, which is then used to draw samples. This is effective when making inference with a small number of parameters. For models with a large number of parameters, other sampling algorithms may be needed (please contact the package maintainer).  The prior distribution p(boldsymboltheta) is controlled through the keyword argument prior (by default, a uniform prior is used).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormean","page":"Core","title":"NeuralEstimators.posteriormean","text":"posteriormean(θ::AbstractMatrix)\t\nposteriormean(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, N::Integer = 1000; kwargs...)\n\nComputes the posterior mean based either on a d × N matrix θ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nSee also posteriormedian(), posteriormode().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormedian","page":"Core","title":"NeuralEstimators.posteriormedian","text":"posteriormedian(θ::AbstractMatrix)\t\nposteriormedian(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, N::Integer = 1000; kwargs...)\n\nComputes the vector of marginal posterior medians based either on a d × N matrix θ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nSee also posteriormean(), posteriorquantile().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriorquantile","page":"Core","title":"NeuralEstimators.posteriorquantile","text":"posteriorquantile(θ::AbstractMatrix, probs)\t\nposteriormedian(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, probs, N::Integer = 1000; kwargs...)\n\nComputes the vector of marginal posterior quantiles with (a collection of) probability levels probs, based either on a d × N matrix θ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nThe return value is a d × length(probs) matrix. \n\nSee also posteriormedian().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormode","page":"Core","title":"NeuralEstimators.posteriormode","text":"posteriormode(estimator::RatioEstimator, Z; θ₀ = nothing, θ_grid = nothing, prior::Function = θ -> 1, use_gpu = true)\n\nComputes the (approximate) posterior mode (maximum a posteriori estimate) given data boldsymbolZ,\n\nundersetboldsymbolthetamathrmargmax ell(boldsymboltheta  boldsymbolZ) + log p(boldsymboltheta)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function implied by estimator, and p(boldsymboltheta) denotes the prior density function controlled through the keyword argument prior. Note that this estimate can be viewed as an approximate maximum penalised likelihood estimate, with penalty term p(boldsymboltheta). \n\nIf a vector θ₀ of initial parameter estimates is given, the approximate posterior density is maximised by gradient descent (requires Optim.jl to be loaded). Otherwise, if a matrix of parameters θ_grid is given, the approximate posterior density is maximised by grid search.\n\nSee also posteriormedian(), posteriormean().\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"Order = [:type, :function]\nPages   = [\"utility.md\"]","category":"page"},{"location":"API/utility/#Core","page":"Miscellaneous","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"numberreplicates\n\nsubsetdata\n\nsubsetparameters","category":"page"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Miscellaneous","title":"NeuralEstimators.numberreplicates","text":"numberreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Miscellaneous","title":"NeuralEstimators.subsetdata","text":"subsetdata(Z::V, i) where {V <: AbstractArray{A}} where {A <: Any}\nsubsetdata(Z::A, i) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, i) where {G <: AbstractGraph}\n\nReturn replicate(s) i from each data set in Z.\n\nIf the user is working with data that are not covered by the default methods, simply overload the function with the appropriate type for Z.\n\nFor graphical data, calls getgraph(), where the replicates are assumed be to stored as batched graphs. Since this can be slow, one should consider using a method of train() that does not require the data to be subsetted when working with graphical data (use numberreplicates() to check that the training and validation data sets are equally replicated, which prevents subsetting).\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nd = 1  # dimension of the response variable\nn = 4  # number of observations in each realisation\nm = 6  # number of replicates in each data set\nK = 2  # number of data sets\n\n# Array data\nZ = [rand(n, d, m) for k ∈ 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ∈ 1:m]) for k ∈ 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Miscellaneous","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Downstream-inference-algorithms","page":"Miscellaneous","title":"Downstream-inference algorithms","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"EM","category":"page"},{"location":"API/utility/#NeuralEstimators.EM","page":"Miscellaneous","title":"NeuralEstimators.EM","text":"EM(simulateconditional::Function, MAP::Union{Function, NeuralEstimator}, θ₀ = nothing)\n\nImplements the (Bayesian) Monte Carlo expectation-maximisation (EM) algorithm, with lth iteration\n\nboldsymboltheta^(l) =\nundersetboldsymbolthetamathrmargmax\nsum_h = 1^H ell(boldsymboltheta  boldsymbolZ_1  boldsymbolZ_2^(lh)) + Hlog pi(boldsymboltheta)\n\nwhere ell(cdot) is the complete-data log-likelihood function, boldsymbolZ equiv (boldsymbolZ_1 boldsymbolZ_2) denotes the complete data with boldsymbolZ_1 and boldsymbolZ_2 the observed and missing components, respectively, boldsymbolZ_2^(lh), h = 1 dots H, is simulated from the distribution of boldsymbolZ_2 mid boldsymbolZ_1 boldsymboltheta^(l-1), and pi(boldsymboltheta) denotes the prior density.\n\nFields\n\nThe function simulateconditional should have a signature of the form,\n\nsimulateconditional(Z::A, θ; nsims = 1) where {A <: AbstractArray{Union{Missing, T}}} where T\n\nThe output of simulateconditional should be the completed-data Z, and it should be returned in whatever form is appropriate to be passed to the MAP estimator as MAP(Z). For example, if the data are gridded and the MAP is a neural MAP estimator based on a CNN architecture, then Z should be returned as a four-dimensional array.\n\nThe field MAP can be a function (to facilitate the conventional Monte Carlo EM algorithm) or a NeuralEstimator (to facilitate the so-called neural EM algorithm).\n\nThe starting values θ₀ may be provided during initialisation (as a vector), or when applying the EM object to data (see below). The starting values given in a function call take precedence over those stored in the object.\n\nMethods\n\nOnce constructed, objects of type EM can be applied to data via the methods,\n\n(em::EM)(Z::A, θ₀::Union{Nothing, Vector} = nothing; ...) where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n(em::EM)(Z::V, θ₀::Union{Nothing, Vector, Matrix} = nothing; ...) where {V <: AbstractVector{A}} where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n\nwhere Z is the complete data containing the observed data and Missing values. Note that the second method caters for the case that one has multiple data sets. The keyword arguments are:\n\nnsims = 1: the number H of conditional simulations in each iteration.\nniterations = 50: the maximum number of iterations.\nnconsecutive = 3: the number of consecutive iterations for which the convergence criterion must be met.\nϵ = 0.01: tolerance used to assess convergence; the algorithm halts if the relative change in parameter values in successive iterations is less than ϵ.\nreturn_iterates::Bool: if true, the estimate at each iteration of the algorithm is returned; otherwise, only the final estimate is returned.\nξ = nothing: model information needed for conditional simulation (e.g., distance matrices) or in the MAP estimator.\nuse_ξ_in_simulateconditional::Bool = false: if set to true, the conditional simulator is called as simulateconditional(Z, θ, ξ; nsims = nsims).\nuse_ξ_in_MAP::Bool = false: if set to true, the MAP estimator is called as MAP(Z, ξ).\nuse_gpu::Bool = true\nverbose::Bool = false\n\nExamples\n\n# See the \"Missing data\" section in \"Advanced usage\"\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#Utility-functions","page":"Miscellaneous","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"adjacencymatrix\n\ncontainertype\n\nencodedata\n\nexpandgrid\n\nIndicatorWeights\n\nKernelWeights\n\nmaternchols\n\nremovedata\n\nrowwisenorm\n\nspatialgraph\n\nstackarrays\n\nvectotril","category":"page"},{"location":"API/utility/#NeuralEstimators.adjacencymatrix","page":"Miscellaneous","title":"NeuralEstimators.adjacencymatrix","text":"adjacencymatrix(S::Matrix, k::Integer; maxmin = false, combined = false)\nadjacencymatrix(S::Matrix, r::AbstractFloat)\nadjacencymatrix(S::Matrix, r::AbstractFloat, k::Integer; random = true)\nadjacencymatrix(M::Matrix; k, r, kwargs...)\n\nComputes a spatially weighted adjacency matrix from spatial locations S based  on either the k-nearest neighbours of each location; all nodes within a disc of fixed radius r; or, if both r and k are provided, a subset of k neighbours within a disc of fixed radius r.\n\nIf S is a square matrix, it is treated as a distance matrix; otherwise, it should be an n x d matrix, where n is the number of spatial locations and d is the spatial dimension (typically d = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that use of a  maxmin ordering currently requires a matrix of spatial locations (not a distance matrix).\n\nWhen using the k nearest neighbours, if maxmin=false (default) the neighbours are chosen based on all points in the graph. If maxmin=true, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the k-nearest neighbours amongst the points that have already appeared in the ordering. If combined=true, the  neighbours are defined to be the union of the k-nearest neighbours and the  k-nearest neighbours subject to a maxmin ordering. \n\nTwo subsampling strategies are implemented when choosing a subset of k neighbours within  a disc of fixed radius r. If random=true (default), the neighbours are randomly selected from  within the disc. If random=false, a deterministic algorithm is used  that aims to preserve the distribution of distances within the neighbourhood set, by choosing  those nodes with distances to the central node corresponding to the  0 frac1k frac2k dots frack-1k 1 quantiles of the empirical  distribution function of distances within the disc (this in fact yields up to k+1 neighbours,  since both the closest and furthest nodes are always included). \n\nBy convention with the functionality in GraphNeuralNetworks.jl which is based on directed graphs,  the neighbours of location i are stored in the column A[:, i] where A is the  returned adjacency matrix. Therefore, the number of neighbours for each location is given by collect(mapslices(nnz, A; dims = 1)), and the number of times each node is  a neighbour of another node is given by collect(mapslices(nnz, A; dims = 2)).\n\nBy convention, we do not consider a location to neighbour itself (i.e., the diagonal elements of the adjacency matrix are zero). \n\nExamples\n\nusing NeuralEstimators, Distances, SparseArrays\n\nn = 250\nd = 2\nS = rand(Float32, n, d)\nk = 10\nr = 0.10\n\n# Memory efficient constructors\nadjacencymatrix(S, k)\nadjacencymatrix(S, k; maxmin = true)\nadjacencymatrix(S, k; maxmin = true, combined = true)\nadjacencymatrix(S, r)\nadjacencymatrix(S, r, k)\nadjacencymatrix(S, r, k; random = false)\n\n# Construct from full distance matrix D\nD = pairwise(Euclidean(), S, dims = 1)\nadjacencymatrix(D, k)\nadjacencymatrix(D, r)\nadjacencymatrix(D, r, k)\nadjacencymatrix(D, r, k; random = false)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.containertype","page":"Miscellaneous","title":"NeuralEstimators.containertype","text":"containertype(A::Type)\ncontainertype(::Type{A}) where A <: SubArray\ncontainertype(a::A) where A\n\nReturns the container type of its argument.\n\nIf given a SubArray, returns the container type of the parent array.\n\nExamples\n\na = rand(3, 4)\ncontainertype(a)\ncontainertype(typeof(a))\n[containertype(x) for x ∈ eachcol(a)]\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.encodedata","page":"Miscellaneous","title":"NeuralEstimators.encodedata","text":"encodedata(Z::A; c::T = zero(T)) where {A <: AbstractArray{Union{Missing, T}, N}} where T, N\n\nFor data Z with missing entries, returns an encoded data set (U, W) where  U is the original data Z with missing entries replaced by a fixed constant c,  and W encodes the missingness pattern as an indicator array  equal to one if the corresponding element of Z is observed and zero otherwise.\n\nThe behavior depends on the dimensionality of Z. If Z has 1 or 2 dimensions,  the indicator array W is concatenated along the first dimension of Z. If Z has more than 2  dimensions, W is concatenated along the second-to-last dimension of Z. \n\nExamples\n\nusing NeuralEstimators\n\nZ = rand(16, 16, 1, 1)\nZ = removedata(Z, 0.25)\t# remove 25% of the data at random\nUW = encodedata(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Miscellaneous","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nGenerates a grid of all possible combinations of the elements from two input vectors, xs and ys. \n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.IndicatorWeights","page":"Miscellaneous","title":"NeuralEstimators.IndicatorWeights","text":"IndicatorWeights(h_max, n_bins::Integer)\n(w::IndicatorWeights)(h::Matrix)\n\nFor spatial locations boldsymbols and  boldsymbolu, creates a spatial weight function defined as\n\nboldsymbolw(boldsymbols boldsymbolu) equiv (mathbbI(h in B_k)  k = 1 dots K)\n\nwhere mathbbI(cdot) denotes the indicator function,  h equiv boldsymbols - boldsymbolu  is the spatial distance between boldsymbols and  boldsymbolu, and B_k  k = 1 dots K is a set of K =n_bins equally-sized distance bins covering the spatial distances between 0 and h_max. \n\nExamples\n\nusing NeuralEstimators \n\nh_max = 1\nn_bins = 10\nw = IndicatorWeights(h_max, n_bins)\nh = rand(1, 30) # distances between 30 pairs of spatial locations \nw(h)\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#NeuralEstimators.KernelWeights","page":"Miscellaneous","title":"NeuralEstimators.KernelWeights","text":"KernelWeights(h_max, n_bins::Integer)\n(w::KernelWeights)(h::Matrix)\n\nFor spatial locations boldsymbols and  boldsymbolu, creates a spatial weight function defined as\n\nboldsymbolw(boldsymbols boldsymbolu) equiv (exp(-(h - mu_k)^2  (2sigma_k^2))  k = 1 dots K)\n\nwhere h equiv boldsymbols - boldsymbolu is the spatial distance between boldsymbols and boldsymbolu, and mu_k  k = 1 dots K and sigma_k  k = 1 dots K are the means and standard deviations of the Gaussian kernels for each bin, covering the spatial distances between 0 and h_max.\n\nExamples\n\nusing NeuralEstimators \n\nh_max = 1\nn_bins = 10\nw = KernelWeights(h_max, n_bins)\nh = rand(1, 30) # distances between 30 pairs of spatial locations \nw(h)\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#NeuralEstimators.maternchols","page":"Miscellaneous","title":"NeuralEstimators.maternchols","text":"maternchols(D, ρ, ν, σ² = 1; stack = true)\n\nGiven a matrix D of distances, constructs the Cholesky factor of the covariance matrix under the Matérn covariance function with range parameter ρ, smoothness parameter ν, and marginal variance σ².\n\nProviding vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices D may also be provided.\n\nIf stack = true, the Cholesky factors will be \"stacked\" into a three-dimensional array (this is only possible if all distance matrices in D are the same size).\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra: norm\nn  = 10\nS  = rand(n, 2)\nD  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S), sⱼ ∈ eachrow(S)]\nρ  = [0.6, 0.5]\nν  = [0.7, 1.2]\nσ² = [0.2, 0.4]\nmaternchols(D, ρ, ν)\nmaternchols([D], ρ, ν)\nmaternchols(D, ρ, ν, σ²; stack = false)\n\nS̃  = rand(n, 2)\nD̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]\nmaternchols([D, D̃], ρ, ν, σ²)\nmaternchols([D, D̃], ρ, ν, σ²; stack = false)\n\nS̃  = rand(2n, 2)\nD̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]\nmaternchols([D, D̃], ρ, ν, σ²; stack = false)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.removedata","page":"Miscellaneous","title":"NeuralEstimators.removedata","text":"removedata(Z::Array, Iᵤ::Vector{T}) where T <: Union{Integer, CartesianIndex}\nremovedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)\nremovedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)\n\nReplaces elements of Z with missing.\n\nThe simplest method accepts a vector Iᵤ that specifes the indices of the data to be removed.\n\nAlternatively, there are two methods available to randomly generate missing data.\n\nFirst, a vector p may be given that specifies the proportion of missingness for each element in the response vector. Hence, p should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If prevent_complete_missing = true, no replicates will contain 100% missingness (note that this can slightly alter the effective values of p).\n\nSecond, if an integer n is provided, all replicates will contain n observations after the data are removed. If fixed_pattern = true, the missingness pattern is fixed for all replicates. If contiguous_pattern = true, the data will be removed in a contiguous block based on a randomly selected starting index. If variable_proportion = true, the proportion of missingness will vary across replicates, with each replicate containing between 1 and n observations after data removal, sampled uniformly (note that variable_proportion overrides fixed_pattern).\n\nThe return type is Array{Union{T, Missing}}.\n\nExamples\n\nd = 5           # dimension of each replicate\nm = 2000        # number of replicates\nZ = rand(d, m)  # simulated data\n\n# Passing a desired proportion of missingness\np = rand(d)\nremovedata(Z, p)\n\n# Passing a desired final sample size\nn = 3  # number of observed elements of each replicate: must have n <= d\nremovedata(Z, n)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.rowwisenorm","page":"Miscellaneous","title":"NeuralEstimators.rowwisenorm","text":"rowwisenorm(A)\n\nComputes the row-wise norm of a matrix A.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.spatialgraph","page":"Miscellaneous","title":"NeuralEstimators.spatialgraph","text":"spatialgraph(S)\nspatialgraph(S, Z)\nspatialgraph(g::GNNGraph, Z)\n\nGiven spatial data Z measured at spatial locations S, constructs a GNNGraph ready for use in a graph neural network that employs SpatialGraphConv layers. \n\nWhen m independent replicates are collected over the same set of n spatial locations,\n\nboldsymbols_1 dots boldsymbols_n subset mathcalD\n\nwhere mathcalD subset mathbbR^d denotes the spatial domain of interest,  Z should be given as an n times m matrix and S should be given as an n times d matrix.  Otherwise, when m independent replicates are collected over differing sets of spatial locations,\n\nboldsymbols_ij dots boldsymbols_in_i subset mathcalD quad i = 1 dots m\n\nZ should be given as an m-vector of n_i-vectors, and S should be given as an m-vector of n_i times d matrices.\n\nThe spatial information between neighbours is stored as an edge feature, with the specific  information controlled by the keyword arguments stationary and isotropic.  Specifically, the edge feature between node j and node j stores the spatial  distance boldsymbols_j - boldsymbols_j (if isotropic), the spatial  displacement boldsymbols_j - boldsymbols_j (if stationary), or the matrix of   locations (boldsymbols_j boldsymbols_j) (if !stationary).  \n\nAdditional keyword arguments inherit from adjacencymatrix() to determine the neighbourhood of each node, with the default being a randomly selected set of  k=30 neighbours within a disc of radius r=0.15 units.\n\nExamples\n\nusing NeuralEstimators\n\n# Number of replicates and spatial dimension\nm = 5  \nd = 2  \n\n# Spatial locations fixed for all replicates\nn = 100\nS = rand(n, d)\nZ = rand(n, m)\ng = spatialgraph(S, Z)\n\n# Spatial locations varying between replicates\nn = rand(50:100, m)\nS = rand.(n, d)\nZ = rand.(n)\ng = spatialgraph(S, Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Miscellaneous","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStacks a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ∈ (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ∈ (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.vectotril","page":"Miscellaneous","title":"NeuralEstimators.vectotril","text":"vectotril(v; strict = false)\nvectotriu(v; strict = false)\n\nConverts a vector v of length d(d+1)2 (a triangular number) into a d  d lower or upper triangular matrix.\n\nIf strict = true, the matrix will be strictly lower or upper triangular, that is, a (d+1)  (d+1) triangular matrix with zero diagonal.\n\nNote that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if v is a GPU array. Note also that the return type is not of type Triangular matrix (i.e., the zeros are materialised) since Triangular matrices are not always compatible with other GPU operations.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nn = d*(d+1)÷2\nv = collect(range(1, n))\nvectotril(v)\nvectotriu(v)\nvectotril(v; strict = true)\nvectotriu(v; strict = true)\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"When training an estimator of type PointEstimator, a loss function must be specified that determines the Bayes estimator that will be approximated. In addition to the standard loss functions provided by Flux (e.g., mae, mse, which allow for the approximation of posterior medians and means, respectively), the following loss functions are provided with the package. ","category":"page"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"tanhloss\n\nkpowerloss\n\nquantileloss\n\nintervalscore","category":"page"},{"location":"API/loss/#NeuralEstimators.tanhloss","page":"Loss functions","title":"NeuralEstimators.tanhloss","text":"tanhloss(θ̂, θ, κ; agg = mean)\n\nFor κ > 0, computes the loss function given in Sainsbury-Dale et al. (2025; Eqn. 14), namely,\n\nL(hatboldsymboltheta boldsymboltheta) = tanhbighatboldsymboltheta - boldsymboltheta_1kappabig)\n\nwhich yields the 0-1 loss function in the limit κ → 0. \n\nCompared with the kpowerloss(), which may also be used as a continuous approximation of the 0–1 loss function, the gradient of this loss is bounded as hatboldsymboltheta - boldsymboltheta_1 to 0, which can improve numerical stability during training. \n\nSee also kpowerloss().\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.kpowerloss","page":"Loss functions","title":"NeuralEstimators.kpowerloss","text":"kpowerloss(θ̂, θ, κ; agg = mean, safeorigin = true, ϵ = 0.1)\n\nFor κ > 0, the κ-th power absolute-distance loss function,\n\nL(hatboldsymboltheta boldsymboltheta) = hatboldsymboltheta - boldsymboltheta_1^kappa\n\ncontains the squared-error (κ = 2), absolute-error (κ = 2), and 0–1 (κ → 0) loss functions as special cases. It is Lipschitz continuous if κ = 1, convex if κ ≥ 1, and strictly convex if κ > 1. It is quasiconvex for all κ > 0.\n\nIf safeorigin = true, the loss function is modified to be piecewise, continuous, and linear in the ϵ-interval surrounding the origin, to avoid pathologies around the origin. \n\nSee also tanhloss().\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.quantileloss","page":"Loss functions","title":"NeuralEstimators.quantileloss","text":"quantileloss(θ̂, θ, τ; agg = mean)\nquantileloss(θ̂, θ, τ::Vector; agg = mean)\n\nThe asymmetric quantile loss function,\n\n  L(θ θ τ) = (θ - θ)(𝕀(θ - θ  0) - τ)\n\nwhere τ ∈ (0, 1) is a probability level and 𝕀(⋅) is the indicator function.\n\nThe method that takes τ as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in θ̂ is assumed to be dr, where d is the number of parameters and r is the number probability levels in τ (i.e., the length of τ).\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.intervalscore","page":"Loss functions","title":"NeuralEstimators.intervalscore","text":"intervalscore(l, u, θ, α; agg = mean)\nintervalscore(θ̂, θ, α; agg = mean)\nintervalscore(assessment::Assessment; average_over_parameters::Bool = false, average_over_sample_sizes::Bool = true)\n\nGiven an interval [l, u] with nominal coverage 100×(1-α)%  and true value θ, the interval score (Gneiting and Raftery, 2007) is defined as\n\nS(l u θ α) = (u - l) + 2α¹(l - θ)𝕀(θ  l) + 2α¹(θ - u)𝕀(θ  u)\n\nwhere α ∈ (0, 1) and 𝕀(⋅) is the indicator function.\n\nThe method that takes a single value θ̂ assumes that θ̂ is a matrix with 2d rows, where d is the dimension of the parameter vector to make inference on. The first and second sets of d rows will be used as l and u, respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/approximatedistributions/#Approximate-distributions","page":"Approximate distributions","title":"Approximate distributions","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"When constructing a PosteriorEstimator, one must choose an approximate distribution q(boldsymboltheta boldsymbolkappa) for the posterior distribution p(boldsymboltheta mid boldsymbolZ). These distributions are implemented as subtypes of the abstract supertype ApproximateDistribution. ","category":"page"},{"location":"API/approximatedistributions/#Distributions","page":"Approximate distributions","title":"Distributions","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"ApproximateDistribution\n\nGaussianDistribution\n\nNormalisingFlow","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.ApproximateDistribution","page":"Approximate distributions","title":"NeuralEstimators.ApproximateDistribution","text":"ApproximateDistribution\n\nAn abstract supertype for approximate posterior distributions used in conjunction with a PosteriorEstimator.\n\nSubtypes A <: ApproximateDistribution should provide methods logdensity(q::A, θ::AbstractMatrix, Z) and sampleposterior(q::A, Z, N::Integer). \n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#NeuralEstimators.GaussianDistribution","page":"Approximate distributions","title":"NeuralEstimators.GaussianDistribution","text":"GaussianDistribution <: ApproximateDistribution\nGaussianDistribution(d::Integer)\n\nA Gaussian distribution for amortised posterior inference, where d is the dimension of the parameter vector. \n\nThe approximate-distribution parameters boldsymbolkappa = (boldsymbolmu textrmvech(boldsymbolL)) consist of a d-dimensional mean parameter boldsymbolmu and the d(d+1)2 non-zero elements of the lower Cholesky factor boldsymbolL of a covariance matrix, where the half-vectorisation operator textrmvech(cdot) vectorises the lower triangle of its matrix argument.\n\nWhen using a GaussianDistribution as the approximate distribution of a PosteriorEstimator,  the neural network of the PosteriorEstimator should be a mapping from the sample space to mathbbR^mathcalK,  where mathcalK denotes the space of boldsymbolkappa. The dimension mathcalK can be determined from an object of type GaussianDistribution using numdistributionalparams(). Given the mathcalK-dimensional real-valued outputs of the neural network, a valid covariance matrix is constructed internally using CovarianceMatrix.\n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#NeuralEstimators.NormalisingFlow","page":"Approximate distributions","title":"NeuralEstimators.NormalisingFlow","text":"NormalisingFlow <: ApproximateDistribution\nNormalisingFlow(d::Integer, dstar::Integer; num_coupling_layers::Integer = 6, kwargs...)\n\nA normalising flow for amortised posterior inference (e.g., Ardizzone et al., 2019; Radev et al., 2022), where d is the dimension of  the parameter vector and dstar is the dimension of the summary statistics for the data. \n\nNormalising flows are diffeomorphisms (i.e., invertible, differentiable transformations with differentiable inverses) that map a simple base distribution (e.g., standard Gaussian) to a more complex target distribution (e.g., the posterior). They achieve this by applying a sequence of learned transformations, the forms of which are chosen to be invertible and allow for tractable density computation via the change of variables formula. This allows for efficient density evaluation during the training stage, and efficient sampling during the inference stage. For further details, see the reviews by Kobyzev et al. (2020) and Papamakarios (2021).\n\nNormalisingFlow uses affine coupling blocks (see AffineCouplingBlock), with activation normalisation (Kingma and Dhariwal, 2018) and permutations used between each block. The base distribution is taken to be a standard multivariate Gaussian distribution. \n\nWhen using a NormalisingFlow as the approximate distribution of a PosteriorEstimator,  the neural network of the PosteriorEstimator should be a mapping from the sample space to mathbbR^d^*,  where d^* is an appropriate number of summary statistics for the given parameter vector (e.g., d^* = d). The summary statistics are then mapped to the parameters of the affine coupling blocks using conventional multilayer perceptrons (see AffineCouplingBlock).\n\nKeyword arguments\n\nnum_coupling_layers::Integer = 6: number of coupling layers. \nkwargs: additional keyword arguments passed to AffineCouplingBlock. \n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#Methods","page":"Approximate distributions","title":"Methods","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"numdistributionalparams","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.numdistributionalparams","page":"Approximate distributions","title":"NeuralEstimators.numdistributionalparams","text":"numdistributionalparams(q::ApproximateDistribution)\nnumdistributionalparams(estimator::PosteriorEstimator)\n\nThe number of distributional parameters (i.e., the dimension of the space mathcalK of approximate-distribution parameters boldsymbolkappa). \n\n\n\n\n\n","category":"function"},{"location":"API/approximatedistributions/#Building-blocks","page":"Approximate distributions","title":"Building blocks","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"AffineCouplingBlock","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.AffineCouplingBlock","page":"Approximate distributions","title":"NeuralEstimators.AffineCouplingBlock","text":"AffineCouplingBlock(κ₁::MLP, κ₂::MLP)\nAffineCouplingBlock(d₁::Integer, dstar::Integer, d₂; kwargs...)\n\nAn affine coupling block used in a NormalisingFlow. \n\nAn affine coupling block splits its input boldsymboltheta into two disjoint components, boldsymboltheta_1 and boldsymboltheta_2, with dimensions d_1 and d_2, respectively. The block then applies the following transformation: \n\nbeginaligned\n    tildeboldsymboltheta_1 = boldsymboltheta_1\n    tildeboldsymboltheta_2 = boldsymboltheta_2 odot expboldsymbolkappa_boldsymbolgamma1(tildeboldsymboltheta_1 boldsymbolT(boldsymbolZ)) + boldsymbolkappa_boldsymbolgamma2(tildeboldsymboltheta_1 boldsymbolT(boldsymbolZ))\nendaligned\n\nwhere boldsymbolkappa_boldsymbolgamma1(cdot) and boldsymbolkappa_boldsymbolgamma2(cdot) are generic, non-invertible multilayer perceptrons (MLPs) that are functions of both the (transformed) first input component tildeboldsymboltheta_1 and the learned d^*-dimensional summary statistics boldsymbolT(boldsymbolZ) (see PosteriorEstimator). \n\nTo prevent numerical overflows and stabilise the training of the model, the scaling factors boldsymbolkappa_boldsymbolgamma1(cdot) are clamped using the function \n\nf(boldsymbols) = frac2cpitan^-1(fracboldsymbolsc)\n\nwhere c = 19 is a fixed clamping threshold. This transformation ensures that the scaling factors do not grow excessively large.\n\nAdditional keyword arguments kwargs are passed to the MLP constructor when creating κ₁ and κ₂. \n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Architectures","page":"Architectures","title":"Architectures","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In principle, any Flux model can be used to construct the neural network (see the Gridded data example). To integrate it into the workflow, one need only define a method that transforms K-dimensional vectors of data sets into matrices with K columns, where the number of rows corresponds to the dimensionality of the output spaces listed in the Overview. ","category":"page"},{"location":"API/architectures/#Modules","page":"Architectures","title":"Modules","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"The following high-level modules are often used when constructing the neural network. In particular, the type DeepSet serves as a convenient wrapper for embedding standard neural networks (e.g., MLPs, CNNs, GNNs) in a framework for making inference with an arbitrary number of independent replicates, and it comes with pre-defined methods for handling the transformations from a K-dimensional vector of data to a matrix output described above. ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DeepSet\n\nGNNSummary\n\nMLP","category":"page"},{"location":"API/architectures/#NeuralEstimators.DeepSet","page":"Architectures","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ, a = mean; S = nothing)\n(ds::DeepSet)(Z::Vector{A}) where A <: Any\n(ds::DeepSet)(tuple::Tuple{Vector{A}, Vector{Vector}}) where A <: Any\n\nThe DeepSets representation (Zaheer et al., 2017; Sainsbury-Dale et al., 2024),\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi(mathbfT(mathbfZ)) quad\nmathbfT(mathbfZ) = mathbfa(boldsymbolpsi(mathbfZ_i)  i = 1 dots m)\n\n\nwhere 𝐙 ≡ (𝐙₁', …, 𝐙ₘ')' are independent replicates of data,  ψ and ϕ are neural networks, and a is a permutation-invariant aggregation function. \n\nThe function a must operate on arrays and have a keyword argument dims for  specifying the dimension of aggregation (e.g., sum, mean, maximum, minimum, logsumexp).\n\nDeepSet objects act on data of type Vector{A}, where each element of the vector is associated with one data set (i.e., one set of independent replicates), and where A depends on the chosen architecture for ψ. As a rule of thumb, when A is an array, replicates are stored in the final dimension. For example, with gridded spatial data and ψ a CNN, A should be a 4-dimensional array, with replicates stored in the 4ᵗʰ dimension.  Note that, when using Flux, the final dimension is usually the \"batch\" dimension, but batching with DeepSet objects is done at the data-set level  (i.e., sets of replicates are always kept together). \n\nFor computational efficiency,  array data are first concatenated along their final dimension  (i.e., the replicates dimension) before being passed into the inner network ψ,  thereby ensuring that ψ is applied to a single large array, rather than multiple small ones. \n\nExpert summary statistics can be incorporated as\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfS(mathbfZ)))\n\nwhere S is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are provided either as a Function that returns a Vector, or as a vector of functions. In the case that ψ is set to nothing, only expert summary statistics will be used. See Expert summary statistics for further discussion on their use. \n\nSet-level inputs (e.g., covariates) 𝐗 can be passed directly into the outer network ϕ in the following manner: \n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfX))\n\nor, when expert summary statistics are also used,\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfS(mathbfZ) mathbfX))\n\nThis is done by calling the DeepSet object on a Tuple{Vector{A}, Vector{Vector}}, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level inputs (i.e., one vector for each data set).\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Two data sets containing 3 and 4 replicates\nd = 5  # number of parameters in the model\nn = 10 # dimension of each replicate\nZ = [rand32(n, m) for m ∈ (3, 4)]\n\n# Construct DeepSet object\nS = samplesize\ndₛ = 1   # dimension of expert summary statistic\ndₜ = 16  # dimension of neural summary statistic\nw  = 32  # width of hidden layers\nψ  = Chain(Dense(n, w, relu), Dense(w, dₜ, relu))\nϕ  = Chain(Dense(dₜ + dₛ, w, relu), Dense(w, d))\nds = DeepSet(ψ, ϕ; S = S)\n\n# Apply DeepSet object to data\nds(Z)\n\n# With set-level inputs \ndₓ = 2 # dimension of set-level inputs \nϕ  = Chain(Dense(dₜ + dₛ + dₓ, w, relu), Dense(w, d))\nds = DeepSet(ψ, ϕ; S = S)\nX  = [rand32(dₓ) for _ ∈ eachindex(Z)]\nds((Z, X))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.GNNSummary","page":"Architectures","title":"NeuralEstimators.GNNSummary","text":"GNNSummary(propagation, readout)\n\nA graph neural network (GNN) module designed to serve as the inner network ψ in the DeepSet representation when the data are graphical (e.g., irregularly observed spatial data).\n\nThe propagation module transforms graph data into a set of hidden-feature graphs. The readout module aggregates these feature graphs into a single hidden feature vector of fixed length. The network ψ is then defined as the composition of the propagation and readout modules.\n\nThe data should be stored as a GNNGraph or Vector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain subgraphs corresponding to independent replicates.\n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\nusing Flux: batch\nusing Statistics: mean\n\n# Propagation module\nr  = 1     # dimension of response variable\nnₕ = 32    # dimension of node feature vectors\npropagation = GNNChain(GraphConv(r => nₕ), GraphConv(nₕ => nₕ))\n\n# Readout module\nreadout = GlobalPool(mean)\n\n# Inner network\nψ = GNNSummary(propagation, readout)\n\n# Outer network\nd = 3     # output dimension \nw = 64    # width of hidden layer\nϕ = Chain(Dense(nₕ, w, relu), Dense(w, d))\n\n# DeepSet object \nds = DeepSet(ψ, ϕ)\n\n# Apply to data \ng₁ = rand_graph(11, 30, ndata = rand32(r, 11)) \ng₂ = rand_graph(13, 40, ndata = rand32(r, 13))\ng₃ = batch([g₁, g₂])  \nds(g₁)                # single graph \nds(g₃)                # graph with subgraphs corresponding to independent replicates\nds([g₁, g₂, g₃])      # vector of graphs, corresponding to multiple data sets \n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.MLP","page":"Architectures","title":"NeuralEstimators.MLP","text":"MLP(in::Integer, out::Integer; kwargs...)\n\nA traditional fully-connected multilayer perceptron (MLP) with input dimension in and output dimension out.\n\nThe method (mlp::MLP)(x, y) concatenates x and y along their first dimension before passing the result through the neural network. This functionality is used in constructs such as AffineCouplingBlock. \n\nKeyword arguments\n\ndepth::Integer = 2: the number of hidden layers.\nwidth::Integer = 128: the width of each hidden layer.\nactivation::Function = relu: the (non-linear) activation function used in each hidden layer.\noutput_activation::Function = identity: the activation function used in the output layer.\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#User-defined-summary-statistics","page":"Architectures","title":"User-defined summary statistics","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"summarystatistics.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"The following functions correspond to summary statistics that are often useful as user-defined summary statistics in DeepSet objects.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"samplesize\n\nsamplecorrelation\n\nsamplecovariance\n\nNeighbourhoodVariogram","category":"page"},{"location":"API/architectures/#NeuralEstimators.samplesize","page":"Architectures","title":"NeuralEstimators.samplesize","text":"samplesize(Z::AbstractArray)\n\nComputes the sample size of a set of independent realisations Z.\n\nNote that this function is a wrapper around numberreplicates, but this function returns the number of replicates as the eltype of Z, rather than as an integer.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecorrelation","page":"Architectures","title":"NeuralEstimators.samplecorrelation","text":"samplecorrelation(Z::AbstractArray)\n\nComputes the sample correlation matrix, R̂, and returns the vectorised strict lower triangle of R̂.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecorrelation(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecovariance","page":"Architectures","title":"NeuralEstimators.samplecovariance","text":"samplecovariance(Z::AbstractArray)\n\nComputes the sample covariance matrix, Σ̂, and returns the vectorised lower triangle of Σ̂.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecovariance(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.NeighbourhoodVariogram","page":"Architectures","title":"NeuralEstimators.NeighbourhoodVariogram","text":"NeighbourhoodVariogram(h_max, n_bins) \n(l::NeighbourhoodVariogram)(g::GNNGraph)\n\nComputes the empirical variogram, \n\nhatgamma(h pm delta) = frac12N(h pm delta) sum_(ij) in N(h pm delta) (Z_i - Z_j)^2\n\nwhere N(h pm delta) equiv left(ij)  boldsymbols_i - boldsymbols_j in (h-delta h+delta)right  is the set of pairs of locations separated by a distance within (h-delta h+delta), and cdot denotes set cardinality. \n\nThe distance bins are constructed to have constant width 2delta, chosen based on the maximum distance  h_max to be considered, and the specified number of bins n_bins. \n\nThe input type is a GNNGraph, and the empirical variogram is computed based on the corresponding graph structure.  Specifically, only locations that are considered neighbours will be used when computing the empirical variogram. \n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n  \n# Simulate Gaussian spatial data with exponential covariance function \nθ = 0.1                                 # true range parameter \nn = 250                                 # number of spatial locations \nS = rand(n, 2)                          # spatial locations \nD = pairwise(Euclidean(), S, dims = 1)  # distance matrix \nΣ = exp.(-D ./ θ)                       # covariance matrix \nL = cholesky(Symmetric(Σ)).L            # Cholesky factor \nm = 5                                   # number of independent replicates \nZ = L * randn(n, m)                     # simulated data \n\n# Construct the spatial graph \nr = 0.15                                # radius of neighbourhood set\ng = spatialgraph(S, Z, r = r)\n\n# Construct the variogram object with 10 bins\nnv = NeighbourhoodVariogram(r, 10) \n\n# Compute the empirical variogram \nnv(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Layers","page":"Architectures","title":"Layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In addition to the built-in layers provided by Flux, the following layers may be used when building a neural-network architecture.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DensePositive\n\nPowerDifference\n\nResidualBlock\n\nSpatialGraphConv","category":"page"},{"location":"API/architectures/#NeuralEstimators.DensePositive","page":"Architectures","title":"NeuralEstimators.DensePositive","text":"DensePositive(layer::Dense; g::Function = relu, last_only::Bool = false)\n\nWrapper around the standard Dense layer that ensures positive weights (biases are left unconstrained).\n\nThis layer can be useful for constucting (partially) monotonic neural networks. \n\nExamples\n\nusing NeuralEstimators, Flux\n\nl = DensePositive(Dense(5 => 2))\nx = rand32(5, 64)\nl(x)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.PowerDifference","page":"Architectures","title":"NeuralEstimators.PowerDifference","text":"PowerDifference(a, b)\n\nFunction f(x y) = ax - (1-a)y^b for trainable parameters a ∈ [0, 1] and b > 0.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Generate some data\nd = 5\nK = 10000\nX = randn32(d, K)\nY = randn32(d, K)\nXY = (X, Y)\na = 0.2f0\nb = 1.3f0\nZ = (abs.(a .* X - (1 .- a) .* Y)).^b\n\n# Initialise layer\nf = PowerDifference([0.5f0], [2.0f0])\n\n# Optimise the layer\nloader = Flux.DataLoader((XY, Z), batchsize=32, shuffle=false)\noptim = Flux.setup(Flux.Adam(0.01), f)\nfor epoch in 1:100\n    for (xy, z) in loader\n        loss, grads = Flux.withgradient(f) do m\n            Flux.mae(m(xy), z)\n        end\n        Flux.update!(optim, f, grads[1])\n    end\nend\n\n# Estimates of a and b\nf.a\nf.b\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.ResidualBlock","page":"Architectures","title":"NeuralEstimators.ResidualBlock","text":"ResidualBlock(filter, in => out; stride = 1)\n\nBasic residual block (see here), consisting of two sequential convolutional layers and a skip (shortcut) connection that connects the input of the block directly to the output, facilitating the training of deep networks.\n\nExamples\n\nusing NeuralEstimators\nz = rand(16, 16, 1, 1)\nb = ResidualBlock((3, 3), 1 => 32)\nb(z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.SpatialGraphConv","page":"Architectures","title":"NeuralEstimators.SpatialGraphConv","text":"SpatialGraphConv(in => out, g=relu; args...)\n\nImplements a spatial graph convolution for isotropic spatial processes (Sainsbury-Dale et al., 2025), \n\n boldsymbolh^(l)_j =\n gBig(\n boldsymbolGamma_1^(l) boldsymbolh^(l-1)_j\n +\n boldsymbolGamma_2^(l) barboldsymbolh^(l)_j\n +\n boldsymbolgamma^(l)\n Big)\n quad\n barboldsymbolh^(l)_j = sum_j in mathcalN(j)boldsymbolw^(l)(boldsymbols_j - boldsymbols_j) odot f^(l)(boldsymbolh^(l-1)_j boldsymbolh^(l-1)_j)\n\nwhere boldsymbolh^(l)_j is the hidden feature vector at location boldsymbols_j at layer l, g(cdot) is a non-linear activation function applied elementwise, boldsymbolGamma_1^(l) and boldsymbolGamma_2^(l) are trainable parameter matrices, boldsymbolgamma^(l) is a trainable bias vector, mathcalN(j) denotes the indices of neighbours of boldsymbols_j, boldsymbolw^(l)(cdot) is a (learnable) spatial weighting function, odot denotes elementwise multiplication,  and f^(l)(cdot cdot) is a (learnable) function. \n\nBy default, the function f^(l)(cdot cdot) is modelled using a PowerDifference function.  One may alternatively employ a nonlearnable function, for example, f = (hᵢ, hⱼ) -> (hᵢ - hⱼ).^2,  specified through the keyword argument f.  \n\nThe spatial distances between locations must be stored as an edge feature, as facilitated by spatialgraph().  The input to boldsymbolw^(l)(cdot) is a 1 times n matrix (i.e., a row vector) of spatial distances.  The output of boldsymbolw^(l)(cdot) must be either a scalar; a vector of the same dimension as the feature vectors of the previous layer;  or, if the features vectors of the previous layer are scalars, a vector of arbitrary dimension.  To promote identifiability, the weights are normalised to sum to one (row-wise) within each neighbourhood set.  By default, boldsymbolw^(l)(cdot) is taken to be a multilayer perceptron with a single hidden layer,  although a custom choice for this function can be provided using the keyword argument w. \n\nArguments\n\nin: dimension of input features.\nout: dimension of output features.\ng = relu: activation function.\nbias = true: add learnable bias?\ninit = glorot_uniform: initialiser for boldsymbolGamma_1^(l), boldsymbolGamma_2^(l), and boldsymbolgamma^(l). \nf = nothing\nw = nothing \nw_width = 128 (applicable only if w = nothing): the width of the hidden layer in the MLP used to model boldsymbolw^(l)(cdot cdot). \nw_out = in (applicable only if w = nothing): the output dimension of boldsymbolw^(l)(cdot cdot).  \n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\n\n# Toy spatial data\nn = 250                # number of spatial locations\nm = 5                  # number of independent replicates\nS = rand(n, 2)         # spatial locations\nZ = rand(n, m)         # data\ng = spatialgraph(S, Z) # construct the graph\n\n# Construct and apply spatial graph convolution layer\nl = SpatialGraphConv(1 => 10)\nl(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Output-layers","page":"Architectures","title":"Output layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"activationfunctions.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In addition to the standard activation functions provided by Flux (e.g., relu, softplus), the following layers can be used at the end of an architecture to ensure valid estimates for certain models. Note that the Flux layer Parallel can be useful for applying several different parameter constraints, as shown in the Univariate data example.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"note: Layers vs. activation functions\nAlthough we may conceptualise the following types as \"output activation functions\", they should be treated as separate layers included in the final stage of a Flux Chain(). In particular, they cannot be used as the activation function of a Dense layer. ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Compress\n\nCorrelationMatrix\n\nCovarianceMatrix","category":"page"},{"location":"API/architectures/#NeuralEstimators.Compress","page":"Architectures","title":"NeuralEstimators.Compress","text":"Compress(a, b, k = 1)\n\nLayer that compresses its input to be within the range a and b, where each element of a is less than the corresponding element of b.\n\nThe layer uses a logistic function,\n\nl(θ) = a + fracb - a1 + e^-kθ\n\nwhere the arguments a and b together combine to shift and scale the logistic function to the range (a, b), and the growth rate k controls the steepness of the curve.\n\nThe logistic function given here contains an additional parameter, θ₀, which is the input value corresponding to the functions midpoint. In Compress, we fix θ₀ = 0, since the output of a randomly initialised neural network is typically around zero.\n\nExamples\n\nusing NeuralEstimators, Flux\n\na = [25, 0.5, -pi/2]\nb = [500, 2.5, 0]\np = length(a)\nK = 100\nθ = randn(p, K)\nl = Compress(a, b)\nl(θ)\n\nn = 20\nθ̂ = Chain(Dense(n, p), l)\nZ = randn(n, K)\nθ̂(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CorrelationMatrix","page":"Architectures","title":"NeuralEstimators.CorrelationMatrix","text":"CorrelationMatrix(d)\n(object::CorrelationMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector 𝐯 ∈ ℝᵈ to the parameters of an unconstrained d×d correlation matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d×d correlation matrix.\n\nThe expected input is a Matrix with T(d-1) = (d-1)d÷2 rows, where T(d-1) is the (d-1)th triangular number (the number of free parameters in an unconstrained d×d correlation matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different correlation matrices).\n\nInternally, the layer constructs a valid Cholesky factor 𝐋 for a correlation matrix, and then extracts the strict lower triangle from the correlation matrix 𝐑 = 𝐋𝐋'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the correlation matrix\n\nbeginbmatrix\n1    R₁₂   R₁₃ \nR₂₁  1     R₂₃\nR₃₁  R₃₂  1\nendbmatrix\n\nthe rows of the matrix returned by a CorrelationMatrix layer are ordered as\n\nbeginbmatrix\nR₂₁ \nR₃₁ \nR₃₂ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied correlation matrices using vectotril and Symmetric.\n\nSee also CovarianceMatrix.\n\nExamples\n\nusing NeuralEstimators, LinearAlgebra, Flux\n\nd  = 4\nl  = CorrelationMatrix(d)\np  = (d-1)*d÷2\nθ  = randn(p, 100)\n\n# Returns a matrix of parameters, which can be converted to correlation matrices\nR = l(θ)\nR = map(eachcol(R)) do r\n\tR = Symmetric(cpu(vectotril(r, strict = true)), :L)\n\tR[diagind(R)] .= 1\n\tR\nend\n\n# Obtain the Cholesky factor directly\nL = l(θ, true)\nL = map(eachcol(L)) do x\n\t# Only the strict lower diagonal elements are returned\n\tL = LowerTriangular(cpu(vectotril(x, strict = true)))\n\n\t# Diagonal elements are determined under the constraint diag(L*L') = 𝟏\n\tL[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)\n\tL\nend\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CovarianceMatrix","page":"Architectures","title":"NeuralEstimators.CovarianceMatrix","text":"CovarianceMatrix(d)\n(object::CovarianceMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector 𝐯 ∈ ℝᵈ to the parameters of an unconstrained d×d covariance matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d×d covariance matrix.\n\nThe expected input is a Matrix with T(d) = d(d+1)÷2 rows, where T(d) is the dth triangular number (the number of free parameters in an unconstrained d×d covariance matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different covariance matrices).\n\nInternally, the layer constructs a valid Cholesky factor 𝐋 and then extracts the lower triangle from the positive-definite covariance matrix 𝚺 = 𝐋𝐋'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the covariance matrix\n\nbeginbmatrix\nΣ₁₁  Σ₁₂  Σ₁₃ \nΣ₂₁  Σ₂₂  Σ₂₃ \nΣ₃₁  Σ₃₂  Σ₃₃ \nendbmatrix\n\nthe rows of the matrix returned by a CovarianceMatrix are ordered as\n\nbeginbmatrix\nΣ₁₁ \nΣ₂₁ \nΣ₃₁ \nΣ₂₂ \nΣ₃₂ \nΣ₃₃ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied covariance matrices using vectotril and Symmetric.\n\nSee also CorrelationMatrix.\n\nExamples\n\nusing NeuralEstimators, Flux, LinearAlgebra\n\nd = 4\nl = CovarianceMatrix(d)\np = d*(d+1)÷2\nθ = randn(p, 50)\n\n# Returns a matrix of parameters, which can be converted to covariance matrices\nΣ = l(θ)\nΣ = [Symmetric(cpu(vectotril(x)), :L) for x ∈ eachcol(Σ)]\n\n# Obtain the Cholesky factor directly\nL = l(θ, true)\nL = [LowerTriangular(cpu(vectotril(x))) for x ∈ eachcol(L)]\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"NeuralEstimators facilitates a suite of neural methods for parameter inference in scenarios where simulation from the model is feasible. These methods are likelihood-free and amortised, in the sense that, once the neural networks are trained on simulated data, they enable rapid inference across arbitrarily many observed data sets in a fraction of the time required by conventional approaches. ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The package supports neural Bayes estimators, which transform data into point summaries of the posterior distribution; neural posterior estimators, which perform approximate posterior inference via KL-divergence minimisation; and neural ratio estimators, which approximate the likelihood-to-evidence ratio and thereby enable frequentist or Bayesian inference through various downstream algorithms, such as MCMC sampling. ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"User-friendliness is a central focus of the package, which is designed to minimise \"boilerplate\" code while preserving complete flexibility in the neural-network architecture and other workflow components. The package accommodates any model for which simulation is feasible by allowing users to define their model implicitly through simulated data. A convenient interface for R users is available on CRAN.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the Methodology, see the Overview of the package workflow and the Examples, or refer to the Quick start section below.","category":"page"},{"location":"#Installation","page":"NeuralEstimators","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"To install the package, please first install the current stable release of Julia. Then, one may install the current stable version of the package using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Alternatively, one may install the current development version using the command:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(url = \"https://github.com/msainsburydale/NeuralEstimators.jl\")","category":"page"},{"location":"#Quick-start","page":"NeuralEstimators","title":"Quick start","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"In the following minimal example, we develop a neural Bayes estimator for boldsymboltheta equiv (mu sigma) from data boldsymbolZ equiv (Z_1 dots Z_m), where each Z_i oversetmathrmiidsim N(mu sigma^2). ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using NeuralEstimators, Flux\n\n# Priors μ,σ ~ U(0, 1) and data Zᵢ|μ,σ ~ N(μ, σ²), i = 1,…, m\nd = 2    # dimension of the parameter vector θ\nn = 1    # dimension of each data replicate Zᵢ\nsample(K) = rand(d, K) \nsimulate(θ, m = 100) = [ϑ[1] .+ ϑ[2] * randn(n, m) for ϑ ∈ eachcol(θ)]  \n\n# Neural network, based on the DeepSets architecture\nw = 128  # width of each hidden layer \nψ = Chain(Dense(n, w, relu), Dense(w, w, relu))\nϕ = Chain(Dense(w, w, relu), Dense(w, d))\nnetwork = DeepSet(ψ, ϕ)\n\n# Initialise a neural point estimator\nestimator = PointEstimator(network) \n\n# Train the estimator\nestimator = train(estimator, sample, simulate, epochs = 20)\n\n# Assess the estimator\nθ_test = sample(1000)\nZ_test = simulate(θ_test)\nassessment = assess(estimator, θ_test, Z_test)\nbias(assessment)   # μ = 0.001, σ = 0.001\nrmse(assessment)   # μ = 0.05,  σ = 0.04\n\n# Apply the estimator to observed data\nθ = [0.8 0.1]'          # true parameters\nZ = simulate(θ)         # \"observed\" data\nestimate(estimator, Z)  # point estimate: μ̂ = 0.797, σ̂ = 0.087","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use it in your research or other activities, please also use the following citations.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@Manual{,\n    title = {{NeuralEstimators}: Likelihood-Free Parameter Estimation\n      using Neural Networks},\n    author = {Matthew Sainsbury-Dale},\n    year = {2024},\n    note = {R package version 0.1-2},\n    url = {https://CRAN.R-project.org/package=NeuralEstimators},\n    doi = {10.32614/CRAN.package.NeuralEstimators},\n  }\n\n@Article{,\n    title = {Likelihood-Free Parameter Estimation with Neural {B}ayes\n      Estimators},\n    author = {Matthew Sainsbury-Dale and Andrew Zammit-Mangion and\n      Raphael Huser},\n    journal = {The American Statistician},\n    year = {2024},\n    volume = {78},\n    pages = {1--14},\n    doi = {10.1080/00031305.2023.2249522},\n  }","category":"page"},{"location":"#Contributing","page":"NeuralEstimators","title":"Contributing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"If you encounter a bug or have a suggestion, please consider opening an issue or submitting a pull request. Instructions for contributing to the documentation can be found in docs/README.md. When adding functionality to the package, you may wish to add unit tests to the file test/runtests.jl. You can then run these tests locally by executing the following command from the root folder:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"julia --project=. -e \"using Pkg; Pkg.test()\"","category":"page"},{"location":"#Papers-using-NeuralEstimators","page":"NeuralEstimators","title":"Papers using NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Likelihood-free parameter estimation with neural Bayes estimators [paper] [code]\nNeural methods for amortized inference [paper][code]\nNeural Bayes estimators for irregular spatial data using graph neural networks [paper][code]\nNeural Bayes estimators for censored inference with peaks-over-threshold models [paper] [code]\nNeural parameter estimation with incomplete data [paper][code]","category":"page"}]
}
