var documenterSearchIndex = {"docs":
[{"location":"framework/#Theoretical-framework","page":"Theoretical framework","title":"Theoretical framework","text":"","category":"section"},{"location":"framework/#Bayes-estimators","page":"Theoretical framework","title":"Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"A statistical model is a set of probability distributions mathcalP on a sample space mathcalS. A parametric statistical model is one where the probability distributions in mathcalP are parameterised via some p-dimensional parameter vector mathbftheta, that is, where mathcalP equiv P_mathbftheta  mathbftheta in Theta, where Theta is the parameter space. Suppose that we have m mutually independent realisations from P_mathbftheta in mathcalP, which we collect in mathbfZ equiv (mathbfZ_1dotsmathbfZ_m). Then, the goal of parameter point estimation is to infer the unknown mathbftheta from mathbfZ using an estimator,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"hatmathbftheta  mathcalS^m to Theta","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"a mapping from m independent realisations from mathcalP_mathbftheta to the parameter space.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Estimators can be constructed intuitively within a decision-theoretic framework. Consider a non-negative loss function, L(mathbftheta hatmathbftheta(mathbfZ)), which assesses an estimator hatmathbftheta(cdot) for a given mathbftheta and data set mathbfZ.    The estimator's risk function is the loss averaged over all possible data realisations. Assume, without loss of generality, that our sample space is mathcalS = mathbbR^n. Then, the risk function is","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" R(mathbftheta hatmathbftheta(cdot)) equiv int_mathcalS^m  L(mathbftheta hatmathbftheta(mathbfZ))p(mathbfZ mid mathbftheta) textd mathbfZ","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"where p(mathbfZ mid mathbftheta) = prod_i=1^mp(mathbfZ_i mid mathbftheta) is the likelihood function. A ubiquitous approach in estimator design is to minimise a weighted summary of the risk function known as the Bayes risk,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" r_Omega(hatmathbftheta(cdot))\n equiv int_Theta R(mathbftheta hatmathbftheta(cdot)) textdOmega(mathbftheta)  ","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"where Omega(cdot) is a prior measure which, for ease of exposition, we will assume admits a density p(cdot) with respect to Lebesgue measure. A minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdotcdot) and Omega(cdot).","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"The Bayes risk cannot typically be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of K parameter vectors sampled from the prior Omega(cdot) denoted by vartheta  and, for each mathbftheta in vartheta, J sets of m mutually independent realisations from P_mathbftheta collected in mathcalZ_mathbftheta,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" r_Omega(hatmathbftheta(cdot))\n approx\nfrac1K sum_mathbftheta in vartheta frac1J sum_mathbfZ in mathcalZ_mathbftheta L(mathbftheta hatmathbftheta(mathbfZ))  ","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Theoretical framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Neural networks are universal function approximators and, hence, they are natural choices for constructing estimators. Let hatmathbftheta(cdot mathbfgamma) denote a neural estimator, that is, a neural network parameterised by mathbfgamma that transforms data into parameter estimates. Then, our neural estimator is hatmathbftheta(cdot mathbfgamma^*), where","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"mathbfgamma^*\nequiv\nundersetmathbfgammamathrmargmin  r_Omega(hatmathbftheta(cdot mathbfgamma))","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"with the Bayes risk approximated using Monte Carlo methods. Since the resulting neural estimator minimises (a Monte Carlo approximation of) the Bayes risk, we call it a neural Bayes estimator.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Under mild conditions, Bayes estimators are invariant to permutations of the conditionally independent data mathbfZ. Hence, we represent our neural estimators in the Deep Set framework, which is a universal representation for permutation-invariant functions. Specifically, we model our neural estimators as","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"hatmathbftheta(mathbfZ mathbfgamma) = mathbfphi(mathbfT(mathbfZ mathbfgamma) mathbfgamma) quad mathbfT(mathbfZ mathbfgamma)  \n= mathbfabig(mathbfpsi(mathbfZ_i mathbfgamma)  i = 1 dots mbig)","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"where mathbfphi mathbbR^q to mathbbR^p and mathbfpsi mathbbR^n to mathbbR^q are neural networks whose parameters are collected in mathbfgamma, and mathbfa (mathbbR^q)^m to mathbbR^q is a permutation-invariant set function (typically elementwise addition, average, or maximum).","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Theoretical framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"The neural Bayes estimator is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Define Omega(cdot), the prior distribution for mathbftheta.\nSample parameters from Omega(cdot) to form sets of parameters vartheta_texttrain, vartheta_textval, and vartheta_texttest.\nSimulate data from the model, mathcalP, using these sets of parameters, yielding the data sets mathcalZ_texttrain, mathcalZ_textval, and mathcalZ_texttest, respectively.\nChoose a loss function L(cdot cdot).\nDesign neural network architectures for mathbfphi(cdot mathbfgamma) and mathbfpsi(cdot mathbfgamma).\nUsing the training sets mathcalZ_textrmtrain and vartheta_texttrain, train the neural network under L(cdotcdot) to obtain the neural Bayes estimator, hatmathbftheta(cdot mathbfgamma^*). During training, continuously monitor progress based on mathcalZ_textrmval and vartheta_textval.\nAssess hatmathbftheta(cdot mathbfgamma^*) using mathcalZ_textrmtest and vartheta_texttest.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"API/utility/#Utility-functions","page":"Utility functions","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/#Core","page":"Utility functions","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"subsetparameters\n\nnumberreplicates\n\nsubsetdata","category":"page"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Utility functions","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Utility functions","title":"NeuralEstimators.numberreplicates","text":"numberofreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Utility functions","title":"NeuralEstimators.subsetdata","text":"Generic function for subsetting replicates from a data set. Default methods are:\n\nsubsetdata(Z::A, m) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, m) where {G <: AbstractGraph}\n\nNote that subsetdata is slow for graphical data, and one should consider using a method of train that does not require the data to be subsetted. Use numberreplicates to check that the training and validation data sets are equally replicated, which prevents the invocation of subsetdata. Note also that subsetdata only applies to vectors of batched graphs.\n\nIf the user is working with data that is not covered by the default methods, simply overload subsetdata with the appropriate type for Z.\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nn = 5  # number of observations in each realisation\nm = 6  # number of replicates for each parameter vector\nd = 1  # dimension of the response variable\nK = 2  # number of parameter vectors\n\n# Array data\nZ = [rand(n, d, m) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates for each parameter vector\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ‚àà 1:m]) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates for each parameter vector\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Utility functions","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"loadbestweights\n\nstackarrays\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Utility functions","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nGiven a path to a training run containing neural networks saved with names \"network_epochx.bson\" and an object saved as \"loss_per_epoch.bson\",  returns the weights of the best network (measured by validation loss).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Utility functions","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Utility functions","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In this section, we discuss practical considerations on how to construct neural estimators most effectively.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution Omega(cdot) may be stored in two ways. Most simply, they can be stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution; this is the approach taken in the example using univariate Gaussian data. Alternatively, they can be stored in a user-defined subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the p times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, mathcalZ_texttrain can be simulated periodically during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Refreshing mathcalZ_texttrain also has an additional computational benefit; data can be simulated \"just-in-time\", in the sense that they can be simulated from a small batch of vartheta_texttrain, used to train the neural estimator, and then removed from memory. This can reduce pressure on memory resources when vartheta_texttrain is very large.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh vartheta_texttrain, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models.  ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with the various methods of train.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as mathbfZ^(m) equiv (mathbfZ_1 dots mathbfZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatmathbftheta(mathbfZ^(m) mathbfgamma^*)\n=\nbegincases\nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_1)  m leq m_1\nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatmathbftheta(mathbfZ^(m) mathbfgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where, here, mathbfgamma^* equiv (mathbfgamma^*_tildem_1 dots mathbfgamma^*_tildem_l-1), and where mathbfgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatmathbftheta(cdot mathbfgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise neural estimators are implemented with the struct, PiecewiseEstimator. The method train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T, M::Vector{I}) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}, I <: Integer} is particularly useful for training piecewise neural estimators. Below, we replicate the example of inferring mu and sigma from N(mu sigma^2) data, but this time we train three neural estimators with sample sizes tildem_l equal to 1, 10, and 30, respectively.   ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nimport NeuralEstimators: simulate\nusing Flux\nusing Distributions\n\nŒ© = (Œº = Normal(0, 1), œÉ = Uniform(0.1, 1))\n\nfunction sample(Œ©, K)\n\tŒº = rand(Œ©.Œº, K)\n\tœÉ = rand(Œ©.œÉ, K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\treturn Œ∏\nend\n\nŒ∏_train = sample(Œ©, 10000)\nŒ∏_val   = sample(Œ©, 2000)\n\nfunction simulate(Œ∏_set, m)\n\tZ = [rand(Normal(Œ∏[1], Œ∏[2]), 1, 1, m) for Œ∏ ‚àà eachcol(Œ∏_set)]\n\tZ = broadcast.(Float32, Z)\n\treturn Z\nend\n\nM = [1, 10, 30]\nZ_train = simulate(Œ∏_train, maximum(M))\nZ_val   = simulate(Œ∏_val, maximum(M))\n\nn = 1    # size of each replicate (univariate data)\nw = 32   # number of neurons in each layer\np = 2    # number of parameters in the statistical model\n\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p), Flux.flatten)\nŒ∏ÃÇ = DeepSet(œà, œï)\n\nestimators = train(Œ∏ÃÇ , Œ∏_train, Œ∏_val, Z_train, Z_val, M, epochs = 10)\nmchange = [5, 20]\n\npiecewise_estimator = PiecewiseEstimator(estimators, mchange)","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case, for the neural Bayes estimator, the risk function becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"R(mathbftheta hatmathbftheta(cdot mathbfgamma))\nequiv\nsum_m in mathcalM\nP(M=m)left(int_mathcalS^m  L(mathbftheta hatmathbftheta(mathbfZ^(m) mathbfgamma))p(mathbfZ^(m) mid mathbftheta) d mathbfZ^(m)right)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a discrete uniform prior for M, the random variable corresponding to sample size.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters, m::R) where {R <: AbstractRange{I}} where I <: Integer\n\n\t# Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t# Generate K sample sizes from the prior distribution for M\n\tmÃÉ = rand(m, K)\n\n\t# Pseudocode for data simulation\n\tZ = [<simulate mÃÉ[k] iid realisations from the model> for k ‚àà 1:K]\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, setting the argument m in train to be an integer range will train the neural estimator with the given variable sample sizes.","category":"page"},{"location":"workflow/advancedusage/#Loading-previously-saved-neural-estimators","page":"Advanced usage","title":"Loading previously saved neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the parameters of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator at a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"train automatically saves the neural estimator's parameters; to load them, one may use the following code, or similar:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Œ∏ÃÇ = architecture()\nFlux.loadparams!(Œ∏ÃÇ, loadbestweights(path))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, architecture() is a user-defined function that returns a neural estimator with the same architecture as the estimator that we wish to load, but with randomly initialised parameters, and the function Flux.loadparams! loads the parameters of the best (as determined by loadbestweights) neural estimator saved in path.","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"To develop a neural estimator with NeuralEstimators.jl,","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior distribution: the parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of parameter configurations in the given parameter set (i.e., training, validation, or test set).\nSimulate data from the assumed model over the parameter sets sampled above. These data are stored as a Vector{A}, with each element of the vector associated with one parameter configuration, and where A depends on the representation of the neural estimator (e.g., an Array for CNN-based estimators, a GNNGraph for GNN-based estimators).\nInitialise a neural network, Œ∏ÃÇ, that will be trained into a neural Bayes estimator.  \nTrain Œ∏ÃÇ under the chosen loss function using train.\nAssess Œ∏ÃÇ using assess. The resulting object of class Assessment can be used to assess the estimator with respect to the entire parameter space by estimating the risk function with risk, or used to inspect the empirical joint distribution of the estimator with plotdistribution.\nApply Œ∏ÃÇ to observed data (once its performance has been checked in the above step). Bootstrap-based uncertainty quantification is facilitated with bootstrap.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"See the Examples and, once familiar with the basic workflow, see Advanced usage for practical considerations on how to construct neural estimators most effectively.","category":"page"},{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by having the user define their statistical model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code provide an example for how a user could formulate code for their own model. If you've developed similar functions that you think may be helpful to others, please get in touch or make a pull request.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussianprocess\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Model-specific functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L, œÉ, m)\nsimulategaussianprocess(L)\n\nSimulates m realisations from a Gau(0, ùö∫ + œÉ¬≤ùêà) distribution, where ùö∫ ‚â° LL'.\n\nIf œÉ and m are omitted, a single field without nugget variance is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L, m; C = 3.5, Gumbel = true)\nsimulateschlather(L;    C = 3.5, Gumbel = true)\n\nGiven the lower Cholesky factor L associated with a Gaussian process, simulates m realisations from Schlather's max-stable model using the algorithm for approximate simulation given by Schlather (2002). By default, the simulated data are log transformed from the unit Fr√©chet scale to the Gumbel scale.\n\nThe accuracy of the algorithm is controlled with a tuning parameter, C, which involves a trade-off between computational efficiency (favouring small C) and accuracy (favouring large R). Schlather (2002) recommends the use of C = 3; conservatively, we set the default to C = 3.5.\n\nSchlather, M. (2002). Models for stationary max-stable random fields. Extremes, 5:33‚Äì44.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Commonly-used-low-level-functions","page":"Model-specific functions","title":"Commonly-used low-level functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These low-level functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\nmaternchols\n\nincgamma","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nFor two points separated by h units, compute the Mat√©rn covariance function, with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance parameter œÉ¬≤.\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright) K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Model-specific functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, œÅ, ŒΩ; œÉ¬≤ = 1)\n\nGiven a distance matrix D, computes the covariance matrix under the Mat√©rn covariance function with range parameter œÅ and smoothness parameter ŒΩ, and returns the Cholesky factor of this covariance matrix.\n\nProviding vectors for œÅ and ŒΩ will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length). Similarly, for a vector of distance matrices D. If both D and the parameters are vectors, they must be of the same length.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.incgamma","page":"Model-specific functions","title":"NeuralEstimators.incgamma","text":"incgamma(a::T, x::T; upper::Bool, reg::Bool) where {T <: AbstractFloat}\n\nFor positive parameter a and positive integration limit x, computes the incomplete gamma function, as described by the Wikipedia article.\n\nKeyword arguments:\n\nupper::Bool: if true, the upper incomplete gamma function is returned; otherwise, the lower version is returned.\nreg::Bool: if true, the regularized incomplete gamma function is returned; otherwise, the unregularized version is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in the manuscript, we have developed the following density functions, and we include them in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::V, L; logdensity = true) where {V <: AbstractVector{T}} where T\ngaussiandensity(y::A, Œ£; logdensity = true) where {A <: AbstractArray{T, N}} where {T, N}\n\nEfficiently computes the density function for y ~ ùëÅ(0, Œ£), with L the lower Cholesky factor of the covariance matrix Œ£.\n\nThe method gaussiandensity(y::A, Œ£) assumes that the last dimension of y corresponds to the independent-replicates dimension, and it exploits the fact that we need to compute the Cholesky factor L for these independent replicates once only.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà; logdensity = true)\n\nThe bivariate density function for Schlather's max-stable model, as given in Huser (2013, pg. 231‚Äì232).\n\nHuser, R. (2013). Statistical Modeling and Inference for Spatio-Temporal Ex- tremes. PhD thesis, Swiss Federal Institute of Technology, Lausanne, Switzerland.\n\n\n\n\n\n","category":"function"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural estimators are neural networks that transform data into parameter estimates, and they are a promising recent approach to inference. They are likelihood free, substantially faster than classical methods, and can be designed to be approximate Bayes estimators.  Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is usually computationally intensive to sample from but which is essentially available \"for free\" with a neural estimator.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The package NeuralEstimators facilitates the development of neural estimators in a user-friendly manner. The package facilitates neural estimation for arbitrary statistical models, which is made possible by having the user implicitly define their model by providing simulated data (or by defining a function for data simulation). Since only simulated data is needed, it is particularly straightforward to develop neural estimators for models with existing implementations, possibly in other programming languages (e.g., R or python).","category":"page"},{"location":"#Getting-started","page":"NeuralEstimators","title":"Getting started","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Install NeuralEstimators from Julia's package manager using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the details of the Theoretical framework, see the Examples.","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use NeuralEstimators in your research or other activities, please use the following citation.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@misc{,\n  author = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Rapha√´l},\n  year = {2022},\n  title = {Fast Optimal Estimation with Intractable Models using Permutation-Invariant Neural Networks},\n  howpublished = {arXiv:2208.12942}\n}","category":"page"},{"location":"API/core/#Core-functions","page":"Core functions","title":"Core functions","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"This page documents the functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core functions","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Parameters sampled from the prior distribution Omega(cdot) are stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core functions","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with simulate.\n\nThe user-defined type must have a field Œ∏ that stores the p √ó K matrix of parameters, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.\n\nSee subsetparameters for the generic function for subsetting these objects.  \n\nExamples\n\nstruct P <: ParameterConfigurations\n\tŒ∏\n\t# ...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core functions","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"NeuralEstimators facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model (by overloading the generic function simulate).","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"The data should be stored as a Vector{A}, where each element of the vector is associated with one parameter configuration, and where A depends on the representation of the neural estimator. For example, if the neural estimator is a DeepSet object, the data should be stored as a Vector{Array}, where each array may store independent replicates in its final dimension. Similarly, if the neural estimator is a GNNEstimator, the data should be stored as a Vector{GNNGraph}, where each graph may store independent replicates in sub-graphs.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"simulate","category":"page"},{"location":"API/core/#NeuralEstimators.simulate","page":"Core functions","title":"NeuralEstimators.simulate","text":"Generic function that may be overloaded to implicitly define a statistical model. Specifically, the user should define provide a method simulate(parameters, m) that returns m simulated replicates for each element in the given set of parameters.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Neural-estimator-representations","page":"Core functions","title":"Neural-estimator representations","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Although the user is free to construct their neural estimator however they see fit, NeuralEstimators provides several useful representations described below.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSet\n\nPiecewiseEstimator\n\nGNNEstimator","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSet","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, a)\nDeepSet(œà, œï; a::String = \"mean\")\n\nA neural estimator in the DeepSet representation,\n\nŒ∏(ùêô)  a(œï(ùêô·µ¢)  i = 1  m)\n\nwhere ùêô ‚â° (ùêô‚ÇÅ', ‚Ä¶, ùêô‚Çò')' are independent replicates from the model, œà and œï are neural networks, and a is a permutation-invariant aggregation function.\n\nThe function a must aggregate over the last dimension (i.e., the replicates dimension) of an input array. It can be specified as a positional argument of type Function, or as a keyword argument of type String with permissible values \"mean\", \"sum\", and \"logsumexp\".\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Apply the estimator to a single set of m=3 realisations:\nZ = rand(n, 3);\nŒ∏ÃÇ(Z)\n\n# Apply the estimator to two sets each containing m=3 realisations:\nZ = [rand(n, m) for m ‚àà (3, 3)];\nŒ∏ÃÇ(Z)\n\n# Apply the estimator to two sets containing m=3 and m=4 realisations, respectively:\nZ = [rand(n, m) for m ‚àà (3, 4)];\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core functions","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator(estimators, mchange)\n\nCreates a piecewise estimator from a collection of estimators, based on the collection of sample-size changepoints, mchange, which should contain one element fewer than the number of estimators.\n\nExamples\n\n# Suppose that we've trained two neural estimators. The first, Œ∏ÃÇ‚ÇÅ, is trained\n# for small sample sizes (e.g., m ‚â§ 30), and the second, `Œ∏ÃÇ‚ÇÇ`, is trained for\n# moderate-to-large sample sizes (e.g., m > 30). Then we construct a piecewise\n# estimator with a sample-size changepoint of 30, which dispatches Œ∏ÃÇ‚ÇÅ if m ‚â§ 30\n# and Œ∏ÃÇ‚ÇÇ if m > 30.\n\nn = 2\np = 3\nw = 8\n\nœà‚ÇÅ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÅ = DeepSet(œà‚ÇÅ, œï‚ÇÅ)\n\nœà‚ÇÇ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu));\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÇ = DeepSet(œà‚ÇÇ, œï‚ÇÇ)\n\nŒ∏ÃÇ = PiecewiseEstimator([Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ], [30])\nZ = [rand(n, 1, m) for m ‚àà (10, 50)]\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.GNNEstimator","page":"Core functions","title":"NeuralEstimators.GNNEstimator","text":"GNNEstimator(propagation, globalpool, deepset)\n\nA neural estimator based on a graph neural network (GNN). The propagation module transforms graphical input data into a set of hidden feature graphs; the globalpool module aggregates the feature graphs (graph-wise) into a single hidden feature vector; and the deepset module maps the hidden feature vectors onto the parameter space.\n\nThe data should be a GNNGraph or AbstractVector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing Flux: batch\nusing GraphNeuralNetworks\nusing Statistics: mean\n\n# Create some graphs\nd = 1             # dimension of the response variable\nn‚ÇÅ, n‚ÇÇ = 11, 27   # number of nodes\ne‚ÇÅ, e‚ÇÇ = 30, 50   # number of edges\ng‚ÇÅ = rand_graph(n‚ÇÅ, e‚ÇÅ, ndata=rand(d, n‚ÇÅ))\ng‚ÇÇ = rand_graph(n‚ÇÇ, e‚ÇÇ, ndata=rand(d, n‚ÇÇ))\ng  = batch([g‚ÇÅ, g‚ÇÇ])\n\n# propagation module\nw = 5; o = 7\npropagation = GNNChain(GraphConv(d => w), GraphConv(w => w), GraphConv(w => o))\n\n# global pooling module\nmeanpool = GlobalPool(mean)\n\n# Deep Set module\nw = 32\np = 3\nœà‚ÇÇ = Chain(Dense(o, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, p))\ndeepset = DeepSet(œà‚ÇÇ, œï‚ÇÇ)\n\n# GNN estimator\nest = GNNEstimator(propagation, meanpool, deepset)\n\n# Apply the estimator to a single graph, a single graph containing sub-graphs,\n# and a vector of graphs:\nŒ∏ÃÇ = est(g‚ÇÅ)\nŒ∏ÃÇ = est(g)\nŒ∏ÃÇ = est([g‚ÇÅ, g‚ÇÇ, g])\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core functions","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"train\n\ntrain(Œ∏ÃÇ, P)\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T, M::Vector{I}) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}, I <: Integer}","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core functions","title":"NeuralEstimators.train","text":"Generic function for training a neural estimator.\n\nThe methods are designed to cater for different forms of \"on-the-fly simulation\" (see the online documentation). In all methods, the validation data are held fixed to reduce noise when evaluating the validation risk function (which is used to monitor the performance of the estimator during training).\n\nKeyword arguments\n\nArguments common to all methods:\n\nloss = mae: the loss function, which should return the average loss when applied to multiple replicates.\nepochs::Integer = 100\nbatchsize::Integer = 32\noptimiser = ADAM(1e-4)\nsavepath::String = \"\": path to save the trained Œ∏ÃÇ and other information; if savepath is an empty string (default), nothing is saved.\nstopping_epochs::Integer = 5: cease training if the risk doesn't improve in stopping_epochs epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nArguments common to train(Œ∏ÃÇ, P) and train(Œ∏ÃÇ, Œ∏_train, Œ∏_val):\n\nm: sample sizes (either an Integer or a collection of Integers).\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nsimulate_just_in_time::Bool = false: should we simulate the data \"just-in-time\"?\n\nArguments unique to train(Œ∏ÃÇ, P):\n\nK::Integer = 10_000: number of parameter vectors in the training set; the size of the validation set is K √∑ 5.\nŒæ = nothing: invariant model information; if Œæ is provided, the constructor P is called as P(K, Œæ).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.train-Tuple{Any, Any}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, P; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing a constructor, P, where P is a subtype of AbstractMatrix or ParameterConfigurations, to automatically sample the sets of training and validation parameters.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P<:Union{ParameterConfigurations, AbstractMatrix}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing the training and validation parameter sets explicitly as Œ∏_train and Œ∏_val, both of which are held fixed during training.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P<:Union{ParameterConfigurations, AbstractMatrix}}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; <keyword args>)\n\nTrain the neural estimator Œ∏ÃÇ by providing the training and validation parameter sets, Œ∏_train and Œ∏_val, and the training and validation data sets, Z_train and Z_val, all of which are held fixed during training.\n\nIf the elements of Z_train and Z_val are equally replicated, and the number of replicates for each element of Z_train is a multiple of the number of replicates for each element of Z_val, then the training data will then be recycled throughout training to imitate on-the-fly simulation. Note that this requires the data to be subsetted throughout training with the function subsetdata.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P<:Union{ParameterConfigurations, AbstractMatrix}, I<:Integer}","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T, M; <keyword args>)\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::V, Z_val::V; <keyword args>) where {V <: Vector{Vector{T}}}\n\nTrain several neural estimators with the architecture Œ∏ÃÇ under different sample sizes.\n\nThe first method accepts sample sizes as a vector of integers, M. Each estimator is pre-trained with the estimator trained for the previous sample size. For example, if M = [m‚ÇÅ, m‚ÇÇ], with m‚ÇÇ > m‚ÇÅ, the estimator for sample size m‚ÇÇ is pre-trained with the estimator for sample size m‚ÇÅ.\n\nThe second method requires Z_train and Z_val to be a Vector{Vector{T}}, where T is arbitrary. In this method, a separate estimator is trained for each element in Z_train and Z_val and, importantly, it does not invoke subsetdata(), which can be slow for graphical data. Again, pre-training is used.\n\nThese methods wrap train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val) and, hence, they inherit its keyword arguments. Further, certain keyword arguments can be given as vectors. For instance, if we are training two neural estimators, we can use a different number of epochs by providing epochs = [e‚ÇÅ, e‚ÇÇ]. Other arguments that allow vectors are batchsize, stopping_epochs, and optimiser.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Assessing-a-neural-estimator","page":"Core functions","title":"Assessing a neural estimator","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"assess\n\nAssessment\n\nrisk\n\ncoverage\n\nplotrisk\n\nplotdistribution","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core functions","title":"NeuralEstimators.assess","text":"assess(estimators, parameters, Z; <keyword args>)\nassess(estimators, parameters; <keyword args>)\n\nUsing a collection of estimators, compute estimates from data simulated from a set of parameters.\n\nTesting data can be automatically simulated by overloading simulate with a method simulate(parameters, m::Integer), and using the keyword argument m to specify the desired sample sizes to assess. Alternatively, one may provide testing data Z as an iterable collection, where each element contains the testing data for a given sample size. If there are more simulated data sets than unique parameter vectors, the data should be stored in an 'outer' fashion, so that the parameter vectors run faster than the replicated data.\n\nKeyword arguments\n\nArguments common to both methods\n\nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided).\nŒæ = nothing: invariant model information.\nuse_Œæ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses the invariant model information, Œæ: If it does, the estimator will be applied as estimator(Z, Œæ).\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\nArguments unique to assess(estimators, parameters)\n\nm::Vector{Integer}: sample sizes to estimate from.\nJ::Integer = 1: the number of times to replicate each parameter in parameters.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Generate fake parameters and corresponding data for a range of sample sizes:\nK = 100        # number of parameter vectors in the test set\nŒ∏ = rand(p, K)\nZ = [[rand(n, m) for _ ‚àà 1:K] for m ‚àà (1, 10, 20)]\n\nassessment = assess([Œ∏ÃÇ], Œ∏, Z)\nrisk(assessment)\nrisk(assessment, average_over_parameters = false)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core functions","title":"NeuralEstimators.Assessment","text":"Assessment(Œ∏andŒ∏ÃÇ::DataFrame, runtime::DataFrame)\n\nAn type for storing the output of assess(). It contains two fields. The field runtime contains the total time taken for each estimator for each sample size m. The field Œ∏andŒ∏ÃÇ is a long-form DataFrame containing the true parameters and corresponding estimates. Specifically, its columns are:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size\nk:         the index of the parameter vector in the test set\nj: the index of the data set\n\nMultiple Assessment objects can be combined with the function merge.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.risk","page":"Core functions","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; loss = (x, y) -> abs(x - y), average_over_parameters = true)\n\nEstimates the Bayes risk with respect to the loss function for each estimator, parameter, and sample size considered in assessment.\n\nThe argument loss should be a binary operator (default absolute-error loss).\n\nIf average_over_parameters = true (default), the risk is averaged over all parameters; otherwise, the risk is evaluated over each parameter separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.coverage","page":"Core functions","title":"NeuralEstimators.coverage","text":"coverage(Œ∏ÃÇ, Z::V, Œ∏, Œ±; kwargs...) where  {V <: AbstractArray{A}} where A\n\nFor each data set contained in Z, compute a non-parametric bootstrap confidence interval with nominal coverage Œ±, and determine if the true parameters, Œ∏, are contained within this interval. The overall empirical coverage is then obtained by averaging the resulting 0-1 matrix over all data sets.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.plotrisk","page":"Core functions","title":"NeuralEstimators.plotrisk","text":"TODO\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.plotdistribution","page":"Core functions","title":"NeuralEstimators.plotdistribution","text":"TODO\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Bootstrapping","page":"Core functions","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"bootstrap\n\nconfidenceinterval","category":"page"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core functions","title":"NeuralEstimators.bootstrap","text":"bootstrap(Œ∏ÃÇ, parameters::P, ZÃÉ) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, parameters::P, m::Integer; B = 400) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, Z; B = 400)\nbootstrap(Œ∏ÃÇ, Z, blocks::Vector{Integer}; B = 400)\n\nGenerates B bootstrap estimates from an estimator Œ∏ÃÇ.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, ZÃÉ, whose length implicitly defines B. Alternatively, if the user has defined a method simulate(parameters, m), one may simply pass the desired sample size m for the simulated data sets.\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be an integer vector specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm aims to produce resampled data sets that are of a similar size to Z, but this can only be achieved exactly if all blocks are equal in length.\n\nThe keyword argument use_gpu is a flag determining whether to use the GPU, if it is available (default true).\n\nThe return type is a p √ó B matrix, where p is the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.confidenceinterval","page":"Core functions","title":"NeuralEstimators.confidenceinterval","text":"confidenceinterval(Œ∏ÃÉ; probs = [0.05, 0.95])\n\nCompute a confidence interval using the quantiles of the p √ó B matrix of bootstrap samples, Œ∏ÃÉ, where p is the number of parameters in the model. The quantile levels are controlled with the argument probs.\n\nThe return type is a p √ó 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the confidence interval.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/#Univariate-Gaussian-data","page":"Examples","title":"Univariate Gaussian data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we develop a neural Bayes estimator for mathbftheta equiv (mu sigma) from data Z_1 dots Z_m that are independent and identically distributed according to a N(mu sigma^2) distribution.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Before proceeding, we load the required packages:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux\nusing Distributions\nimport NeuralEstimators: simulate","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we sample parameters from the prior Omega(cdot) to construct parameter sets used for training, validating, and testing the estimator. Here, we use the priors mu sim N(0 1) and sigma sim U(01 1), and we assume that the parameters are independent a priori. The sampled parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of sampled parameter vectors.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(Œ©, K)\n\tŒº = rand(Œ©.Œº, K)\n\tœÉ = rand(Œ©.œÉ, K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\treturn Œ∏\nend\n\nŒ© = (Œº = Normal(0, 1), œÉ = Uniform(0.1, 1))\nŒ∏_train = sample(Œ©, 10000)\nŒ∏_val   = sample(Œ©, 2000)\nŒ∏_test  = sample(Œ©, 1000)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model via simulated data. In general, the data are stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and where A depends on the representation of the neural estimator. Since our data is replicated, we will use the Deep Sets framework and, since each replicate is univariate, we will use a dense neural network (DNN) for the inner network. Since the inner network is a DNN, the data should be stored as an Array, with independent replicates stored in the final dimension.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(Œ∏_set, m)\n\tZ = [rand(Normal(Œ∏[1], Œ∏[2]), n, m) for Œ∏ ‚àà eachcol(Œ∏_set)]\n\tZ = broadcast.(Float32, Z) # convert to Float32 for computational efficiency\n\treturn Z\nend\n\nn = 1  # dimension of each replicate (univariate data)\nm = 15 # number of independent replicates for each parameter vector\n\nZ_train = simulate(Œ∏_train, m)\nZ_val   = simulate(Œ∏_val, m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design architectures for the inner and outer neural networks, mathbfpsi(cdot) and mathbfphi(cdot) respectively, in the Deep Sets framework, and initialise the neural estimator as a DeepSet object.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"p = length(Œ©)   # number of parameters in the statistical model\nw = 32          # number of neurons in each layer\n\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p))\nŒ∏ÃÇ = DeepSet(œà, œï)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 30)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To test the accuracy of the resulting neural Bayes estimator, we use the function assess, which can be used to assess the performance of the estimator (or multiple estimators) over a range of sample sizes. Note that, in this example, we trained the neural estimator using a single sample size, m = 15, and hence the estimator will not necessarily be optimal for other sample sizes; see Variable sample sizes for approaches that one could adopt if data sets with varying sample size are envisaged.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z_test     = [simulate(Œ∏_test, m) for m ‚àà (5, 10, 15, 20, 30)]\nassessment = assess([Œ∏ÃÇ], Œ∏_test, Z_test)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The returned object is an object of type Assessment, which contains the true parameters and their corresponding estimates, and the time taken to compute the estimates for each sample size and each estimator. The risk function may computed using risk, and plotted against the sample size with plotrisk:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"risk(assessment)\nplotrisk(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"It is often helpful to visualise the empirical joint distribution of an estimator for a particular parameter configuration and a particular sample size. This can be done by providing assess with J data sets simulated under a particular parameter configuration, and then calling plotdistribution:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"J = 100\nŒ∏ = sample(Œ©, 1)\nZ = [simulate(Œ∏, m, J)]\nassessment = assess([Œ∏ÃÇ], Œ∏, Z)  \nplotdistribution(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the neural Bayes estimator has passed our assessment, it may then be applied to observed data, with parametric/non-parametric bootstrap-based uncertainty quantification facilitated by bootstrap and confidenceinterval. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z = simulate(Œ∏, m)     # pretend that this is observed data\nŒ∏ÃÇ(Z)                   # point estimates from the observed data\nŒ∏ÃÉ = bootstrap(Œ∏ÃÇ, Z)    # non-parametric bootstrap estimates\nconfidenceinterval(Œ∏ÃÉ)  # confidence interval from the bootstrap estimates","category":"page"},{"location":"workflow/examples/#Time-series-(AR1)","page":"Examples","title":"Time series (AR1)","text":"","category":"section"},{"location":"workflow/examples/#Irregular-spatial-data","page":"Examples","title":"Irregular spatial data","text":"","category":"section"}]
}
