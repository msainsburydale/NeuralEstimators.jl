var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by having the user define their statistical model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code illustrates how a user could formulate code for their own model.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"See also Distributions.jl for a large range of distributions implemented in Julia, and the package RCall for calling R functions within Julia. ","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussianprocess\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Model-specific functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::Matrix, m = 1)\nsimulategaussianprocess(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from a mean-zero Gaussian process.\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or an object from the package GaussianRandomFields.\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nExamples\n\nusing NeuralEstimators\nusing Distances\nusing LinearAlgebra\n\nn = 500\nρ = 0.6\nν = 1.0\nS = rand(n, 2)\n\n# Passing a Cholesky factor:\nD = pairwise(Euclidean(), S, S, dims = 1)\nΣ = Symmetric(matern.(D, ρ, ν))\nL = cholesky(Σ).L\nsimulategaussianprocess(L)\n\n# Passing a GaussianRandomField with Cholesky decomposition:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(ρ, ν))\ngrf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), S)\nsimulategaussianprocess(grf)\n\n# Passing a GaussianRandomField with circulant embedding (fast but requires regular grid):\npts = range(0.0, 1.0, 20)\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulategaussianprocess(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::Matrix, m = 1)\nsimulateschlather(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from Schlather's max-stable model using the algorithm for approximate simulation given by Schlather (2002).\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or an object from the package GaussianRandomFields.\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nKeyword arguments\n\nC = 3.5: a tuning parameter that controls the accuracy of the algorithm: small C favours computational efficiency, while large C favours accuracy. Schlather (2002) recommends the use of C = 3.\nGumbel = true: flag indicating whether the data should be log-transformed from the unit Fréchet scale to the Gumbel scale.\n\nExamples\n\nusing NeuralEstimators\nusing Distances\nusing LinearAlgebra\n\nn = 500\nρ = 0.6\nν = 1.0\nS = rand(n, 2)\n\n# Passing a Cholesky factor:\nD = pairwise(Euclidean(), S, S, dims = 1)\nΣ = Symmetric(matern.(D, ρ, ν))\nL = cholesky(Σ).L\nsimulateschlather(L)\n\n# Passing a GaussianRandomField with Cholesky decomposition:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(ρ, ν))\ngrf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), S)\nsimulateschlather(grf)\n\n# Passing a GaussianRandomField with circulant embedding (fast but requires regular grid):\npts = range(0.0, 1.0, 20)\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulateschlather(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Spatial-point-processes","page":"Model-specific functions","title":"Spatial point processes","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"maternclusterprocess","category":"page"},{"location":"API/simulation/#NeuralEstimators.maternclusterprocess","page":"Model-specific functions","title":"NeuralEstimators.maternclusterprocess","text":"maternclusterprocess(; λ=10, μ=10, r=0.1, xmin=0, xmax=1, ymin=0, ymax=1, unit_bounding_box=false)\n\nSimulates a Matérn cluster process with density of parent Poisson point process λ, mean number of daughter points μ, and radius of cluster disk r, over the simulation window defined by xmin and xmax, ymin and ymax.\n\nIf unit_bounding_box is true, then the simulated points will be scaled so that the longest side of their bounding box is equal to one (this may change the simulation window). \n\nSee also the R package spatstat, which provides functions for simulating from a range of point processes and which can be interfaced from Julia using RCall.\n\nExamples\n\nusing NeuralEstimators\n\n# Simulate a realisation from a Matérn cluster process\nS = maternclusterprocess()\n\n# Visualise realisation (requires UnicodePlots)\nusing UnicodePlots\nscatterplot(S[:, 1], S[:, 2])\n\n# Visualise realisations from the cluster process with varying parameters\nn = 250\nλ = [10, 25, 50, 90]\nμ = n ./ λ\nplots = map(eachindex(λ)) do i\n\tS = maternclusterprocess(λ = λ[i], μ = μ[i])\n\tscatterplot(S[:, 1], S[:, 2])\nend\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Covariance-functions","page":"Model-specific functions","title":"Covariance functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These covariance functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\nmaternchols","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, ρ, ν, σ² = 1)\n\nFor two points separated by h units, compute the Matérn covariance function, with range parameter ρ, smoothness parameter ν, and marginal variance parameter σ².\n\nWe use the parametrisation C(boldsymbolh) = sigma^2 frac2^1 - nuGamma(nu) left(fracboldsymbolhrhoright)^nu K_nu left(fracboldsymbolhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Model-specific functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, ρ, ν, σ² = 1; stack = true)\n\nGiven a distance matrix D, constructs the Cholesky factor of the covariance matrix under the Matérn covariance function with range parameter ρ, smoothness parameter ν, and marginal variance σ².\n\nProviding vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices D may also be provided.\n\nIf stack = true, the Cholesky factors will be \"stacked\" into a three-dimensional array (this is only possible if all distance matrices in D are the same size).\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra: norm\nn  = 10\nS  = rand(n, 2)\nD  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S), sⱼ ∈ eachrow(S)]\nρ  = [0.6, 0.5]\nν  = [0.7, 1.2]\nσ² = [0.2, 0.4]\nmaternchols(D, ρ, ν)\nmaternchols([D], ρ, ν)\nmaternchols(D, ρ, ν, σ²; stack = false)\n\nS̃  = rand(n, 2)\nD̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]\nmaternchols([D, D̃], ρ, ν, σ²)\nmaternchols([D, D̃], ρ, ν, σ²; stack = false)\n\nS̃  = rand(2n, 2)\nD̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]\nmaternchols([D, D̃], ρ, ν, σ²; stack = false)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in various paper, we have developed the following functions for evaluating the density function for several popular distributions. We include these in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::V, L::LT) where {V <: AbstractVector, LT <: LowerTriangular}\ngaussiandensity(y::A, L::LT) where {A <: AbstractArray, LT <: LowerTriangular}\ngaussiandensity(y::A, Σ::M) where {A <: AbstractArray, M <: AbstractMatrix}\n\nEfficiently computes the density function for y ~ 𝑁(0, Σ) for covariance matrix Σ, and where L is lower Cholesky factor of Σ.\n\nThe method gaussiandensity(y::A, L::LT) assumes that the last dimension of y contains independent and identically distributed (iid) replicates.\n\nThe log-density is returned if the keyword argument logdensity is true (default).\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z₁, z₂, ψ; logdensity = true)\n\nThe bivariate density function for Schlather's max-stable model.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages are used throughout these examples:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators, Flux, GraphNeuralNetworks, Distances, Distributions, Folds, LinearAlgebra, Statistics","category":"page"},{"location":"workflow/examples/#Univariate-data","page":"Examples","title":"Univariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here we develop a neural Bayes estimator for boldsymboltheta equiv (mu sigma) from data Z_1 dots Z_m that are independent and identically distributed realisations from the distribution N(mu sigma^2). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we define a function to sample parameters from the prior distribution. Here, we assume that the parameters are independent a priori and we adopt the marginal priors mu sim N(0 1) and sigma sim IG(3 1). The sampled parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of sampled parameter vectors:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tμ = rand(Normal(0, 1), 1, K)\n\tσ = rand(InverseGamma(3, 1), 1, K)\n\tθ = vcat(μ, σ)\n\treturn θ\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model through data simulation. In this package, the data are always stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and where the type A depends on the multivariate structure of the data. Since in this example each replicate Z_1 dots Z_m is univariate, A should be a Matrix with d=1 row and m columns. Below, we define our simulator given a single parameter vector, and given a matrix of parameter vectors (which simply applies the simulator to each column):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"simulate(θ, m) = θ[1] .+ θ[2] .* randn(1, m)\nsimulate(θ::AbstractMatrix, m) = simulate.(eachcol(θ), m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design our neural-network architecture. The workhorse of the package is the DeepSet architecture, which provides an elegant framework for making inference with an arbitrary number of independent replicates and for incorporating both neural and user-defined statistics. The DeepSets framework consists of two neural networks, a summary network and an inference network. The inference network (also known as the outer network) is always a multilayer perceptron (MLP). However, the architecture of the summary network (also known as the inner network) depends on the multivariate structure of the data. With unstructured data (i.e., when there is no spatial or temporal correlation within a replicate), we use an MLP with input dimension equal to the dimension of each replicate of the statistical model (i.e., one for univariate data): ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"p = 2                                                # number of parameters \nψ = Chain(Dense(1, 32, relu), Dense(32, 32, relu))   # summary network\nϕ = Chain(Dense(32, 32, relu), Dense(32, p))         # inference network\narchitecture = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In this example, we wish to construct a point estimator for the unknown parameter vector, and we therefore initialise a PointEstimator object based on our chosen architecture (see Estimators for a list of other estimators available in the package): ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ̂ = PointEstimator(architecture)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator using train(), here using the default absolute-error loss. We'll train the estimator using 50 independent replicates per parameter configuration. Below, we pass our user-defined functions for sampling parameters and simulating data, but one may also pass parameter or data instances, which will be held fixed during training:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 50\nθ̂ = train(θ̂, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To fully exploit the amortised nature of neural estimators, one may wish to save a trained estimator and load it in later sessions: see Saving and loading neural estimators for details on how this can be done. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The function assess() can be used to assess the trained estimator. Parametric and non-parametric bootstrap-based uncertainty quantification are facilitated by bootstrap() and interval(), and this can also be included in the assessment stage through the keyword argument boot:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test, m)\nassessment = assess(θ̂, θ_test, Z_test, boot = true)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting Assessment object contains the sampled parameters, the corresponding point estimates, and the corresponding lower and upper bounds of the bootstrap intervals. This object can be used to compute various diagnostics:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"bias(assessment)      # μ = 0.002, σ = 0.017\nrmse(assessment)      # μ = 0.086, σ = 0.078\nrisk(assessment)      # μ = 0.055, σ = 0.056\nplot(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Univariate Gaussian example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As an alternative form of uncertainty quantification, one may approximate a set of marginal posterior quantiles by training a second estimator under the quantile loss function, which allows one to generate approximate marginal posterior credible intervals. This is facilitated with IntervalEstimator which, by default, targets 95% central credible intervals:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"q̂ = IntervalEstimator(architecture)\nq̂ = train(q̂, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting posterior credible-interval estimator can also be assessed with empirical simulation-based methods using assess(), as we did above for the point estimator. Often, these intervals have better coverage than bootstrap-based intervals.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once an estimator is deemed to be satisfactorily calibrated, it may be applied to observed data (below, we use simulated data as a substitute for observed data):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ = sample(1)               # true parameters\nZ = simulate(θ, m)          # \"observed\" data\nθ̂(Z)                        # point estimates\ninterval(bootstrap(θ̂, Z))   # 95% non-parametric bootstrap intervals\ninterval(q̂, Z)              # 95% marginal posterior credible intervals","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To utilise a GPU for improved computational efficiency, one may simply move the estimator and the data to the GPU through the calls θ̂ = gpu(θ̂) and Z = gpu(Z) before applying the estimator. Note that GPUs often have limited memory relative to CPUs, and this can sometimes lead to memory issues when working with very large data sets: in these cases, the function estimateinbatches() can be used to apply the estimator over batches of data to circumvent any memory concerns. ","category":"page"},{"location":"workflow/examples/#Unstructured-multivariate-data","page":"Examples","title":"Unstructured multivariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Suppose now that each data set now consists of m replicates boldsymbolZ_1 dots boldsymbolZ_m of a d-dimensional multivariate distribution. Everything remains as given in the univariate example above, except that we now store each data set as a d times m matrix (previously they were stored as 1times m matrices), and the summary network of the DeepSets representation takes a d-dimensional input (previously it took a 1-dimensional input).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that, when estimating a full covariance matrix, one may wish to constrain the neural estimator to only produce parameters that imply a valid (i.e., positive definite) covariance matrix. This can be achieved by appending a  CovarianceMatrix layer to the end of the outer network of the DeepSets representation. However, the estimator will often learn to provide valid estimates, even if not constrained to do so.","category":"page"},{"location":"workflow/examples/#Gridded-data","page":"Examples","title":"Gridded data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For data collected over a regular grid, neural estimators are typically based on a convolutional neural network (CNN; see, for example, Dumoulin and Visin, 2016). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In these settings, each data set must be stored as an (N + 2)-dimensional array, where N is the dimension of the grid (e.g., N = 1 for time series, N = 2 for images, etc.). The penultimate dimension of the array stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes, etc.), while the final dimension stores independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10times15 grid, one would construct an array of dimension 10times15times2times50.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, here we develop a neural Bayes estimator for the spatial Gaussian process model with exponential covariance function and unknown range parameter theta. The spatial domain is taken to be the unit square, and we adopt the prior theta sim U(005 05). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Simulation from Gaussian processes typically involves the computation of an expensive intermediate object, namely, the Cholesky factor of a covariance matrix. Storing intermediate objects can enable the fast simulation of new data sets when the parameters are held fixed. Hence, in this example, we define a custom type Parameters subtyping ParameterConfigurations for storing the matrix of parameters and the corresponding Cholesky factors: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters{T} <: ParameterConfigurations\n\tθ::Matrix{T}\n\tL\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Further, we define two constructors for our custom type: one that accepts an integer K, and another that accepts a ptimes K matrix of parameters. The former constructor will be useful during the training stage for sampling from the prior distribution, while the latter constructor will be useful for parametric bootstrap (since this involves repeated simulation from the fitted model):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\n\t# Sample parameters from the prior \n\tθ = rand(Uniform(0.05, 0.5), 1, K)\n\n\t# Pass to matrix constructor\n\tParameters(θ)\nend\n\nfunction Parameters(θ::Matrix)\n\n\t# Spatial locations, a 16x16 grid over the unit square\n\tpts = range(0, 1, length = 16)\n\tS = expandgrid(pts, pts)\n\n\t# Distance matrix, covariance matrices, and Cholesky factors\n\tD = pairwise(Euclidean(), S, dims = 1)\n\tK = size(θ, 2)\n\tL = map(1:K) do k\n\t\tΣ = exp.(-D ./ θ[k])\n\t\tcholesky(Symmetric(Σ)).L\n\tend\n\n\tParameters(θ, L)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define the model simulator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(L, m = 1)\n\n\t# Spatial field\n\tn = size(L, 1)\n\tZ = L * randn(n, m)\n\n\t# Reshape to 16x16 images\n\tZ = reshape(Z, 16, 16, 1, m)\n\n\treturn Z\nend\nsimulate(parameters::Parameters, m = 1) = [simulate(L, m) for L ∈ parameters.L]","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"A possible architecture is as follows:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Summary network\nψ = Chain(\n\tConv((3, 3), 1 => 32, relu),\n\tMaxPool((2, 2)),\n\tConv((3, 3),  32 => 64, relu),\n\tMaxPool((2, 2)),\n\tFlux.flatten\n\t)\n\n# Inference network\nϕ = Chain(Dense(256, 64, relu), Dense(64, 1))\n\n# DeepSet\narchitecture = DeepSet(ψ, ϕ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we initialise a point estimator and a posterior credible-interval estimator:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ̂ = PointEstimator(architecture)\nq̂ = IntervalEstimator(architecture)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Now we train the estimators, here using fixed parameter instances to avoid repeated Cholesky factorisations (see Storing expensive intermediate objects for data simulation and On-the-fly and just-in-time simulation for further discussion):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"K = 10000  # number of training parameter vectors\nm = 1      # number of independent replicates in each data set\nθ_train = sample(K)\nθ_val = sample(K ÷ 10)\nθ̂ = train(θ̂, θ_train, θ_val, simulate, m = m)\nq̂ = train(q̂, θ_train, θ_val, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the estimators have been trained, we assess them using empirical simulation-based methods:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test)\nassessment = assess([θ̂, q̂], θ_test, Z_test)\n\nbias(assessment)       # 0.005\nrmse(assessment)       # 0.032\ncoverage(assessment)   # 0.953\nplot(assessment)       ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Gridded spatial Gaussian process example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, we can apply our estimators to observed data. Note that when we have a single replicate only (which is often the case in spatial statistics), non-parametric bootstrap is not possible, and we instead use parametric bootstrap:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ = sample(1)                          # true parameter\nZ = simulate(θ)                        # \"observed\" data\nθ̂(Z)                                   # point estimates\ninterval(q̂, Z)                         # 95% marginal posterior credible intervals\nbs = bootstrap(θ̂, θ̂(Z), simulate, m)   # parametric bootstrap intervals\ninterval(bs)                           # 95% parametric bootstrap intervals","category":"page"},{"location":"workflow/examples/#Irregular-spatial-data","page":"Examples","title":"Irregular spatial data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To cater for spatial data collected over arbitrary spatial locations, one may construct a neural estimator with a graph neural network (GNN) architecture (see Sainsbury-Dale, Zammit-Mangion, Richards, and Huser, 2023). The overall workflow remains as given in previous examples, with some key additional steps:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Sampling spatial configurations during the training phase, typically using an appropriately chosen spatial point process: see, for example, maternclusterprocess.\nStoring the spatial data as a graph: see spatialgraph.\nConstructing an appropriate architecture: see GNNSummary and SpatialGraphConv.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, we again consider the spatial Gaussian process model with exponential covariance function, and we define a struct for storing expensive intermediate objects needed for data simulation. In this case, these objects include Cholesky factors and spatial graphs (which store the adjacency matrices needed to perform graph convolution): ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters{T} <: ParameterConfigurations\n\tθ::Matrix{T}   # true parameters  \n\tL              # Cholesky factors\n\tg              # spatial graphs\n\tS              # spatial locations \nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Again, we define two constructors, which will be convenient for sampling parameters from the prior during training and assessment, and for performing parametric bootstrap sampling when making inferences from observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\n\t# Sample parameters from the prior \n\tθ = rand(Uniform(0.05, 0.5), 1, K)\n\n\t# Simulate spatial configurations over the unit square\n\tn = rand(200:300, K)\n\tλ = rand(Uniform(10, 50), K)\n\tS = [maternclusterprocess(λ = λ[k], μ = n[k]/λ[k]) for k ∈ 1:K]\n\n\t# Pass to constructor\n\tParameters(θ, S)\nend\n\nfunction Parameters(θ::Matrix, S)\n\n\t# Number of parameter vectors\n\tK = size(θ, 2)\n\n\t# Distance matrices, covariance matrices, and Cholesky factors\n\tD = pairwise.(Ref(Euclidean()), S, dims = 1)\n\tL = Folds.map(1:K) do k\n\t\tΣ = exp.(-D[k] ./ θ[k])\n\t\tcholesky(Symmetric(Σ)).L\n\tend\n\n\t# Construct spatial graphs\n\tg = spatialgraph.(S)\n\n\tParameters(θ, L, g, S)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define a function for simulating from the model given an object of type Parameters. The code below enables simulation of an arbitrary number of independent replicates m, and one may provide a single integer for m, or any object that can be sampled using rand(m, K) (e.g., an integer range or some distribution over the possible sample sizes):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m)\n\tK = size(parameters, 2)\n\tm = rand(m, K)\n\tmap(1:K) do k\n\t\tL = parameters.L[k]\n\t\tg = parameters.g[k]\n\t\tn = size(L, 1)\n\t\tZ = L * randn(n, m[k])      \n\t\tspatialgraph(g, Z)            \n\tend\nend\nsimulate(parameters::Parameters, m::Integer = 1) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next we construct an appropriate GNN architecture, as illustrated below. Here, our goal is to construct a point estimator, however any other kind of estimator (see Estimators) can be constructed by simply substituting the appropriate estimator class in the final line below:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Spatial weight function constructed using 0-1 basis functions \nh_max = 0.15 # maximum distance to consider \nq = 10       # output dimension of the spatial weights\nw = IndicatorWeights(h_max, q)\n\n# Propagation module\npropagation = GNNChain(\n\tSpatialGraphConv(1 => q, relu, w = w, w_out = q),\n\tSpatialGraphConv(q => q, relu, w = w, w_out = q)\n)\n\n# Readout module\nreadout = GlobalPool(mean)\n\n# Global features \nglobalfeatures = SpatialGraphConv(1 => q, relu, w = w, w_out = q, glob = true)\n\n# Summary network\nψ = GNNSummary(propagation, readout, globalfeatures)\n\n# Mapping module\nϕ = Chain(\n\tDense(2q => 128, relu), \n\tDense(128 => 128, relu), \n\tDense(128 => 1, identity)\n)\n\n# DeepSet object\ndeepset = DeepSet(ψ, ϕ)\n\n# Point estimator\nθ̂ = PointEstimator(deepset)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 1\nK = 3000\nθ_train = sample(K)\nθ_val   = sample(K÷5)\nθ̂ = train(θ̂, θ_train, θ_val, simulate, m = m, epochs = 5)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Then, we assess our trained estimator as before: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"θ_test = sample(1000)\nZ_test = simulate(θ_test, m)\nassessment = assess(θ̂, θ_test, Z_test)\nbias(assessment)    # 0.001\nrmse(assessment)    # 0.037\nrisk(assessment)    # 0.029\nplot(assessment)   ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Estimates from a graph neural network (GNN) based neural Bayes estimator)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, once the estimator has been assessed and is deemed to be performant, it may be applied to observed data, with bootstrap-based uncertainty quantification facilitated by bootstrap and interval. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"parameters = sample(1)       # sample a single parameter vector\nZ = simulate(parameters)     # simulate data                  \nθ = parameters.θ             # true parameters used to generate data\nS = parameters.S             # observed locations\nθ̂(Z)                         # point estimates\nbs = bootstrap(θ̂, Parameters(θ̂(Z), S), simulate, m)   \ninterval(bs)                 # parametric bootstrap confidence interval              ","category":"page"},{"location":"framework/#Framework","page":"Framework","title":"Framework","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"In this section, we provide an overview of point estimation using neural Bayes estimators. For a more detailed discussion on the framework and its implementation, see the paper Likelihood-Free Parameter Estimation with Neural Bayes Estimators. For an accessible introduction to amortised neural inferential methods more broadly, see the review paper Neural Methods for Amortised Parameter Inference. ","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"A parametric statistical model is a set of probability distributions on a sample space mathcalZ, where the probability distributions are parameterised via some p-dimensional parameter vector boldsymboltheta on a parameter space Theta. Suppose that we have data from one such distribution, which we denote as boldsymbolZ. Then, the goal of parameter point estimation is to come up with an estimate of the unknown boldsymboltheta from boldsymbolZ using an estimator,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" hatboldsymboltheta  mathcalZ to Theta","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"which is a mapping from the sample space to the parameter space.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Estimators can be constructed within a decision-theoretic framework. Assume that the sample space is mathcalZ = mathbbR^n, and consider a nonnegative loss function, L(boldsymboltheta hatboldsymboltheta(boldsymbolZ)), which assesses an estimator hatboldsymboltheta(cdot) for a given boldsymboltheta and data set boldsymbolZ sim f(boldsymbolz mid boldsymboltheta), where f(boldsymbolz mid boldsymboltheta) is the probability density function of the data conditional on boldsymboltheta. An estimator's Bayes risk is its loss averaged over all possible parameter values and data realisations,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"int_Theta int_mathcalZ  L(boldsymboltheta hatboldsymboltheta(boldsymbolz))f(boldsymbolz mid boldsymboltheta) rmd boldsymbolz rmd Pi(boldsymboltheta)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where Pi(cdot) is a prior measure for boldsymboltheta. Any minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdot cdot) and Pi(cdot).","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Bayes estimators are theoretically attractive: for example, unique Bayes estimators are admissible and, under suitable regularity conditions and the squared-error loss, are consistent and asymptotically efficient. Further, for a large class of prior distributions, every set of conditions that imply consistency of the maximum likelihood (ML) estimator also imply consistency of Bayes estimators. Importantly, Bayes estimators are not motivated purely by asymptotics: by construction, they are Bayes irrespective of the sample size and model class. Unfortunately, however, Bayes estimators are typically unavailable in closed form for the complex models often encountered in practice. A way forward is to assume a flexible parametric model for hatboldsymboltheta(cdot), and to optimise the parameters within that model in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are also fast to evaluate, usually involving only simple matrix-vector operations.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Let hatboldsymboltheta(boldsymbolZ boldsymbolgamma) denote a neural network that returns a point estimate from data boldsymbolZ, where boldsymbolgamma contains the neural-network parameters. Bayes estimators may be approximated with hatboldsymboltheta(cdot boldsymbolgamma^*) by solving the optimisation problem,  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"boldsymbolgamma^*\nequiv\nundersetboldsymbolgammamathrmargmin  \nfrac1K sum_k = 1^K L(boldsymboltheta hatboldsymboltheta(boldsymbolz boldsymbolgamma))","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"whose objective function is a Monte Carlo approximation of the Bayes risk made using a set boldsymboltheta^(k)  k = 1 dots K of parameter vectors sampled from the prior Pi(cdot) and, for each k, data boldsymbolZ^(k) simulated from f(boldsymbolz mid  boldsymboltheta). Note that this Monte Carlo approximation does not involve evaluation, or knowledge, of the likelihood function.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"The Monte Carlo approximation of the Bayes risk can be straightforwardly minimised with respect to boldsymbolgamma using back-propagation and stochastic gradient descent. For sufficiently flexible architectures, the point estimator targets a Bayes estimator with respect to L(cdot cdot) and Pi(cdot). We therefore call the fitted neural point estimator a  neural Bayes estimator. Like Bayes estimators, neural Bayes estimators target a specific point summary of the posterior distribution. For instance, the absolute-error and squared-error loss functions lead to neural Bayes estimators that approximate the posterior median and mean, respectively.","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"The neural Bayes estimators is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Define the prior, Pi(cdot).\nChoose a loss function, L(cdot cdot), typically the mean-absolute-error or mean-squared-error loss.\nDesign a suitable neural-network architecture for the neural point estimator hatboldsymboltheta(cdot boldsymbolgamma).\nSample parameters from Pi(cdot) to form training/validation/test parameter sets.\nGiven the above parameter sets, simulate data from the model, to form training/validation/test data sets.\nTrain the neural network (i.e., estimate boldsymbolgamma) by minimising the loss function averaged over the training sets. During training, monitor performance and convergence using the validation sets.\nAssess the fitted neural Bayes estimator, hatboldsymboltheta(cdot boldsymbolgamma^*), using the test set.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Saving-and-loading-neural-estimators","page":"Advanced usage","title":"Saving and loading neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one often trains an estimator and then saves it for later use. As discussed in the Flux documentation, there are a number of ways to do this. Perhaps the simplest approach is to save the parameters (i.e., weights and biases) of the neural network in a BSON file:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using Flux\nusing BSON: @save, @load\nmodel_state = Flux.state(θ̂)\n@save \"estimator.bson\" model_state","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, in a later session, one may initialise a neural network with the same architecture used previously, and load the saved parameters:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"@load \"estimator.bson\" model_state\nFlux.loadmodel!(θ̂, model_state)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that the estimator θ̂ must be already defined (i.e., only the network parameters are saved, not the architecture). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For convenience, the function train() allows for the automatic saving of the neural-network parameters during the training stage, via the argument savepath. Specifically, if savepath is specified, neural estimator's parameters will be saved in the folder savepath and, to load the optimal parameters post training, one may use the following code, or similar:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nFlux.loadparams!(θ̂, loadbestweights(savepath))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, the function loadparams!() loads the parameters of the best (as determined by loadbestweights()) neural estimator saved in savepath.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution may be stored in two ways. Most simply, they can be stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. Alternatively, they can be stored in a user-defined struct subtyping ParameterConfigurations, whose only requirement is a field θ that stores the p times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, the training data set, mathcalZ_texttrain, can be simulated continuously during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Further, this technique allows for data be simulated \"just-in-time\", in the sense that they can be simulated in small batches, used to train the neural estimator, and then removed from memory. This can substantially reduce pressure on memory resources, particularly when working with large data sets. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh the set vartheta_texttrain of parameter vectors used during training, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models. Hybrid approaches are also possible, whereby the parameters (and possibly the data) are held fixed for several epochs (i.e., several passes through the training set when performing stochastic gradient descent) before being refreshed. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with various methods of train().","category":"page"},{"location":"workflow/advancedusage/#Regularisation","page":"Advanced usage","title":"Regularisation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The term regularisation refers to a variety of techniques aimed to reduce overfitting when training a neural network, primarily by discouraging complex models.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One common regularisation technique is known as dropout (Srivastava et al., 2014), implemented in Flux's Dropout layer. Dropout involves temporarily dropping (\"turning off\") a randomly selected set of neurons (along with their connections) at each iteration of the training stage, and this results in a computationally-efficient form of model (neural-network) averaging.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Another class of regularisation techniques involve modifying the loss function. For instance, L₁ regularisation (sometimes called lasso regression) adds to the loss a penalty based on the absolute value of the neural-network parameters. Similarly, L₂ regularisation (sometimes called ridge regression) adds to the loss a penalty based on the square of the neural-network parameters. Note that these penalty terms are not functions of the data or of the statistical-model parameters that we are trying to infer, and therefore do not modify the Bayes risk or the associated Bayes estimator. These regularisation techniques can be implemented straightforwardly by providing a custom optimiser to train that includes a SignDecay object for L₁ regularisation, or a WeightDecay object for L₂ regularisation. See the Flux documentation for further details.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For example, the following code constructs a neural Bayes estimator using dropout and L₁ regularisation with penalty coefficient lambda = 10^-4:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nusing Flux\n\n# Generate data from the model Z ~ N(θ, 1) and θ ~ N(0, 1)\np = 1       # number of unknown parameters in the statistical model\nm = 5       # number of independent replicates\nd = 1       # dimension of each independent replicate\nK = 3000    # number of training samples\nθ_train = randn(1, K)\nθ_val   = randn(1, K)\nZ_train = [μ .+ randn(1, m) for μ ∈ eachcol(θ_train)]\nZ_val   = [μ .+ randn(1, m) for μ ∈ eachcol(θ_val)]\n\n# Architecture with dropout layers\nψ = Chain(\n\tDense(1, 32, relu),\n\tDropout(0.1),\n\tDense(32, 32, relu),\n\tDropout(0.5)\n\t)     \nϕ = Chain(\n\tDense(32, 32, relu),\n\tDropout(0.5),\n\tDense(32, 1)\n\t)           \nθ̂ = DeepSet(ψ, ϕ)\n\n# Optimiser with L₂ regularisation\noptimiser = Flux.setup(OptimiserChain(SignDecay(1e-4), Adam()), θ̂)\n\n# Train the estimator\ntrain(θ̂, θ_train, θ_val, Z_train, Z_val; optimiser = optimiser)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that when the training data and/or parameters are held fixed during training, L₂ regularisation with penalty coefficient lambda = 10^-4 is applied by default.","category":"page"},{"location":"workflow/advancedusage/#Expert-summary-statistics","page":"Advanced usage","title":"Expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Implicitly, neural estimators involve the learning of summary statistics. However, some summary statistics are available in closed form, simple to compute, and highly informative (e.g., sample quantiles, the empirical variogram, etc.). Often, explicitly incorporating these expert summary statistics in a neural estimator can simplify the optimisation problem, and lead to a better estimator. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The fusion of learned and expert summary statistics is facilitated by our implementation of the DeepSet framework. Note that this implementation also allows the user to construct a neural estimator using only expert summary statistics, following, for example, Gerber and Nychka (2021) and Rai et al. (2024). Note also that the user may specify arbitrary expert summary statistics, however, for convenience several standard User-defined summary statistics are provided with the package, including a fast approximate version of the empirical variogram. ","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as boldsymbolZ^(m) equiv (boldsymbolZ_1 dots boldsymbolZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*)\n=\nbegincases\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_1)  m leq m_1\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where, here, boldsymbolgamma^* equiv (boldsymbolgamma^*_tildem_1 dots boldsymbolgamma^*_tildem_l-1), and where boldsymbolgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatboldsymboltheta(cdot boldsymbolgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise neural estimators are implemented with the struct, PiecewiseEstimator, and their construction is facilitated with trainx().  ","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case, for the neural Bayes estimator, the risk function becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"sum_m in mathcalM\nP(M=m)left(\nint_Theta int_mathcalZ^m  L(boldsymboltheta hatboldsymboltheta(boldsymbolz^(m)))f(boldsymbolz^(m) mid boldsymboltheta) rmd boldsymbolz^(m) rmd Pi(boldsymboltheta)\nright)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data during the training phase. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The following pseudocode illustrates how one may modify a general data simulator to train under a range of sample sizes, with the distribution of M defined by passing any object that can be sampled using rand(m, K) (e.g., an integer range like 1:30, an integer-valued distribution from Distributions.jl, etc.):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters, m) \n\n\t## Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t## Generate K sample sizes from the prior distribution for M\n\tm̃ = rand(m, K)\n\n\t## Pseudocode for data simulation\n\tZ = [<simulate m̃[k] realisations from the model> for k ∈ 1:K]\n\n\treturn Z\nend\n\n## Method that allows an integer to be passed for m\nsimulate(parameters, m::Integer) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"To develop a neural estimator with NeuralEstimators,","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior distribution. The parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of parameter vectors in the given parameter set (i.e., training, validation, or test set).\nSimulate data from the assumed model over the parameter sets generated above. These data are stored as a Vector{A}, with each element of the vector associated with one parameter configuration, and where A depends on the multivariate structure of the data and the representation of the neural estimator (e.g., an Array for CNN-based estimators, a GNNGraph for GNN-based estimators, etc.).\nInitialise a neural network θ̂.  \nTrain θ̂ under the chosen loss function using train().\nAssess θ̂ using assess(), which utilises various simulation-based empirical methods for assessing the estimator with respect to its sampling distribution.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Once the estimator θ̂ has passed our assessments and is therefore deemed to be well calibratred, it may be applied to observed data. See the Examples and, once familiar with the basic workflow, see Advanced usage for practical considerations on how to most effectively construct neural estimators.","category":"page"},{"location":"API/core/#Core","page":"Core","title":"Core","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"This page documents the classes and functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these classes and functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to using a neural Bayes estimator to make inference with observed data sets.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Parameters sampled from the prior distribution are stored as a p times K matrix, where p is the number of parameters in the statistical model and K is the number of parameter vectors sampled from the prior distribution.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type ParameterConfigurations, whose only requirement is a field θ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.\n\nThe user-defined type must have a field θ that stores the p × K matrix of parameters, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.\n\nSee subsetparameters for the generic function for subsetting these objects.\n\nExamples\n\nstruct P <: ParameterConfigurations\n\tθ\n\t# other expensive intermediate objects...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimators facilitates neural estimation for arbitrary statistical models by having the user implicitly define their model via simulated data, either as fixed instances or via a function that simulates data from the statistical model.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The data are always stored as a Vector{A}, where each element of the vector corresponds to a data set of m independent replicates associated with one parameter vector (note that m is arbitrary), and where the type A depends on the multivariate structure of the data:","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"For univariate and unstructured multivariate data, A is a d times m matrix where d is the dimension each replicate (e.g., d=1 for univariate data).\nFor data collected over a regular grid, A is a (N + 2)-dimensional array, where N is the dimension of the grid (e.g., N = 1 for time series, N = 2 for two-dimensional spatial grids, etc.). The first N dimensions of the array correspond to the dimensions of the grid; the penultimate dimension stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes, and so on); and the final dimension stores the independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10x15 grid, one would construct an array of dimension 10x15x2x50.\nFor spatial data collected over irregular spatial locations, A is a GNNGraph with independent replicates (possibly with differing spatial locations) stored as subgraphs using the function batch.","category":"page"},{"location":"API/core/#Estimators","page":"Core","title":"Estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Several classes of neural estimators are available in the package.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The simplest class is PointEstimator, used for constructing arbitrary mappings from the sample space to the parameter space. When constructing a generic point estimator, the user defines the loss function and therefore the Bayes estimator that will be targeted.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Several classes cater for the estimation of marginal posterior quantiles, based on the quantile loss function (see quantileloss()); in particular, see IntervalEstimator and QuantileEstimatorDiscrete for estimating marginal posterior quantiles for a fixed set of probability levels, and QuantileEstimatorContinuous for estimating marginal posterior quantiles with the probability level as an input to the neural network.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"In addition to point estimation, the package also provides the class RatioEstimator for approximating the so-called likelihood-to-evidence ratio. The binary classification problem at the heart of this approach proceeds based on the binary cross-entropy loss.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Users are free to choose the neural-network architecture of these estimators as they see fit (subject to some class-specific requirements), but the package also provides the convenience constructor initialise_estimator().","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimator\n\nPointEstimator\n\nIntervalEstimator\n\nQuantileEstimatorDiscrete\n\nQuantileEstimatorContinuous\n\nRatioEstimator\n\nPiecewiseEstimator","category":"page"},{"location":"API/core/#NeuralEstimators.NeuralEstimator","page":"Core","title":"NeuralEstimators.NeuralEstimator","text":"NeuralEstimator\n\nAn abstract supertype for neural estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PointEstimator","page":"Core","title":"NeuralEstimators.PointEstimator","text":"PointEstimator(deepset::DeepSet)\n\nA neural point estimator, a mapping from the sample space to the parameter space.\n\nThe estimator leverages the DeepSet architecture. The only requirement is that number of output neurons in the final layer of the inference network (i.e., the outer network) is equal to the number of parameters in the statistical model.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.IntervalEstimator","page":"Core","title":"NeuralEstimators.IntervalEstimator","text":"IntervalEstimator(u, v = u; probs = [0.025, 0.975], g::Function = exp)\nIntervalEstimator(u, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)\nIntervalEstimator(u, v, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)\n\nA neural interval estimator which, given data Z, jointly estimates marginal posterior credible intervals based on the probability levels probs.\n\nThe estimator employs a representation that prevents quantile crossing, namely, it constructs marginal posterior credible intervals for each parameter theta_i, i = 1 dots p  of the form,\n\nc_i(u_i(boldsymbolZ))  c_i(u_i(boldsymbolZ)) + g(v_i(boldsymbolZ)))\n\nwhere  boldsymbolu() equiv (u_1(cdot) dots u_p(cdot)) and boldsymbolv() equiv (v_1(cdot) dots v_p(cdot)) are neural networks that transform data into p-dimensional vectors; g(cdot) is a monotonically increasing function (e.g., exponential or softplus); and each c_i() is a monotonically increasing function that maps its input to the prior support of theta_i.\n\nThe functions c_i() may be defined by a p-dimensional object of type Compress. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted.\n\nIf only a single neural-network architecture is provided, it will be used for both boldsymbolu() and boldsymbolv().\n\nThe return value  when applied to data is a matrix with 2p rows, where the first and second p rows correspond to the lower and upper bounds, respectively.\n\nSee also QuantileEstimatorDiscrete and QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Generate some toy data\nn = 2   # bivariate data\nm = 100 # number of independent replicates\nZ = rand(n, m)\n\n# prior\np = 3  # number of parameters in the statistical model\nmin_supp = [25, 0.5, -pi/2]\nmax_supp = [500, 2.5, 0]\ng = Compress(min_supp, max_supp)\n\n# Create an architecture\nw = 8  # width of each layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ = Chain(Dense(w, w, relu), Dense(w, p));\nu = DeepSet(ψ, ϕ)\n\n# Initialise the interval estimator\nestimator = IntervalEstimator(u, g)\n\n# Apply the (untrained) interval estimator\nestimator(Z)\ninterval(estimator, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorDiscrete","page":"Core","title":"NeuralEstimators.QuantileEstimatorDiscrete","text":"QuantileEstimatorDiscrete(v::DeepSet; probs = [0.05, 0.25, 0.5, 0.75, 0.95], g = Flux.softplus, i = nothing)\n(estimator::QuantileEstimatorDiscrete)(Z)\n(estimator::QuantileEstimatorDiscrete)(Z, θ₋ᵢ)\n\nA neural estimator that jointly estimates a fixed set of marginal posterior quantiles with probability levels tau_1 dots tau_T, controlled by the keyword argument probs.\n\nBy default, the estimator approximates the marginal quantiles for all parameters in the model,  that is, the quantiles of \n\ntheta_i mid boldsymbolZ\n\nfor parameters boldsymboltheta equiv (theta_1 dots theta_p). Alternatively, if initialised with i set to a positive integer (or collection of integers), the estimator approximates the quantiles of the full conditional distribution\n\ntheta_i mid boldsymbolZ boldsymboltheta_-i\n\nwhere boldsymboltheta_-i denotes the parameter vector with its ith element(s) removed. For ease of exposition, when targetting marginal posteriors of the form theta_i mid boldsymbolZ (i.e., the default behaviour), we define textdim(boldsymboltheta_-i)  0.\n\nThe estimator leverages the DeepSet architecture, subject to two requirements. First, the number of input neurons in the first layer of the inference network (i.e., the outer network) must be equal to the number of neurons in the final layer of the summary network plus textdim(boldsymboltheta_-i). Second, the number of output neurons in the final layer of the inference network must be equal to p - textdim(boldsymboltheta_-i).   The estimator employs a representation that prevents quantile crossing, namely,\n\nbeginaligned\nboldsymbolq^(tau_1)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ)\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ) + sum_j=2^t g(boldsymbolv^(tau_j)(boldsymbolZ)) quad t = 2 dots T\nendaligned\n\nwhere boldsymbolq^(tau)(boldsymbolZ) denotes the vector of tau-quantiles for parameters boldsymboltheta equiv (theta_1 dots theta_p),  and boldsymbolv^(tau_t)(cdot), t = 1 dots T, are unconstrained neural networks that transform data into p-dimensional vectors, and g(cdot) is a non-negative function (e.g., exponential or softplus) applied elementwise to its arguments. If g=nothing, the quantiles are estimated independently through the representation,  \n\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_t)(boldsymbolZ) quad t = 1 dots T \n\nThe return value is a matrix with  (p - textdim(boldsymboltheta_-i)) times T rows, where the first set of T rows corresponds to the estimated quantiles for the first parameter, the second set of T rows corresponds to the estimated quantiles for the second parameter, and so on.\n\nSee also IntervalEstimator and QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux, Distributions\n\n# Simple model Z|θ ~ N(θ, 1) with prior θ ~ N(0, 1)\nd = 1   # dimension of each independent replicate\np = 1   # number of unknown parameters in the statistical model\nm = 30  # number of independent replicates in each data set\nprior(K) = randn32(p, K)\nsimulate(θ, m) = [μ .+ randn32(d, m) for μ ∈ eachcol(θ)]\n\n# Architecture\nψ = Chain(Dense(d, 32, relu), Dense(32, 32, relu))\nϕ = Chain(Dense(32, 32, relu), Dense(32, p))\nv = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nτ = [0.05, 0.25, 0.5, 0.75, 0.95]\nq̂ = QuantileEstimatorDiscrete(v; probs = τ)\n\n# Train the estimator\nq̂ = train(q̂, prior, simulate, m = m)\n\n# Closed-form posterior for comparison\nfunction posterior(Z; μ₀ = 0, σ₀ = 1, σ² = 1)\n\n\t# Parameters of posterior distribution\n\tμ̃ = (1/σ₀^2 + length(Z)/σ²)^-1 * (μ₀/σ₀^2 + sum(Z)/σ²)\n\tσ̃ = sqrt((1/σ₀^2 + length(Z)/σ²)^-1)\n\n\t# Posterior\n\tNormal(μ̃, σ̃)\nend\n\n# Estimate posterior quantiles for 1000 test data sets\nθ = prior(1000)\nZ = simulate(θ, m)\nq̂(Z)                                             # neural quantiles\nreduce(hcat, quantile.(posterior.(Z), Ref(τ)))   # true quantiles\n\n\n# ---- Full conditionals ----\n\n\n# Simple model Z|μ,σ ~ N(μ, σ²) with μ ~ N(0, 1), σ ∼ IG(3,1)\nd = 1         # dimension of each independent replicate\np = 2         # number of unknown parameters in the statistical model\nm = 30        # number of independent replicates in each data set\nfunction sample(K)\n\tμ = randn32(1, K)\n\tσ = rand(InverseGamma(3, 1), 1, K)\n\tθ = vcat(μ, σ)\nend\nsimulate(θ, m) = θ[1] .+ θ[2] .* randn32(1, m)\nsimulate(θ::Matrix, m) = simulate.(eachcol(θ), m)\n\n# Architecture\nψ = Chain(Dense(d, 32, relu), Dense(32, 32, relu))\nϕ = Chain(Dense(32 + 1, 32, relu), Dense(32, 1))\nv = DeepSet(ψ, ϕ)\n\n# Initialise estimators respectively targetting quantiles of μ∣Z,σ and σ∣Z,μ \nτ = [0.05, 0.25, 0.5, 0.75, 0.95]\nq₁ = QuantileEstimatorDiscrete(v; probs = τ, i = 1)\nq₂ = QuantileEstimatorDiscrete(v; probs = τ, i = 2)\n\n# Train the estimators\nq₁ = train(q₁, sample, simulate, m = m)\nq₂ = train(q₂, sample, simulate, m = m)\n\n# Estimate quantiles of μ∣Z,σ with σ = 0.5 and for 1000 data sets\nθ = prior(1000)\nZ = simulate(θ, m)    \nθ₋ᵢ = 0.5f0 \nq₁(Z, θ₋ᵢ)\n\n# Can also apply to a single data set only \nq₁(Z[1], θ₋ᵢ)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorContinuous","page":"Core","title":"NeuralEstimators.QuantileEstimatorContinuous","text":"QuantileEstimatorContinuous(deepset::DeepSet; i = nothing)\n(estimator::QuantileEstimatorContinuous)(Z, τ)\n(estimator::QuantileEstimatorContinuous)(Z, θ₋ᵢ, τ)\n\nA neural estimator targetting posterior quantiles.\n\nGiven as input data boldsymbolZ and the desired probability level tau  (0 1), by default the estimator approximates the tau-quantile of\n\ntheta_i mid boldsymbolZ\n\nfor parameters boldsymboltheta equiv (theta_1 dots theta_p). Alternatively, if initialised with i set to a positive integer (or collection of integers), the estimator approximates the tau-quantile of the full conditional distribution\n\ntheta_i mid boldsymbolZ boldsymboltheta_-i\n\nwhere boldsymboltheta_-i denotes the parameter vector with its ith element(s) removed. For ease of exposition, when targetting marginal posteriors of the form theta_i mid boldsymbolZ (i.e., the default behaviour), we define textdim(boldsymboltheta_-i)  0.\n\nThe estimator leverages the DeepSet architecture, subject to two requirements. First, the number of input neurons in the first layer of the inference network (i.e., the outer network) must be equal to the number of neurons in the final layer of the summary network plus 1 + textdim(boldsymboltheta_-i). Second, the number of output neurons in the final layer of the inference network must be equal to p - textdim(boldsymboltheta_-i).\n\nAlthough not a requirement, one may employ a (partially) monotonic neural network to prevent quantile crossing (i.e., to ensure that the tau_1-quantile does not exceed the tau_2-quantile for any tau_2  tau_1). There are several ways to construct such a neural network: one simple yet effective approach is to ensure that all weights associated with tau are strictly positive (see, e.g., Cannon, 2018), and this can be done using the DensePositive layer as illustrated in the examples below.\n\nThe return value is a matrix with p - textdim(boldsymboltheta_-i) rows, corresponding to the estimated quantile for each parameter not in boldsymboltheta_-i.\n\nSee also QuantileEstimatorDiscrete.\n\nExamples\n\nusing NeuralEstimators, Flux, Distributions, InvertedIndices, Statistics\n\n# Simple model Z|θ ~ N(θ, 1) with prior θ ~ N(0, 1)\nd = 1         # dimension of each independent replicate\np = 1         # number of unknown parameters in the statistical model\nm = 30        # number of independent replicates in each data set\nprior(K) = randn32(p, K)\nsimulateZ(θ, m) = [μ .+ randn32(d, m) for μ ∈ eachcol(θ)]\nsimulateτ(K)    = [rand32(1) for k in 1:K]\nsimulate(θ, m)  = simulateZ(θ, m), simulateτ(size(θ, 2))\n\n# Architecture: partially monotonic network to preclude quantile crossing\nw = 64  # width of each hidden layer\nψ = Chain(\n\tDense(d, w, relu),\n\tDense(w, w, relu),\n\tDense(w, w, relu)\n\t)\nϕ = Chain(\n\tDensePositive(Dense(w + 1, w, relu); last_only = true),\n\tDensePositive(Dense(w, w, relu)),\n\tDensePositive(Dense(w, p))\n\t)\ndeepset = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nq̂ = QuantileEstimatorContinuous(deepset)\n\n# Train the estimator\nq̂ = train(q̂, prior, simulate, m = m)\n\n# Closed-form posterior for comparison\nfunction posterior(Z; μ₀ = 0, σ₀ = 1, σ² = 1)\n\n\t# Parameters of posterior distribution\n\tμ̃ = (1/σ₀^2 + length(Z)/σ²)^-1 * (μ₀/σ₀^2 + sum(Z)/σ²)\n\tσ̃ = sqrt((1/σ₀^2 + length(Z)/σ²)^-1)\n\n\t# Posterior\n\tNormal(μ̃, σ̃)\nend\n\n# Estimate the posterior 0.1-quantile for 1000 test data sets\nθ = prior(1000)\nZ = simulateZ(θ, m)\nτ = 0.1f0\nq̂(Z, τ)                        # neural quantiles\nquantile.(posterior.(Z), τ)'   # true quantiles\n\n# Estimate several quantiles for a single data set\nz = Z[1]\nτ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])\nq̂(z, τ')                     # neural quantiles (note that τ is given as row vector)\nquantile.(posterior(z), τ)   # true quantiles\n\n\n# ---- Full conditionals ----\n\n\n# Simple model Z|μ,σ ~ N(μ, σ²) with μ ~ N(0, 1), σ ∼ IG(3,1)\nd = 1         # dimension of each independent replicate\np = 2         # number of unknown parameters in the statistical model\nm = 30        # number of independent replicates in each data set\nfunction sample(K)\n\tμ = randn32(K)\n\tσ = rand(InverseGamma(3, 1), K)\n\tθ = hcat(μ, σ)'\n\tθ = Float32.(θ)\n\treturn θ\nend\nsimulateZ(θ, m) = θ[1] .+ θ[2] .* randn32(1, m)\nsimulateZ(θ::Matrix, m) = simulateZ.(eachcol(θ), m)\nsimulateτ(K)    = [rand32(1) for k in 1:K]\nsimulate(θ, m)  = simulateZ(θ, m), simulateτ(size(θ, 2))\n\n# Architecture: partially monotonic network to preclude quantile crossing\nw = 64  # width of each hidden layer\nψ = Chain(\n\tDense(d, w, relu),\n\tDense(w, w, relu),\n\tDense(w, w, relu)\n\t)\nϕ = Chain(\n\tDensePositive(Dense(w + p, w, relu); last_only = true),\n\tDensePositive(Dense(w, w, relu)),\n\tDensePositive(Dense(w, 1))\n\t)\ndeepset = DeepSet(ψ, ϕ)\n\n# Initialise the estimator for the first parameter, targetting μ∣Z,σ\ni = 1\nq̂ = QuantileEstimatorContinuous(deepset; i = i)\n\n# Train the estimator\nq̂ = train(q̂, sample, simulate, m = m)\n\n# Estimate quantiles of μ∣Z,σ with σ = 0.5 and for 1000 data sets\nθ = prior(1000)\nZ = simulateZ(θ, m)\nθ₋ᵢ = 0.5f0    # for multiparameter scenarios, use θ[Not(i), :] to determine the order that the conditioned parameters should be given\nτ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])\nq̂(Z, θ₋ᵢ, τ)\n\n# Estimate quantiles for a single data set\nq̂(Z[1], θ₋ᵢ, τ)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.RatioEstimator","page":"Core","title":"NeuralEstimators.RatioEstimator","text":"RatioEstimator(deepset::DeepSet)\n\nA neural estimator that estimates the likelihood-to-evidence ratio,\n\nr(boldsymbolZ boldsymboltheta) equiv p(boldsymbolZ mid boldsymboltheta)p(boldsymbolZ)\n\nwhere p(boldsymbolZ mid boldsymboltheta) is the likelihood and p(boldsymbolZ) is the marginal likelihood, also known as the model evidence.\n\nThe estimator leverages the DeepSet architecture, subject to two requirements. First, the number of input neurons in the first layer of the inference network (i.e., the outer network) must equal the number of output neurons in the final layer of the summary network plus the number of parameters in the statistical model. Second, the number of output neurons in the final layer of the inference network must be equal to one.\n\nThe ratio estimator is trained by solving a relatively straightforward binary classification problem. Specifically, consider the problem of distinguishing dependent parameter–data pairs (boldsymboltheta boldsymbolZ) sim p(boldsymbolZ boldsymboltheta) with class labels Y=1 from independent parameter–data pairs (tildeboldsymboltheta tildeboldsymbolZ) sim p(boldsymboltheta)p(boldsymbolZ) with class labels Y=0, and where the classes are balanced. Then the Bayes classifier under binary cross-entropy loss is given by\n\nc(boldsymbolZ boldsymboltheta) = fracp(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta) + p(boldsymboltheta)p(boldsymbolZ)\n\nand hence,\n\nr(boldsymbolZ boldsymboltheta) = fracc(boldsymbolZ boldsymboltheta)1 - c(boldsymbolZ boldsymboltheta)\n\nFor numerical stability, training is done on the log-scale using log r(boldsymbolZ boldsymboltheta) = textlogit(c(boldsymbolZ boldsymboltheta)).\n\nWhen applying the estimator to data, by default the likelihood-to-evidence ratio r(boldsymbolZ boldsymboltheta) is returned (setting the keyword argument classifier = true will yield class probability estimates). The estimated ratio can then be used in various downstream Bayesian (e.g., Hermans et al., 2020) or Frequentist (e.g., Walchessen et al., 2023) inferential algorithms.\n\nSee also mlestimate and mapestimate for obtaining approximate maximum-likelihood and maximum-a-posteriori estimates, and sampleposterior for obtaining approximate posterior samples.\n\nExamples\n\nusing NeuralEstimators, Flux, Statistics\n\n# Generate data from Z|μ,σ ~ N(μ, σ²) with μ, σ ~ U(0, 1)\np = 2     # number of unknown parameters in the statistical model\nd = 1     # dimension of each independent replicate\nm = 100   # number of independent replicates\n\nprior(K) = rand32(p, K)\nsimulate(θ, m) = θ[1] .+ θ[2] .* randn32(d, m)\nsimulate(θ::AbstractMatrix, m) = simulate.(eachcol(θ), m)\n\n# Architecture\nw = 64 # width of each hidden layer\nψ = Chain(\n\tDense(d, w, relu),\n\tDense(w, w, relu),\n\tDense(w, q, relu)\n\t)\nϕ = Chain(\n\tDense(w + p, w, relu),\n\tDense(w, w, relu),\n\tDense(w, 1)\n\t)\ndeepset = DeepSet(ψ, ϕ)\n\n# Initialise the estimator\nr̂ = RatioEstimator(deepset)\n\n# Train the estimator\nr̂ = train(r̂, prior, simulate, m = m)\n\n# Inference with \"observed\" data set\nθ = prior(1)\nz = simulate(θ, m)[1]\nθ₀ = [0.5, 0.5]                           # initial estimate\nmlestimate(r̂, z;  θ₀ = θ₀)                # maximum-likelihood estimate\nmapestimate(r̂, z; θ₀ = θ₀)                # maximum-a-posteriori estimate\nθ_grid = expandgrid(0:0.01:1, 0:0.01:1)'  # fine gridding of the parameter space\nθ_grid = Float32.(θ_grid)\nr̂(z, θ_grid)                              # likelihood-to-evidence ratios over grid\nsampleposterior(r̂, z; θ_grid = θ_grid)    # posterior samples\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator(estimators, changepoints)\n\nCreates a piecewise estimator (Sainsbury-Dale et al., 2024, sec. 2.2.2) from a collection of estimators and sample-size changepoints.\n\nSpecifically, with l estimators and sample-size changepoints m_1  m_2  dots  m_l-1, the piecewise etimator takes the form,\n\nhatboldsymboltheta(boldsymbolZ)\n=\nbegincases\nhatboldsymboltheta_1(boldsymbolZ)  m leq m_1\nhatboldsymboltheta_2(boldsymbolZ)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta_l(boldsymbolZ)  m  m_l-1\nendcases\n\nFor example, given an estimator  hatboldsymboltheta_1(cdot) trained for small sample sizes (e.g., m ≤ 30) and an estimator hatboldsymboltheta_2(cdot) trained for moderate-to-large sample sizes (e.g., m > 30), we may construct a PiecewiseEstimator that dispatches hatboldsymboltheta_1(cdot) if m ≤ 30 and hatboldsymboltheta_2(cdot) otherwise.\n\nSee also trainx() for training estimators for a range of sample sizes.\n\nExamples\n\nusing NeuralEstimators, Flux\n\nd = 2  # bivariate data\np = 3  # number of parameters in the statistical model\nw = 8  # width of each hidden layer\n\n# Small-sample estimator\nψ₁ = Chain(Dense(d, w, relu), Dense(w, w, relu));\nϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));\nθ̂₁ = PointEstimator(DeepSet(ψ₁, ϕ₁))\n\n# Large-sample estimator\nψ₂ = Chain(Dense(d, w, relu), Dense(w, w, relu));\nϕ₂ = Chain(Dense(w, w, relu), Dense(w, p));\nθ̂₂ = PointEstimator(DeepSet(ψ₂, ϕ₂))\n\n# Piecewise estimator with changepoint m=30\nθ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], 30)\n\n# Apply the (untrained) piecewise estimator to data\nZ = [rand(d, 1, m) for m ∈ (10, 50)]\nθ̂(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The function train is used to train a single neural estimator, while the wrapper function trainx is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"train\n\ntrainx","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core","title":"NeuralEstimators.train","text":"train(θ̂, sampler::Function, simulator::Function; ...)\ntrain(θ̂, θ_train::P, θ_val::P, simulator::Function; ...) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\ntrain(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; ...) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\nTrain a neural estimator θ̂.\n\nThe methods cater for different variants of \"on-the-fly\" simulation. Specifically, a sampler can be provided to continuously sample new parameter vectors from the prior, and a simulator can be provided to continuously simulate new data conditional on the parameters. If provided with specific sets of parameters (θ_train and θ_val) and/or data (Z_train and Z_val), they will be held fixed during training.\n\nIn all methods, the validation parameters and data are held fixed to reduce noise when evaluating the validation risk.\n\nKeyword arguments common to all methods:\n\nloss = mae\nepochs::Integer = 100\nbatchsize::Integer = 32\noptimiser = ADAM()\nsavepath::String = \"\": path to save the neural-network weights during training (as bson files) and other information, such as the risk vs epoch (the risk function evaluated over the training and validation sets are saved in the first and second columns of loss_per_epoch.csv). If savepath is an empty string (default), nothing is saved.\nstopping_epochs::Integer = 5: cease training if the risk doesn't improve in this number of epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nKeyword arguments common to train(θ̂, sampler, simulator) and train(θ̂, θ_train, θ_val, simulator):\n\nm: sample sizes (either an Integer or a collection of Integers). The simulator is called as simulator(θ, m).\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nsimulate_just_in_time::Bool = false: flag indicating whether we should simulate just-in-time, in the sense that only a batchsize number of parameter vectors and corresponding data are in memory at a given time.\n\nKeyword arguments unique to train(θ̂, sampler, simulator):\n\nK::Integer = 10000: number of parameter vectors in the training set; the size of the validation set is K ÷ 5.\nξ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices). If provided, the parameter sampler is called as sampler(K, ξ); otherwise, the parameter sampler will be called as sampler(K). Can also be provided as xi.\nepochs_per_θ_refresh::Integer = 1: how often to refresh the training parameters. Must be a multiple of epochs_per_Z_refresh. Can also be provided as epochs_per_theta_refresh.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# parameter sampler\nfunction sampler(K)\n\tμ = randn(K) # Gaussian prior\n\tσ = rand(K)  # Uniform prior\n\tθ = hcat(μ, σ)'\n\treturn θ\nend\n\n# data simulator\nsimulator(θ_matrix, m) = [θ[1] .+ θ[2] * randn32(1, m) for θ ∈ eachcol(θ_matrix)]\n\n# architecture\nd = 1   # dimension of each replicate\np = 2   # number of parameters in the statistical model\nψ = Chain(Dense(1, 32, relu), Dense(32, 32, relu))\nϕ = Chain(Dense(32, 32, relu), Dense(32, p))\nθ̂ = DeepSet(ψ, ϕ)\n\n# number of independent replicates to use during training\nm = 15\n\n# training: full simulation on-the-fly\nθ̂ = train(θ̂, sampler, simulator, m = m, epochs = 5)\n\n# training: simulation on-the-fly with fixed parameters\nK = 10000\nθ_train = sampler(K)\nθ_val   = sampler(K ÷ 5)\nθ̂ \t\t = train(θ̂, θ_train, θ_val, simulator, m = m, epochs = 5)\n\n# training: fixed parameters and fixed data\nZ_train = simulator(θ_train, m)\nZ_val   = simulator(θ_val, m)\nθ̂ \t\t = train(θ̂, θ_train, θ_val, Z_train, Z_val, epochs = 5)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.trainx","page":"Core","title":"NeuralEstimators.trainx","text":"trainx(θ̂, sampler::Function, simulator::Function, m::Vector{Integer}; ...)\ntrainx(θ̂, θ_train, θ_val, simulator::Function, m::Vector{Integer}; ...)\ntrainx(θ̂, θ_train, θ_val, Z_train, Z_val, m::Vector{Integer}; ...)\ntrainx(θ̂, θ_train, θ_val, Z_train::V, Z_val::V; ...) where {V <: AbstractVector{AbstractVector{Any}}}\n\nA wrapper around train() to construct neural estimators for different sample sizes.\n\nThe positional argument m specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if m = [m₁, m₂], the estimator for sample size m₂ is pre-trained with the estimator for sample size m₁.\n\nThe method for Z_train and Z_val subsets the data using subsetdata(Z, 1:mᵢ) for each mᵢ ∈ m. The method for Z_train::V and Z_val::V trains an estimator for each element of Z_train::V and Z_val::V and, hence, it does not need to invoke subsetdata(), which can be slow or difficult to define in some cases (e.g., for graphical data). Note that, in this case, m is inferred from the data.\n\nThe keyword arguments inherit from train(). The keyword arguments epochs, batchsize, stopping_epochs, and optimiser can each be given as vectors. For example, if we are training two estimators, we can use a different number of epochs for each estimator by providing epochs = [epoch₁, epoch₂].\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessment/calibration","page":"Core","title":"Assessment/calibration","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"assess\n\nAssessment\n\ndiagnostics\n\nrisk\n\nbias\n\nrmse\n\ncoverage","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core","title":"NeuralEstimators.assess","text":"assess(estimators, θ, Z)\n\nUsing a collection of estimators, compute estimates from data Z simulated based on true parameter vectors stored in θ.\n\nThe data Z should be a Vector, with each element corresponding to a single simulated data set. If Z contains more data sets than parameter vectors, the parameter matrix θ will be recycled by horizontal concatenation via the call θ = repeat(θ, outer = (1, J)) where J = length(Z) ÷ K is the number of simulated data sets and K = size(θ, 2) is the number of parameter vectors.\n\nThe output is of type Assessment; see ?Assessment for details.\n\nKeyword arguments\n\nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided). If ξ is provided with a field parameter_names, those names will be used.\nξ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices). Can also be provided as xi.\nuse_ξ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses ξ: if it does, the estimator will be applied as estimator(Z, ξ). This argument is useful when multiple estimators are provided, only some of which need ξ; hence, if only one estimator is provided and ξ is not nothing, use_ξ is automatically set to true. Can also be provided as use_xi.\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\nExamples\n\nusing NeuralEstimators, Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nψ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nϕ = Chain(Dense(w, w, relu), Dense(w, p));\nθ̂ = DeepSet(ψ, ϕ)\n\n# Generate testing parameters\nK = 100\nθ = rand32(p, K)\n\n# Data for a single sample size\nm = 30\nZ = [rand32(n, m) for _ ∈ 1:K];\nassessment = assess(θ̂, θ, Z);\nrisk(assessment)\n\n# Multiple data sets for each parameter vector\nJ = 5\nZ = repeat(Z, J);\nassessment = assess(θ̂, θ, Z);\nrisk(assessment)\n\n# With set-level information\nqₓ = 2\nϕ  = Chain(Dense(w + qₓ, w, relu), Dense(w, p));\nθ̂ = DeepSet(ψ, ϕ)\nx = [rand(qₓ) for _ ∈ eachindex(Z)]\nassessment = assess(θ̂, θ, (Z, x));\nrisk(assessment)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core","title":"NeuralEstimators.Assessment","text":"Assessment(df::DataFrame, runtime::DataFrame)\n\nA type for storing the output of assess(). The field runtime contains the total time taken for each estimator. The field df is a long-form DataFrame with columns:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size (number of iid replicates) for the given data set\nk:         the index of the parameter vector\nj:         the index of the data set (in the case that multiple data sets are associated with each parameter vector)\n\nNote that if estimator is an IntervalEstimator, the column estimate will be replaced by the columns lower and upper, containing the lower and upper bounds of the interval, respectively.\n\nMultiple Assessment objects can be combined with merge() (used for combining assessments from multiple point estimators) or join() (used for combining assessments from a point estimator and an interval estimator).\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.diagnostics","page":"Core","title":"NeuralEstimators.diagnostics","text":"diagnostics(assessment::Assessment; args...)\n\nComputes all applicable diagnostics.\n\nFor a PointEstimator, the relevant diagnostics are the estimator's bias, rmse, and risk, while for an IntervalEstimator the relevant diagnostics are the coverage and intervalscore.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.risk","page":"Core","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's Bayes risk,\n\nr(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K L(boldsymboltheta^(k) hatboldsymboltheta(boldsymbolZ^(k)))\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nKeyword arguments\n\nloss = (x, y) -> abs(x - y): a binary operator defining the loss function (default absolute-error loss).\naverage_over_parameters::Bool = false: if true, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the loss is averaged over all sample sizes m; otherwise, the loss is averaged over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bias","page":"Core","title":"NeuralEstimators.bias","text":"bias(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's bias,\n\nrmbias(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k)\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.rmse","page":"Core","title":"NeuralEstimators.rmse","text":"rmse(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's root-mean-squared error,\n\nrmrmse(hatboldsymboltheta(cdot))\napprox\nsqrtfrac1K sum_k=1^K (hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k))^2\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.coverage","page":"Core","title":"NeuralEstimators.coverage","text":"coverage(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an interval estimator's expected coverage.\n\nKeyword arguments\n\naverage_over_parameters::Bool = false: if true, the coverage is averaged over all parameters; otherwise (default), it is computed over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the coverage is averaged over all sample sizes m; otherwise, it is computed over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-with-observed-data","page":"Core","title":"Inference with observed data","text":"","category":"section"},{"location":"API/core/#Inference-using-point-estimators","page":"Core","title":"Inference using point estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Inference with a neural Bayes (point) estimator proceeds simply by applying the estimator θ̂ to the observed data Z (possibly containing multiple data sets) in a call of the form θ̂(Z). To leverage a GPU, simply move the estimator and the data to the GPU using gpu(); see also estimateinbatches() to apply the estimator over batches of data, which can alleviate memory issues when working with a large number of data sets.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Uncertainty quantification often proceeds through the bootstrap distribution, which is essentially available \"for free\" when bootstrap data sets can be quickly generated; this is facilitated by bootstrap() and interval(). Alternatively, one may approximate a set of low and high marginal posterior quantiles using a specially constructed neural Bayes estimator, which can then be used to construct credible intervals: see IntervalEstimator, QuantileEstimatorDiscrete, and QuantileEstimatorContinuous.  ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"bootstrap\n\ninterval","category":"page"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core","title":"NeuralEstimators.bootstrap","text":"bootstrap(θ̂, parameters::P, Z) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(θ̂, parameters::P, simulator, m::Integer; B = 400) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(θ̂, Z; B = 400, blocks = nothing)\n\nGenerates B bootstrap estimates from an estimator θ̂.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, Z, whose length implicitly defines B. Alternatively, one may provide a simulator and the desired sample size, in which case the data will be simulated using simulator(parameters, m).\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm aims to produce resampled data sets that are of a similar size to Z, but this can only be achieved exactly if all blocks are equal in length.\n\nThe keyword argument use_gpu is a flag determining whether to use the GPU, if it is available (default true).\n\nThe return type is a p × B matrix, where p is the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.interval","page":"Core","title":"NeuralEstimators.interval","text":"interval(θ::Matrix; probs = [0.05, 0.95], parameter_names = nothing)\ninterval(estimator::IntervalEstimator, Z; parameter_names = nothing, use_gpu = true)\n\nCompute a confidence interval based either on a p × B matrix θ of parameters (typically containing bootstrap estimates or posterior draws) with p the number of parameters in the model, or from an IntervalEstimator and data Z.\n\nWhen given θ, the intervals are constructed by compute quantiles with probability levels controlled by the keyword argument probs.\n\nThe return type is a p × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval. The rows of this matrix can be named by passing a vector of strings to the keyword argument parameter_names.\n\nExamples\n\nusing NeuralEstimators\np = 3\nB = 50\nθ = rand(p, B)\ninterval(θ)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators","page":"Core","title":"Inference using likelihood and likelihood-to-evidence-ratio estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"mlestimate\n\nmapestimate\n\nsampleposterior","category":"page"},{"location":"API/core/#NeuralEstimators.mlestimate","page":"Core","title":"NeuralEstimators.mlestimate","text":"mlestimate(estimator::RatioEstimator, Z; θ₀ = nothing, θ_grid = nothing, penalty::Function = θ -> 1, use_gpu = true)\n\nComputes the (approximate) maximum likelihood estimate given data boldsymbolZ,\n\nargmax_boldsymboltheta ell(boldsymboltheta  boldsymbolZ)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function derived from estimator.\n\nIf a vector θ₀ of initial parameter estimates is given, the approximate likelihood is maximised by gradient descent. Otherwise, if a matrix of parameters θ_grid is given, the approximate likelihood is maximised by grid search.\n\nA maximum penalised likelihood estimate,\n\nargmax_boldsymboltheta ell(boldsymboltheta  boldsymbolZ) + log p(boldsymboltheta)\n\ncan be obtained by specifying the keyword argument penalty that defines the penalty term p(boldsymboltheta).\n\nSee also mapestimate() for computing (approximate) maximum a posteriori estimates.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.mapestimate","page":"Core","title":"NeuralEstimators.mapestimate","text":"mapestimate(estimator::RatioEstimator, Z; θ₀ = nothing, θ_grid = nothing, prior::Function = θ -> 1, use_gpu = true)\n\nComputes the (approximate) maximum a posteriori estimate given data boldsymbolZ,\n\nargmax_boldsymboltheta ell(boldsymboltheta  boldsymbolZ) + log p(boldsymboltheta)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function derived from estimator, and p(boldsymboltheta) denotes the prior density function controlled through the keyword argument prior (by default, a uniform prior is used).\n\nIf a vector θ₀ of initial parameter estimates is given, the approximate posterior density is maximised by gradient descent. Otherwise, if a matrix of parameters θ_grid is given, the approximate posterior density is maximised by grid search.\n\nSee also mlestimate() for computing (approximate) maximum likelihood estimates.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.sampleposterior","page":"Core","title":"NeuralEstimators.sampleposterior","text":"sampleposterior(estimator::RatioEstimator, Z, N::Integer = 1000; θ_grid, prior::Function = θ -> 1f0)\n\nSamples from the approximate posterior distribution p(boldsymboltheta mid boldsymbolZ) implied by estimator.\n\nThe positional argument N controls the size of the posterior sample.\n\nThe keyword agument θ_grid requires a (fine) gridding of the parameter space, given as a matrix with p rows, with p the number of parameters in the statistical model.\n\nThe prior distribution p(boldsymboltheta) is controlled through the keyword argument prior (by default, a uniform prior is used).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"Order = [:type, :function]\nPages   = [\"utility.md\"]","category":"page"},{"location":"API/utility/#Core","page":"Miscellaneous","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"numberreplicates\n\nsubsetdata\n\nsubsetparameters","category":"page"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Miscellaneous","title":"NeuralEstimators.numberreplicates","text":"numberofreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Miscellaneous","title":"NeuralEstimators.subsetdata","text":"subsetdata(Z::V, i) where {V <: AbstractArray{A}} where {A <: Any}\nsubsetdata(Z::A, i) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, i) where {G <: AbstractGraph}\n\nReturn replicate(s) i from each data set in Z.\n\nIf the user is working with data that are not covered by the default methods, simply overload the function with the appropriate type for Z.\n\nFor graphical data, calls getgraph(), where the replicates are assumed be to stored as batched graphs. Since this can be slow, one should consider using a method of train() that does not require the data to be subsetted when working with graphical data (use numberreplicates() to check that the training and validation data sets are equally replicated, which prevents subsetting).\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nd = 1  # dimension of the response variable\nn = 4  # number of observations in each realisation\nm = 6  # number of replicates in each data set\nK = 2  # number of data sets\n\n# Array data\nZ = [rand(n, d, m) for k ∈ 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ∈ 1:m]) for k ∈ 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Miscellaneous","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Utility-functions","page":"Miscellaneous","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"adjacencymatrix\n\ncontainertype\n\nencodedata\n\nestimateinbatches\n\nexpandgrid\n\nIndicatorWeights\n\ninitialise_estimator\n\nloadbestweights\n\nremovedata\n\nrowwisenorm\n\nspatialgraph\n\nstackarrays\n\nvectotril","category":"page"},{"location":"API/utility/#NeuralEstimators.adjacencymatrix","page":"Miscellaneous","title":"NeuralEstimators.adjacencymatrix","text":"adjacencymatrix(S::Matrix, k::Integer; maxmin = false, combined = false)\nadjacencymatrix(S::Matrix, r::AbstractFloat)\nadjacencymatrix(S::Matrix, r::AbstractFloat, k::Integer; random = true)\nadjacencymatrix(M::Matrix; k, r, kwargs...)\n\nComputes a spatially weighted adjacency matrix from spatial locations S based  on either the k-nearest neighbours of each location; all nodes within a disc of fixed radius r; or, if both r and k are provided, a subset of k neighbours within a disc of fixed radius r.\n\nSeveral subsampling strategies are possible when choosing a subset of k neighbours within  a disc of fixed radius r. If random=true (default), the neighbours are randomly selected from  within the disc (note that this also approximately preserves the distribution of  distances within the neighbourhood set). If random=false, a deterministic algorithm is used  that aims to preserve the distribution of distances within the neighbourhood set, by choosing  those nodes with distances to the central node corresponding to the  0 frac1k frac2k dots frack-1k 1 quantiles of the empirical  distribution function of distances within the disc.  (This algorithm in fact yields k+1 neighbours, since both the closest and furthest nodes are always included.)  Otherwise, \n\nIf maxmin=false (default) the k-nearest neighbours are chosen based on all points in the graph. If maxmin=true, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the k-nearest neighbours amongst the points that have already appeared in the ordering. If combined=true, the  neighbours are defined to be the union of the k-nearest neighbours and the  k-nearest neighbours subject to a maxmin ordering. \n\nIf S is a square matrix, it is treated as a distance matrix; otherwise, it should be an n x d matrix, where n is the number of spatial locations and d is the spatial dimension (typically d = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that use of a  maxmin ordering currently requires a matrix of spatial locations (not a distance matrix).\n\nBy convention with the functionality in GraphNeuralNetworks.jl which is based on directed graphs,  the neighbours of location i are stored in the column A[:, i] where A is the  returned adjacency matrix. Therefore, the number of neighbours for each location is given by collect(mapslices(nnz, A; dims = 1)), and the number of times each node is  a neighbour of another node is given by collect(mapslices(nnz, A; dims = 2)).\n\nExamples\n\nusing NeuralEstimators, Distances, SparseArrays\n\nn = 250\nd = 2\nS = rand(Float32, n, d)\nk = 10\nr = 0.10\n\n# Memory efficient constructors\nadjacencymatrix(S, k)\nadjacencymatrix(S, k; maxmin = true)\nadjacencymatrix(S, k; maxmin = true, combined = true)\nadjacencymatrix(S, r)\nadjacencymatrix(S, r, k)\nadjacencymatrix(S, r, k; random = false)\n\n# Construct from full distance matrix D\nD = pairwise(Euclidean(), S, dims = 1)\nadjacencymatrix(D, k)\nadjacencymatrix(D, r)\nadjacencymatrix(D, r, k)\nadjacencymatrix(D, r, k; random = false)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.containertype","page":"Miscellaneous","title":"NeuralEstimators.containertype","text":"containertype(A::Type)\ncontainertype(::Type{A}) where A <: SubArray\ncontainertype(a::A) where A\n\nReturns the container type of its argument.\n\nIf given a SubArray, returns the container type of the parent array.\n\nExamples\n\na = rand(3, 4)\ncontainertype(a)\ncontainertype(typeof(a))\n[containertype(x) for x ∈ eachcol(a)]\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.encodedata","page":"Miscellaneous","title":"NeuralEstimators.encodedata","text":"encodedata(Z::A; fixed_constant::T = zero(T)) where {A <: AbstractArray{Union{Missing, T}, N}} where T, N\n\nFor data Z with missing entries, returns an augmented data set (U, W) where W encodes the missingness pattern as an indicator vector and U is the original data Z with missing entries replaced by a fixed_constant.\n\nThe indicator vector W is stored in the second-to-last dimension of Z, which should be a singleton. If the second-to-last dimension is not singleton, then two singleton dimensions will be added to the array, and W will be stored in the new second-to-last dimension.\n\nExamples\n\nusing NeuralEstimators\n\n# Generate some missing data\nZ = rand(16, 16, 1, 1)\nZ = removedata(Z, 0.25)\t # remove 25% of the data\n\n# Encode the data\nUW = encodedata(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.estimateinbatches","page":"Miscellaneous","title":"NeuralEstimators.estimateinbatches","text":"estimateinbatches(θ̂, z, θ = nothing; batchsize::Integer = 32, use_gpu::Bool = true, kwargs...)\n\nApply the estimator θ̂ on minibatches of z (and optionally parameter vectors or other set-level information θ) of size batchsize.\n\nThis can prevent memory issues that can occur with large data sets, particularly on the GPU.\n\nMinibatching will only be done if there are multiple data sets in z; this will be inferred by z being a vector, or a tuple whose first element is a vector.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Miscellaneous","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.IndicatorWeights","page":"Miscellaneous","title":"NeuralEstimators.IndicatorWeights","text":"IndicatorWeights(h_max, n_bins::Integer)\n(w::IndicatorWeights)(h::Matrix)\n\nFor spatial locations boldsymbols and  boldsymbolu, creates a spatial weight function defined as\n\nboldsymbolw(boldsymbols boldsymbolu) equiv (mathbbI(h in B_k)  k = 1 dots K)\n\nwhere mathbbI(cdot) denotes the indicator function,  h equiv boldsymbols - boldsymbolu  is the spatial distance between boldsymbols and  boldsymbolu, and B_k  k = 1 dots K is a set of K =n_bins equally-sized distance bins covering the spatial distances between 0 and h_max. \n\nExamples\n\nusing NeuralEstimators \n\nh_max = 1\nn_bins = 10\nw = IndicatorWeights(h_max, n_bins)\nh = rand(1, 30) # distances between 30 pairs of spatial locations \nw(h)\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#NeuralEstimators.initialise_estimator","page":"Miscellaneous","title":"NeuralEstimators.initialise_estimator","text":"initialise_estimator(p::Integer; ...)\ninitialise_estimator(p::Integer, data_type::String; ...)\n\nInitialise a neural estimator for a statistical model with p unknown parameters.\n\nThe estimator is couched in the DeepSets framework (see DeepSet) so that it can be applied to data sets containing an arbitrary number of independent replicates (including the special case of a single replicate).\n\nNote also that the user is free to initialise their neural estimator however they see fit using arbitrary Flux code; see here for Flux's API reference.\n\nFinally, the method with positional argument data_typeis a wrapper that allows one to specify the type of their data (either \"unstructured\", \"gridded\", or \"irregular_spatial\").\n\nKeyword arguments\n\narchitecture::String: for unstructured multivariate data, one may use a densely-connected neural network (\"DNN\"); for data collected over a grid, a convolutional neural network (\"CNN\"); and for graphical or irregular spatial data, a graphical neural network (\"GNN\").\nd::Integer = 1: for unstructured multivariate data (i.e., when architecture = \"DNN\"), the dimension of the data (e.g., d = 3 for trivariate data); otherwise, if architecture ∈ [\"CNN\", \"GNN\"], the argument d controls the number of input channels (e.g., d = 1 for univariate spatial processes).\nestimator_type::String = \"point\": the type of estimator; either \"point\" or \"interval\".\ndepth = 3: the number of hidden layers; either a single integer or an integer vector of length two specifying the depth of the inner (summary) and outer (inference) network of the DeepSets framework.\nwidth = 32: a single integer or an integer vector of length sum(depth) specifying the width (or number of convolutional filters/channels) in each hidden layer.\nactivation::Function = relu: the (non-linear) activation function of each hidden layer.\nactivation_output::Function = identity: the activation function of the output layer.\nvariance_stabiliser::Union{Nothing, Function} = nothing: a function that will be applied directly to the input, usually to stabilise the variance.\nkernel_size = nothing: (applicable only to CNNs) a vector of length depth[1] containing integer tuples of length D, where D is the dimension of the convolution (e.g., D = 2 for two-dimensional convolution).\nweight_by_distance::Bool = false: (applicable only to GNNs) flag indicating whether the estimator will weight by spatial distance; if true, a SpatialGraphConv layer is used in the propagation module; otherwise, a regular GraphConv layer is used.\n\nExamples\n\n## DNN, GNN, 1D CNN, and 2D CNN for a statistical model with two parameters:\np = 2\ninitialise_estimator(p, architecture = \"DNN\")\ninitialise_estimator(p, architecture = \"GNN\")\ninitialise_estimator(p, architecture = \"CNN\", kernel_size = [10, 5, 3])\ninitialise_estimator(p, architecture = \"CNN\", kernel_size = [(10, 10), (5, 5), (3, 3)])\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Miscellaneous","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nReturns the weights of the neural network saved as 'best_network.bson' in the given path.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.removedata","page":"Miscellaneous","title":"NeuralEstimators.removedata","text":"removedata(Z::Array, Iᵤ::Vector{Integer})\nremovedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)\nremovedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)\n\nReplaces elements of Z with missing.\n\nThe simplest method accepts a vector of integers Iᵤ that give the specific indices of the data to be removed.\n\nAlterntivaly, there are two methods available to generate data that are missing completely at random (MCAR).\n\nFirst, a vector p may be given that specifies the proportion of missingness for each element in the response vector. Hence, p should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If prevent_complete_missing = true, no replicates will contain 100% missingness (note that this can slightly alter the effective values of p).\n\nSecond, if an integer n is provided, all replicates will contain n observations after the data are removed. If fixed_pattern = true, the missingness pattern is fixed for all replicates. If contiguous_pattern = true, the data will be removed in a contiguous block. If variable_proportion = true, the proportion of missingness will vary across replicates, with each replicate containing between 1 and n observations after data removal, sampled uniformly (note that variable_proportion overrides fixed_pattern).\n\nThe return type is Array{Union{T, Missing}}.\n\nExamples\n\nd = 5           # dimension of each replicate\nm = 2000        # number of replicates\nZ = rand(d, m)  # simulated data\n\n# Passing a desired proportion of missingness\np = rand(d)\nremovedata(Z, p)\n\n# Passing a desired final sample size\nn = 3  # number of observed elements of each replicate: must have n <= d\nremovedata(Z, n)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.rowwisenorm","page":"Miscellaneous","title":"NeuralEstimators.rowwisenorm","text":"rowwisenorm(A)\n\nComputes the row-wise norm of a matrix A.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.spatialgraph","page":"Miscellaneous","title":"NeuralEstimators.spatialgraph","text":"spatialgraph(S)\nspatialgraph(S, Z)\nspatialgraph(g::GNNGraph, Z)\n\nGiven spatial data Z measured at spatial locations S, constructs a GNNGraph ready for use in a graph neural network that employs SpatialGraphConv layers. \n\nWhen m independent replicates are collected over the same set of n spatial locations,\n\nboldsymbols_1 dots boldsymbols_n subset mathcalD\n\nwhere mathcalD subset mathbbR^d denotes the spatial domain of interest,  Z should be given as an n times m matrix and S should be given as an n times d matrix.  Otherwise, when m independent replicates are collected over differing sets of spatial locations,\n\nboldsymbols_ij dots boldsymbols_in_i subset mathcalD quad i = 1 dots m\n\nZ should be given as an m-vector of n_i-vectors, and S should be given as an m-vector of n_i times d matrices.\n\nThe spatial information between neighbours is stored as an edge feature, with the specific  information controlled by the keyword arguments stationary and isotropic.  Specifically, the edge feature between node  j and node j stores the spatial  distance boldsymbols_j - boldsymbols_j (if isotropic), the spatial  displacement boldsymbols_j - boldsymbols_j (if stationary), or the matrix of   locations (boldsymbols_j boldsymbols_j) (if !stationary).  \n\nAdditional keyword arguments inherit from adjacencymatrix() to determine the neighbourhood of each node, with the default being a randomly selected set of  k=30 neighbours within a disc of radius r=0.15 units.\n\nExamples\n\nusing NeuralEstimators\n\n# Number of replicates and spatial dimension\nm = 5  \nd = 2  \n\n# Spatial locations fixed for all replicates\nn = 100\nS = rand(n, d)\nZ = rand(n, m)\ng = spatialgraph(S, Z)\n\n# Spatial locations varying between replicates\nn = rand(50:100, m)\nS = rand.(n, d)\nZ = rand.(n)\ng = spatialgraph(S, Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Miscellaneous","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ∈ (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ∈ (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.vectotril","page":"Miscellaneous","title":"NeuralEstimators.vectotril","text":"vectotril(v; strict = false)\nvectotriu(v; strict = false)\n\nConverts a vector v of length d(d+1)2 (a triangular number) into a d  d lower or upper triangular matrix.\n\nIf strict = true, the matrix will be strictly lower or upper triangular, that is, a (d+1)  (d+1) triangular matrix with zero diagonal.\n\nNote that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if v is a GPU array. Note also that the return type is not of type Triangular matrix (i.e., the zeros are materialised) since Traingular matrices are not always compatible with other GPU operations.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nn = d*(d+1)÷2\nv = collect(range(1, n))\nvectotril(v)\nvectotriu(v)\nvectotril(v; strict = true)\nvectotriu(v; strict = true)\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"In addition to the standard loss functions provided by Flux (e.g., mae, mse, etc.), NeuralEstimators provides the following loss functions.","category":"page"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"tanhloss\n\nkpowerloss\n\nquantileloss\n\nintervalscore","category":"page"},{"location":"API/loss/#NeuralEstimators.tanhloss","page":"Loss functions","title":"NeuralEstimators.tanhloss","text":"tanhloss(θ̂, y, k; agg = mean, joint = true)\n\nFor k > 0, computes the loss function,\n\nL(θ θ) = tanh(θ - θk)\n\nwhich approximates the 0-1 loss as k → 0. Compared with the kpowerloss,  which may also be used as a continuous surrogate for the 0-1 loss, the gradient of the tanh loss is bounded as |θ̂ - θ| → 0, which can improve numerical stability during  training. \n\nIf joint = true, the L₁ norm is computed over each parameter vector, so that, with  k close to zero, the resulting Bayes estimator is the mode of the joint posterior distribution; otherwise, if joint = false, the Bayes estimator is the vector containing the modes of the marginal posterior distributions.\n\nSee also kpowerloss.\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.kpowerloss","page":"Loss functions","title":"NeuralEstimators.kpowerloss","text":"kpowerloss(θ̂, y, k; agg = mean, joint = true, safeorigin = true, ϵ = 0.1)\n\nFor k > 0, the k-th power absolute-distance loss function,\n\nL(θ θ) = θ - θᵏ\n\ncontains the squared-error, absolute-error, and 0-1 loss functions as special cases (the latter obtained in the limit as k → 0). It is Lipschitz continuous iff k = 1, convex iff k ≥ 1, and strictly convex iff k > 1: it is quasiconvex for all k > 0.\n\nIf joint = true, the L₁ norm is computed over each parameter vector, so that, with  k close to zero, the resulting Bayes estimator is the mode of the joint posterior distribution; otherwise, if joint = false, the Bayes estimator is the vector containing the modes of the marginal posterior distributions.\n\nIf safeorigin = true, the loss function is modified to avoid pathologies around the origin, so that the resulting loss function behaves similarly to the absolute-error loss in the ϵ-interval surrounding the origin.\n\nSee also tanhloss.\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.quantileloss","page":"Loss functions","title":"NeuralEstimators.quantileloss","text":"quantileloss(θ̂, θ, τ; agg = mean)\nquantileloss(θ̂, θ, τ::Vector; agg = mean)\n\nThe asymmetric quantile loss function,\n\n  L(θ θ τ) = (θ - θ)(𝕀(θ - θ  0) - τ)\n\nwhere τ ∈ (0, 1) is a probability level and 𝕀(⋅) is the indicator function.\n\nThe method that takes τ as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in θ̂ is assumed to be pr, where p is the number of parameters and r is the number probability levels in τ (i.e., the length of τ).\n\nExamples\n\np = 1\nK = 10\nθ = rand(p, K)\nθ̂ = rand(p, K)\nquantileloss(θ̂, θ, 0.1)\n\nθ̂ = rand(3p, K)\nquantileloss(θ̂, θ, [0.1, 0.5, 0.9])\n\np = 2\nθ = rand(p, K)\nθ̂ = rand(p, K)\nquantileloss(θ̂, θ, 0.1)\n\nθ̂ = rand(3p, K)\nquantileloss(θ̂, θ, [0.1, 0.5, 0.9])\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.intervalscore","page":"Loss functions","title":"NeuralEstimators.intervalscore","text":"intervalscore(l, u, θ, α; agg = mean)\nintervalscore(θ̂, θ, α; agg = mean)\nintervalscore(assessment::Assessment; average_over_parameters::Bool = false, average_over_sample_sizes::Bool = true)\n\nGiven an interval [l, u] with nominal coverage 100×(1-α)%  and true value θ, the interval score is defined by\n\nS(l u θ α) = (u - l) + 2α¹(l - θ)𝕀(θ  l) + 2α¹(θ - u)𝕀(θ  u)\n\nwhere α ∈ (0, 1) and 𝕀(⋅) is the indicator function.\n\nThe method that takes a single value θ̂ assumes that θ̂ is a matrix with 2p rows, where p is the number of parameters in the statistical model. Then, the first and second set of p rows will be used as l and u, respectively.\n\nFor further discussion, see Section 6 of Gneiting, T. and Raftery, A. E. (2007), \"Strictly proper scoring rules, prediction, and estimation\", Journal of the American statistical Association, 102, 359–378.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#Architectures","page":"Architectures","title":"Architectures","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"architectures.md\"]","category":"page"},{"location":"API/architectures/#Modules","page":"Architectures","title":"Modules","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DeepSet\n\nGNNSummary","category":"page"},{"location":"API/architectures/#NeuralEstimators.DeepSet","page":"Architectures","title":"NeuralEstimators.DeepSet","text":"DeepSet(ψ, ϕ, a = mean; S = nothing)\n\nThe DeepSets representation,\n\nθ(𝐙) = ϕ(𝐓(𝐙))\t \t 𝐓(𝐙) = 𝐚(ψ(𝐙ᵢ)  i = 1  m)\n\nwhere 𝐙 ≡ (𝐙₁', …, 𝐙ₘ')' are independent replicates from the statistical model, ψ and ϕ are neural networks, and a is a permutation-invariant aggregation function. Expert summary statistics can be incorporated as,\n\nθ(𝐙) = ϕ((𝐓(𝐙) 𝐒(𝐙)))\n\nwhere S is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are provided either as a Function that returns a Vector, or as a vector of functions. In the case that  ψ is set to nothing, only expert summary statistics will be used. \n\nThe aggregation function a can be any function that acts on an array and has a keyword argument dims that allows aggregation over a specific dimension of the array (e.g., sum, mean, maximum, minimum, logsumexp).\n\nDeepSet objects act on data of type Vector{A}, where each element of the vector is associated with one data set (i.e., one set of independent replicates from the statistical model), and where the type A depends on the form of the data and the chosen architecture for ψ. As a rule of thumb, when A is an array, the replicates are stored in the final dimension. For example, with gridded spatial data and ψ a CNN, A should be a 4-dimensional array, with the replicates stored in the 4ᵗʰ dimension. Note that in Flux, the final dimension is usually the \"batch\" dimension, but batching with DeepSet objects is done at the data set level (i.e., sets of replicates are batched together).\n\nData stored as Vector{Arrays} are first concatenated along the replicates dimension before being passed into the summary network ψ. This means that ψ is applied to a single large array rather than many small arrays, which can substantially improve computational efficiency.\n\nSet-level information, 𝐱, that is not a function of the data can be passed directly into the inference network ϕ in the following manner,\n\nθ(𝐙) = ϕ((𝐓(𝐙) 𝐱))\t \t \n\nor, in the case that expert summary statistics are also used,\n\nθ(𝐙) = ϕ((𝐓(𝐙) 𝐒(𝐙) 𝐱))\t \n\nThis is done by calling the DeepSet object on a Tuple{Vector{A}, Vector{Vector}}, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level information (i.e., one vector for each data set).\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Two dummy data sets containing 3 and 4 replicates\np = 5  # number of parameters in the statistical model\nn = 10 # dimension of each replicate\nZ = [rand32(n, m) for m ∈ (3, 4)]\n\n# Construct the deepset object\nS = samplesize\nqₛ = 1   # dimension of expert summary statistic\nqₜ = 16  # dimension of neural summary statistic\nw = 32  # width of hidden layers\nψ = Chain(Dense(n, w, relu), Dense(w, qₜ, relu))\nϕ = Chain(Dense(qₜ + qₛ, w, relu), Dense(w, p))\nθ̂ = DeepSet(ψ, ϕ; S = S)\n\n# Apply the deepset object\nθ̂(Z)\n\n# Data with set-level information\nqₓ = 2 # dimension of set-level vector\nϕ = Chain(Dense(qₜ + qₛ + qₓ, w, relu), Dense(w, p))\nθ̂ = DeepSet(ψ, ϕ; S = S)\nx = [rand32(qₓ) for _ ∈ eachindex(Z)]\nθ̂((Z, x))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.GNNSummary","page":"Architectures","title":"NeuralEstimators.GNNSummary","text":"GNNSummary(propagation, readout; globalfeatures = nothing)\n\nA graph neural network (GNN) module designed to serve as the summary network ψ in the DeepSet representation when the data are graphical (e.g., irregularly observed spatial data).\n\nThe propagation module transforms graphical input data into a set of hidden-feature graphs. The readout module aggregates these feature graphs into a single hidden feature vector of fixed length (i.e., a vector of summary statistics). The summary network is then defined as the composition of the propagation and readout modules.\n\nOptionally, one may also include a module that extracts features directly  from the graph, through the keyword argument globalfeatures. This module,  when applied to a GNNGraph, should return a matrix of features. \n\nThe data should be stored as a GNNGraph or Vector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain subgraphs corresponding to independent replicates.\n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\nusing Flux: batch\nusing Statistics: mean\n\n# Propagation module\nd = 1      # dimension of response variable\nnₕ = 32    # dimension of node feature vectors\npropagation = GNNChain(GraphConv(d => nₕ), GraphConv(nₕ => nₕ))\n\n# Readout module\nreadout = GlobalPool(mean)\nnᵣ = nₕ   # dimension of readout vector\n\n# Summary network\nψ = GNNSummary(propagation, readout)\n\n# Inference network\np = 3     # number of parameters in the statistical model\nw = 64    # width of hidden layer\nϕ = Chain(Dense(nᵣ, w, relu), Dense(w, p))\n\n# Construct the estimator\nθ̂ = DeepSet(ψ, ϕ)\n\n# Apply the estimator to a single graph, a single graph with subgraphs\n# (corresponding to independent replicates), and a vector of graphs\n# (corresponding to multiple data sets each with independent replicates)\ng₁ = rand_graph(11, 30, ndata=rand(d, 11))\ng₂ = rand_graph(13, 40, ndata=rand(d, 13))\ng₃ = batch([g₁, g₂])\nθ̂(g₁)\nθ̂(g₃)\nθ̂([g₁, g₂, g₃])\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#User-defined-summary-statistics","page":"Architectures","title":"User-defined summary statistics","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"summarystatistics.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"The following functions correspond to summary statistics that are often useful as user-defined summary statistics in DeepSet objects.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"samplesize\n\nsamplecorrelation\n\nsamplecovariance\n\nNeighbourhoodVariogram","category":"page"},{"location":"API/architectures/#NeuralEstimators.samplesize","page":"Architectures","title":"NeuralEstimators.samplesize","text":"samplesize(Z::AbstractArray)\n\nComputes the sample size of a set of independent realisations Z.\n\nNote that this function is a wrapper around numberreplicates, but this function returns the number of replicates as the eltype of Z, rather than as an integer.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecorrelation","page":"Architectures","title":"NeuralEstimators.samplecorrelation","text":"samplecorrelation(Z::AbstractArray)\n\nComputes the sample correlation matrix, R̂, and returns the vectorised strict lower triangle of R̂.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecorrelation(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecovariance","page":"Architectures","title":"NeuralEstimators.samplecovariance","text":"samplecovariance(Z::AbstractArray)\n\nComputes the sample covariance matrix, Σ̂, and returns the vectorised lower triangle of Σ̂.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecovariance(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.NeighbourhoodVariogram","page":"Architectures","title":"NeuralEstimators.NeighbourhoodVariogram","text":"NeighbourhoodVariogram(h_max, n_bins) \n(l::NeighbourhoodVariogram)(g::GNNGraph)\n\nComputes the empirical variogram, \n\nhatgamma(h pm delta) = frac12N(h pm delta) sum_(ij) in N(h pm delta) (Z_i - Z_j)^2\n\nwhere N(h pm delta) equiv left(ij)  boldsymbols_i - boldsymbols_j in (h-delta h+delta)right  is the set of pairs of locations separated by a distance within (h-delta h+delta), and cdot denotes set cardinality. \n\nThe distance bins are constructed to have constant width 2delta, chosen based on the maximum distance  h_max to be considered, and the specified number of bins n_bins. \n\nThe input type is a GNNGraph, and the empirical variogram is computed based on the corresponding graph structure.  Specifically, only locations that are considered neighbours will be used when computing the empirical variogram. \n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n  \n# Simulate Gaussian spatial data with exponential covariance function \nθ = 0.1                                 # true range parameter \nn = 250                                 # number of spatial locations \nS = rand(n, 2)                          # spatial locations \nD = pairwise(Euclidean(), S, dims = 1)  # distance matrix \nΣ = exp.(-D ./ θ)                       # covariance matrix \nL = cholesky(Symmetric(Σ)).L            # Cholesky factor \nm = 5                                   # number of independent replicates \nZ = L * randn(n, m)                     # simulated data \n\n# Construct the spatial graph \nr = 0.15                                # radius of neighbourhood set\ng = spatialgraph(S, Z, r = r)\n\n# Construct the variogram object wth 10 bins\nnv = NeighbourhoodVariogram(r, 10) \n\n# Compute the empirical variogram \nnv(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Layers","page":"Architectures","title":"Layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"SpatialGraphConv\n\nDensePositive","category":"page"},{"location":"API/architectures/#NeuralEstimators.SpatialGraphConv","page":"Architectures","title":"NeuralEstimators.SpatialGraphConv","text":"SpatialGraphConv(in => out, g=relu; bias=true, init=glorot_uniform, ...)\n\nImplements a spatial graph convolution, \n\n boldsymbolh^(l)_j =\n gBig(\n boldsymbolGamma_1^(l) boldsymbolh^(l-1)_j\n +\n boldsymbolGamma_2^(l) barboldsymbolh^(l)_j\n +\n boldsymbolgamma^(l)\n Big)\n quad\n barboldsymbolh^(l)_j = sum_j in mathcalN(j)boldsymbolw(boldsymbols_j boldsymbols_j boldsymbolbeta^(l)) odot f(boldsymbolh^(l-1)_j boldsymbolh^(l-1)_j)\n\nwhere boldsymbolh^(l)_j is the hidden feature vector at location boldsymbols_j at layer l, g(cdot) is a non-linear activation function applied elementwise, boldsymbolGamma_1^(l) and boldsymbolGamma_2^(l) are trainable parameter matrices, boldsymbolgamma^(l) is a trainable bias vector, mathcalN(j) denotes the indices of neighbours of boldsymbols_j, boldsymbolw(cdot cdot boldsymbolbeta^(l)) is a learnable weight function parameterised by boldsymbolbeta^(l), odot denotes elementwise multiplication, and f(cdot cdot) is a (possibly learnable) function. \n\nThe function f(cdot cdot) is by default taken to be f(boldsymbolh^(l-1)_j boldsymbolh^(l-1)_j) = a boldsymbolh^(l-1)_j - (1 - a) boldsymbolh^(l-1)_j^b for learnable parameters a  0 1 and b  0. One may alternatively employ a nonlearnable function, for example, f = (hᵢ, hⱼ) -> (hᵢ - hⱼ).^2. \n\nThe spatial information must be stored as an edge feature, as facilitated by spatialgraph(): \n\nWhen modelling isotropic processes (isotropic = true), boldsymbolw(cdot cdot) is made a function of spatial distance, so that boldsymbolw(boldsymbols_j boldsymbols_j) equiv boldsymbolw(boldsymbols_j - boldsymbols_j). \nWhen modelling stationary processes (isotropic = false and stationary = true), boldsymbolw(cdot cdot) is made a function of spatial displacement, so that boldsymbolw(boldsymbols_j boldsymbols_j) equiv boldsymbolw(boldsymbols_j - boldsymbols_j). \nWhen modelling nonstationary processes (stationary = false), boldsymbolw(cdot cdot) is made a function of the raw spatial locations. \n\nIn all cases, the output of boldsymbolw(cdot cdot) must be either a scalar; a vector of the same dimension as the feature vectors of the previous layer; or a vector of arbitrary dimension if the features vectors of the previous layer are scalars.  To promote identifiability, the weights are normalised to sum to one (row-wise) within each neighbourhood set.  By default, boldsymbolw(cdot cdot) is taken to be a multilayer perceptron with a single hidden layer,  although a custom choice for this function can be provided using the keyword argument w. \n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\ng = relu: Activation function.\nbias = true: Add learnable bias?\ninit = glorot_uniform: Initialiser for boldsymbolGamma_1^(l), boldsymbolGamma_2^(l), and boldsymbolgamma^(l). \nisotropic = true\nstationary = true\nf = nothing\nw = nothing \nw_width = 128: (Only applicable if w = nothing) The width of the hidden layer in the MLP used to model boldsymbolw(cdot cdot). \nw_out = in: (Only applicable if w = nothing) The output dimension of boldsymbolw(cdot cdot).  \nglob = false: If true`, a global features will be computed directly from the entire spatial graph.\n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\nusing Statistics: mean\n\n# Toy spatial data\nm = 5                  # number of replicates\nd = 2                  # spatial dimension\nn = 250                # number of spatial locations\nS = rand(n, d)         # spatial locations\nZ = rand(n, m)         # data\ng = spatialgraph(S, Z) # construct the graph\n\n# Construct and apply spatial graph convolution layer\nl = SpatialGraphConv(1 => 10, relu)\nl(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.DensePositive","page":"Architectures","title":"NeuralEstimators.DensePositive","text":"DensePositive(layer::Dense, g::Function)\nDensePositive(layer::Dense; g::Function = Flux.relu)\n\nWrapper around the standard Dense layer that ensures positive weights (biases are left unconstrained).\n\nThis layer can be useful for constucting (partially) monotonic neural networks (see, e.g., QuantileEstimatorContinuous).\n\nExamples\n\nusing NeuralEstimators, Flux\n\nlayer = DensePositive(Dense(5 => 2))\nx = rand32(5, 64)\nlayer(x)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Output-activation-functions","page":"Architectures","title":"Output activation functions","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"activationfunctions.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In addition to the standard activation functions provided by Flux, the following layers can be used at the end of an architecture, to act as output activation functions that ensure valid estimates for certain models. NB: Although we refer to the following objects as \"activation functions\", they should be treated as layers that are included in the final stage of a Flux Chain(). ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Compress\n\nCorrelationMatrix\n\nCovarianceMatrix","category":"page"},{"location":"API/architectures/#NeuralEstimators.Compress","page":"Architectures","title":"NeuralEstimators.Compress","text":"Compress(a, b, k = 1)\n\nLayer that compresses its input to be within the range a and b, where each element of a is less than the corresponding element of b.\n\nThe layer uses a logistic function,\n\nl(θ) = a + fracb - a1 + e^-kθ\n\nwhere the arguments a and b together combine to shift and scale the logistic function to the range (a, b), and the growth rate k controls the steepness of the curve.\n\nThe logistic function given here contains an additional parameter, θ₀, which is the input value corresponding to the functions midpoint. In Compress, we fix θ₀ = 0, since the output of a randomly initialised neural network is typically around zero.\n\nExamples\n\nusing NeuralEstimators, Flux\n\na = [25, 0.5, -pi/2]\nb = [500, 2.5, 0]\np = length(a)\nK = 100\nθ = randn(p, K)\nl = Compress(a, b)\nl(θ)\n\nn = 20\nθ̂ = Chain(Dense(n, p), l)\nZ = randn(n, K)\nθ̂(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CorrelationMatrix","page":"Architectures","title":"NeuralEstimators.CorrelationMatrix","text":"CorrelationMatrix(d)\n(object::CorrelationMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector 𝐯 ∈ ℝᵈ to the parameters of an unconstrained d×d correlation matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d×d correlation matrix.\n\nThe expected input is a Matrix with T(d-1) = (d-1)d÷2 rows, where T(d-1) is the (d-1)th triangular number (the number of free parameters in an unconstrained d×d correlation matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different correlation matrices).\n\nInternally, the layer constructs a valid Cholesky factor 𝐋 for a correlation matrix, and then extracts the strict lower triangle from the correlation matrix 𝐑 = 𝐋𝐋'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the correlation matrix\n\nbeginbmatrix\n1    R₁₂   R₁₃ \nR₂₁  1     R₂₃\nR₃₁  R₃₂  1\nendbmatrix\n\nthe rows of the matrix returned by a CorrelationMatrix layer are ordered as\n\nbeginbmatrix\nR₂₁ \nR₃₁ \nR₃₂ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied correlation matrices using vectotril and Symmetric.\n\nSee also CovarianceMatrix.\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra\nusing Flux\n\nd  = 4\nl  = CorrelationMatrix(d)\np  = (d-1)*d÷2\nθ  = randn(p, 100)\n\n# Returns a matrix of parameters, which can be converted to correlation matrices\nR = l(θ)\nR = map(eachcol(R)) do r\n\tR = Symmetric(cpu(vectotril(r, strict = true)), :L)\n\tR[diagind(R)] .= 1\n\tR\nend\n\n# Obtain the Cholesky factor directly\nL = l(θ, true)\nL = map(eachcol(L)) do x\n\t# Only the strict lower diagonal elements are returned\n\tL = LowerTriangular(cpu(vectotril(x, strict = true)))\n\n\t# Diagonal elements are determined under the constraint diag(L*L') = 𝟏\n\tL[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)\n\tL\nend\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CovarianceMatrix","page":"Architectures","title":"NeuralEstimators.CovarianceMatrix","text":"CovarianceMatrix(d)\n(object::CovarianceMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector 𝐯 ∈ ℝᵈ to the parameters of an unconstrained d×d covariance matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d×d covariance matrix.\n\nThe expected input is a Matrix with T(d) = d(d+1)÷2 rows, where T(d) is the dth triangular number (the number of free parameters in an unconstrained d×d covariance matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different covariance matrices).\n\nInternally, the layer constructs a valid Cholesky factor 𝐋 and then extracts the lower triangle from the positive-definite covariance matrix 𝚺 = 𝐋𝐋'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the covariance matrix\n\nbeginbmatrix\nΣ₁₁  Σ₁₂  Σ₁₃ \nΣ₂₁  Σ₂₂  Σ₂₃ \nΣ₃₁  Σ₃₂  Σ₃₃ \nendbmatrix\n\nthe rows of the matrix returned by a CovarianceMatrix are ordered as\n\nbeginbmatrix\nΣ₁₁ \nΣ₂₁ \nΣ₃₁ \nΣ₂₂ \nΣ₃₂ \nΣ₃₃ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied covariance matrices using vectotril and Symmetric.\n\nSee also CorrelationMatrix.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing LinearAlgebra\n\nd = 4\nl = CovarianceMatrix(d)\np = d*(d+1)÷2\nθ = randn(p, 50)\n\n# Returns a matrix of parameters, which can be converted to covariance matrices\nΣ = l(θ)\nΣ = [Symmetric(cpu(vectotril(x)), :L) for x ∈ eachcol(Σ)]\n\n# Obtain the Cholesky factor directly\nL = l(θ, true)\nL = [LowerTriangular(cpu(vectotril(x))) for x ∈ eachcol(L)]\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural Bayes estimators are neural networks that transform data into point summaries of the posterior distribution. They are likelihood free and, once constructed, substantially faster than classical methods. Uncertainty quantification with neural Bayes estimators is also straightforward through the bootstrap distribution, which is essentially available \"for free\" with a neural estimator, or by training a neural Bayes estimator to approximate a set of marginal posterior quantiles. A related class of methods uses neural networks to approximate the likelihood function, the likelihood-to-evidence ratio, and the full posterior distribution. ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The package NeuralEstimators facilitates the development of neural Bayes estimators and related neural inferential methods in a user-friendly manner. It caters for arbitrary models by having the user implicitly define their model via simulated data. This makes development particularly straightforward for models with existing implementations (possibly in other programming languages, e.g., R or python). A convenient interface for R users is available here.","category":"page"},{"location":"#Getting-started","page":"NeuralEstimators","title":"Getting started","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Install NeuralEstimators using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(url = \"https://github.com/msainsburydale/NeuralEstimators.jl\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the details of the Framework, see the Examples.","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use it in your research or other activities, please also use the following citation.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@article{,\n\tauthor = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Raphaël},\n\ttitle = {Likelihood-Free Parameter Estimation with Neural {B}ayes Estimators},\n\tjournal = {The American Statistician},\n\tyear = {2024},\n\tvolume = {78},\n\tpages = {1--14},\n\tdoi = {10.1080/00031305.2023.2249522},\n\turl = {https://doi.org/10.1080/00031305.2023.2249522}\n}","category":"page"},{"location":"#Papers-using-NeuralEstimators","page":"NeuralEstimators","title":"Papers using NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Likelihood-free parameter estimation with neural Bayes estimators [paper]\n","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Raphaël Huser (2024)","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural Bayes estimators for censored inference with peaks-over-threshold models [paper]\n","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Jordan Richards, Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Raphaël Huser (2023)","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural Bayes estimators for irregular spatial data using graph neural networks [paper]\n","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Matthew Sainsbury-Dale, Andrew Zammit-Mangion, Jordan Richards, Raphaël Huser (2023)","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Modern extreme value statistics for Utopian extremes [paper]\n","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Jordan Richards, Noura Alotaibi, Daniela Cisneros, Yan Gong, Matheus B. Guerrero, Paolo Redondo, Xuanjie Shao (2023)","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural Methods for Amortised Parameter Inference [paper]\n","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Andrew Zammit-Mangion, Matthew Sainsbury-Dale, Raphaël Huser (2024)","category":"page"}]
}
