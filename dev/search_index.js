var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for any model for which simulation is feasible by allowing users to define their model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code illustrates how a user could formulate code for their own model.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"See also Distributions.jl for a range of distributions implemented in Julia, and the package RCall for calling R functions within Julia. ","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussian\n\nsimulatepotts\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussian","page":"Model-specific functions","title":"NeuralEstimators.simulategaussian","text":"simulategaussian(L::AbstractMatrix, m = 1)\n\nSimulates m independent and identically distributed realisations from a mean-zero multivariate Gaussian random vector with associated lower Cholesky  factor L. \n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n\nn = 500\nœÅ = 0.6\nŒΩ = 1.0\nS = rand(n, 2)\nD = pairwise(Euclidean(), S, dims = 1)\nŒ£ = Symmetric(matern.(D, œÅ, ŒΩ))\nL = cholesky(Œ£).L\nsimulategaussian(L)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulatepotts","page":"Model-specific functions","title":"NeuralEstimators.simulatepotts","text":"simulatepotts(grid::Matrix{Int}, Œ≤)\nsimulatepotts(grid::Matrix{Union{Int, Nothing}}, Œ≤)\nsimulatepotts(nrows::Int, ncols::Int, num_states::Int, Œ≤)\n\nChequerboard Gibbs sampling from a spatial Potts model with parameter Œ≤>0 (see, e.g., Sainsbury-Dale et al., 2025, Sec. 3.3, and the references therein).\n\nApproximately independent simulations can be obtained by setting  nsims > 1 or num_iterations > burn. The degree to which the  resulting simulations can be considered independent depends on the  thinning factor (thin) and the burn-in (burn).\n\nKeyword arguments\n\nnsims = 1: number of approximately independent replicates. \nnum_iterations = 2000: number of MCMC iterations.\nburn = num_iterations: burn-in.\nthin = 10: thinning factor.\n\nExamples\n\nusing NeuralEstimators \n\n## Marginal simulation \nŒ≤ = 0.8\nsimulatepotts(10, 10, 5, Œ≤)\n\n## Marginal simulation: approximately independent samples \nsimulatepotts(10, 10, 5, Œ≤; nsims = 100, thin = 10)\n\n## Conditional simulation \nŒ≤ = 0.8\ncomplete_grid   = simulatepotts(50, 50, 2, Œ≤)        # simulate marginally from the Ising model \nincomplete_grid = removedata(complete_grid, 0.1)     # remove 10% of the pixels at random  \nimputed_grid    = simulatepotts(incomplete_grid, Œ≤)  # conditionally simulate over missing pixels\n\n## Multiple conditional simulations \nimputed_grids   = simulatepotts(incomplete_grid, Œ≤; num_iterations = 2000, burn = 1000, thin = 10)\n\n## Recreate Fig. 8.8 of Marin & Robert (2007) ‚ÄúBayesian Core‚Äù\nusing Plots \ngrids = [simulatepotts(100, 100, 2, Œ≤) for Œ≤ ‚àà 0.3:0.1:1.2]\nheatmaps = heatmap.(grids, legend = false, aspect_ratio=1)\nPlots.plot(heatmaps...)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::Matrix, m = 1; C = 3.5, Gumbel::Bool = false)\n\nSimulates m independent and identically distributed realisations from Schlather's (2002) max-stable model given the lower Cholesky factor L of the covariance matrix of the underlying Gaussian process. \n\nThe function uses the algorithm for approximate simulation given by Schlather (2002).\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are  returned as an nxm matrix.\n\nKeyword arguments\n\nC = 3.5: a tuning parameter that controls the accuracy of the algorithm. Small C favours computational efficiency, while large C favours accuracy. \nGumbel = true: flag indicating whether the data should be log-transformed from the unit Fr√©chet scale to the Gumbel scale.\n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n\nn = 500\nœÅ = 0.6\nŒΩ = 1.0\nS = rand(n, 2)\nD = pairwise(Euclidean(), S, dims = 1)\nŒ£ = Symmetric(matern.(D, œÅ, ŒΩ))\nL = cholesky(Œ£).L\nsimulateschlather(L)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Spatial-point-processes","page":"Model-specific functions","title":"Spatial point processes","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"maternclusterprocess","category":"page"},{"location":"API/simulation/#NeuralEstimators.maternclusterprocess","page":"Model-specific functions","title":"NeuralEstimators.maternclusterprocess","text":"maternclusterprocess(; Œª=10, Œº=10, r=0.1, xmin=0, xmax=1, ymin=0, ymax=1, unit_bounding_box=false)\n\nGenerates a realisation from a Mat√©rn cluster process (e.g., Baddeley et al., 2015, Ch. 12). \n\nThe process is defined by a parent homogenous Poisson point process with intensity Œª > 0, a mean number of daughter points Œº > 0, and a cluster radius r > 0. The simulation is performed over a rectangular window defined by [xmin, xmax] √ó [ymin, ymax].\n\nIf unit_bounding_box = true, the simulated points will be scaled so that the longest side of their bounding box is equal to one (this may change the simulation window). \n\nSee also the R package spatstat, which provides functions for simulating from a range of point processes and which can be interfaced from Julia using RCall.\n\nExamples\n\nusing NeuralEstimators\n\n# Simulate a realisation from a Mat√©rn cluster process\nS = maternclusterprocess()\n\n# Visualise realisation (requires UnicodePlots)\nusing UnicodePlots\nscatterplot(S[:, 1], S[:, 2])\n\n# Visualise realisations from the cluster process with varying parameters\nn = 250\nŒª = [10, 25, 50, 90]\nŒº = n ./ Œª\nplots = map(eachindex(Œª)) do i\n\tS = maternclusterprocess(Œª = Œª[i], Œº = Œº[i])\n\tscatterplot(S[:, 1], S[:, 2])\nend\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Covariance-functions","page":"Model-specific functions","title":"Covariance functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These covariance functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\npaciorek","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nGiven distance boldsymbolh (h), computes the Mat√©rn covariance function\n\nC(boldsymbolh) = sigma^2 frac2^1 - nuGamma(nu) left(fracboldsymbolhrhoright)^nu K_nu left(fracboldsymbolhrhoright)\n\nwhere œÅ is a range parameter, ŒΩ is a smoothness parameter, œÉ¬≤ is the marginal variance,  Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.paciorek","page":"Model-specific functions","title":"NeuralEstimators.paciorek","text":"paciorek(s, r, œâ‚ÇÅ, œâ‚ÇÇ, œÅ, Œ≤)\n\nGiven spatial locations s and r, computes the nonstationary covariance function \n\nC(boldsymbols boldsymbolr) = \nboldsymbolSigma(boldsymbols)^14\nboldsymbolSigma(boldsymbolr)^14\nleftfracboldsymbolSigma(boldsymbols) + boldsymbolSigma(boldsymbolr)2right^-12\nC^0big(sqrtQ(boldsymbols boldsymbolr)big) \n\nwhere C^0(h) = exp-(hrho)^32 for range parameter rho  0,  the matrix boldsymbolSigma(boldsymbols) = exp(betaboldsymbols - boldsymbolomega)boldsymbolI  is a kernel matrix (Paciorek and Schervish, 2006)  with scale parameter beta  0 and reference point boldsymbolomega equiv (omega_1 omega_2) in mathbbR^2, and \n\nQ(boldsymbols boldsymbolr) = \n(boldsymbols - boldsymbolr)\nleft(fracboldsymbolSigma(boldsymbols) + boldsymbolSigma(boldsymbolr)2right)^-1\n(boldsymbols - boldsymbolr)\n\nis the squared Mahalanobis distance between boldsymbols and boldsymbolr. \n\nNote that, in practical applications, the reference point boldsymbolomega is often taken to be an estimable parameter rather than fixed and known. \n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in various paper, we have developed the following functions for evaluating the density function for several popular distributions. We include these in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(Z::V, L::LT) where {V <: AbstractVector, LT <: LowerTriangular}\ngaussiandensity(Z::A, L::LT) where {A <: AbstractArray, LT <: LowerTriangular}\ngaussiandensity(Z::A, Œ£::M) where {A <: AbstractArray, M <: AbstractMatrix}\n\nEfficiently computes the density function for Z ~ ùëÅ(0, Œ£), namely,  \n\n2piboldsymbolSigma^-12 exp-frac12boldsymbolZ^top boldsymbolSigma^-1boldsymbolZ\n\nfor covariance matrix Œ£, and where L is lower Cholesky factor of Œ£.\n\nThe method gaussiandensity(Z::A, L::LT) assumes that the last dimension of Z contains independent and identically distributed replicates.\n\nIf logdensity = true (default), the log-density is returned.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà‚ÇÅ‚ÇÇ; logdensity = true)\n\nThe bivariate density function (see, e.g., Sainsbury-Dale et al., 2024, Sec. S6.2) for Schlather's (2002) max-stable model, where œà‚ÇÅ‚ÇÇ denotes the spatial correlation function evaluated at the locations of observations z‚ÇÅ and z‚ÇÇ.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We first load the required packages, the following of which are used throughout these examples:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux                                  # Julia's deep-learning library\nusing Distributions: InverseGamma, Uniform  # sampling from probability distributions\nusing AlgebraOfGraphics, CairoMakie         # visualisation","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages will be used in the examples with Gridded data and Irregular spatial data:  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using Distances                             # distance matrices \nusing Folds                                 # parallel simulation (start Julia with --threads=auto)\nusing LinearAlgebra                         # Cholesky factorisation","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages are used only in the example with Irregular spatial data: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using GraphNeuralNetworks                   # GNN architecture\nusing Statistics: mean                            ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, various GPU backends can be used (see the Flux documentation for details). For instance, to use an NVIDIA GPU in the following examples, simply the load the CUDA.jl package:  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using CUDA","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once a GPU package is loaded and an appropriate GPU is available, the functions in NeuralEstimators will automatically leverage the GPU to improve computational efficiency, while maintaining memory safety through the use of batched operations.","category":"page"},{"location":"workflow/examples/#Univariate-data","page":"Examples","title":"Univariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we develop a neural Bayes estimator  for boldsymboltheta equiv (mu sigma) from data Z_1 dots Z_m that are independent and identically distributed realisations from the distribution N(mu sigma^2). (See Estimators for a list of other classes of estimators available in the package.)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We begin by defining a function to sample parameters from the prior distribution. Assuming prior independence, we adopt the marginal priors mu sim N(0 1) and sigma sim IG(3 1):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tŒº = randn(K)\n\tœÉ = rand(InverseGamma(3, 1), K)\n\tŒ∏ = vcat(Œº', œÉ')\n\treturn Œ∏\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define the statistical model implicitly through data simulation. The simulated data are stored as a Vector{A}, where each element corresponds to one parameter vector. The type A reflects the multivariate structure of the data. In this example, each replicate Z_1 dots Z_m is univariate, so A is a Matrix with n = 1 row and m columns:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(Œ∏, m)\n    [œë[1] .+ œë[2] .* randn(1, m) for œë in eachcol(Œ∏)]\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design our neural network. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As we are constructing a neural Bayes estimator, the neural network is a mapping mathcalZtoTheta, and the dimensionality of the neural-network output is therefore d equiv textrmdim(Theta) = 2. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Since our data are replicated, we adopt the DeepSets framework, implemented via the type DeepSet. DeepSets consist of two neural networks: an inner network and an outer network. The inner network extracts summary statistics from the data, and its architecture depends on the multivariate structure of the data. For unstructured data (i.e., data without spatial or temporal correlation within each replicate), we use a multilayer perceptron (MLP). The input dimension matches the dimensionality of each data replicate, while the output dimension corresponds to the number of summary statistics appropriate for the model (a common choice is d). The outer network maps the learned summary statistics to the output space (here, the parameter space, Theta). The outer network is always an MLP. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Below is an example of a DeepSets architecture for neural Bayes estimation in this example. Note that many models have parameter constraints (e.g., variance and range parameters that must be strictly positive). These constraints can be incorporated in the final layer of the neural network by choosing appropriate activation functions for each parameter. Here, we enforce the constraint sigma  0 by applying the softplus activation function in the final layer of the outer network, ensuring that all parameter estimates are valid. For some additional ways to constrain parameter estimates, see Output layers. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"n = 1    # dimension of each data replicate (univariate)\nd = 2    # dimension of the parameter vector Œ∏\nw = 128  # width of each hidden layer \n\n# Final layer has output dimension d and enforces parameter constraints\nfinal_layer = Parallel(\n    vcat,\n    Dense(w, 1, identity),     # Œº ‚àà ‚Ñù\n    Dense(w, 1, softplus)      # œÉ > 0\n)\n\n# Inner and outer networks\nœà = Chain(Dense(n, w, relu), Dense(w, d, relu))    \nœï = Chain(Dense(d, w, relu), final_layer)          \n\n# Combine into a DeepSet\nnetwork = DeepSet(œà, œï)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We then initialise the neural Bayes estimator by wrapping the neural network in a PointEstimator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"estimator = PointEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator using train(), here using the default absolute-error loss. We'll train the estimator using 50 independent replicates per parameter configuration. Below, we pass our user-defined functions for sampling parameters and simulating data, but one may also pass parameter or data instances, which will be held fixed during training:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 50\nestimator = train(estimator, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"One may wish to save a trained estimator and load it in a later session: see Saving and loading neural estimators for details on how this can be done. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The function assess() can be used to assess the trained estimator. Parametric and non-parametric bootstrap estimates can be obtained via bootstrap(), with corresponding confidence intervals computed using interval(). Additionally, bootstrap-based uncertainty quantification can be included in the assessment stage through the keyword argument boot:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏_test = sample(1000)\nZ_test = simulate(Œ∏_test, m)\nassessment = assess(estimator, Œ∏_test, Z_test, boot = true)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting Assessment object contains the sampled parameters, the corresponding point estimates, and the corresponding lower and upper bounds of the bootstrap intervals. This object can be used to compute various diagnostics:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"bias(assessment)      # Œº = 0.002, œÉ = 0.017\nrmse(assessment)      # Œº = 0.086, œÉ = 0.078\nrisk(assessment)      # Œº = 0.055, œÉ = 0.056\nplot(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Univariate Gaussian example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As an alternative form of uncertainty quantification with neural Bayes estimators, one may approximate a set of marginal posterior quantiles by training a neural Bayes estimator under the quantile loss function, which allows one to generate approximate marginal posterior credible intervals. This is facilitated with IntervalEstimator which, by default, targets 95% central credible intervals:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"qÃÇ = IntervalEstimator(network)\nqÃÇ = train(qÃÇ, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting posterior credible-interval estimator can also be assessed using assess(). Often, these intervals have better coverage than bootstrap-based intervals.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once an estimator is deemed to be well calibrated, it may be applied to observed data (below, we use simulated data as a substitute for observed data):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ = sample(1)                       # true parameters\nZ = simulate(Œ∏, m)                  # \"observed\" data\nestimate(estimator, Z)              # point estimate\ninterval(bootstrap(estimator, Z))   # 95% non-parametric bootstrap intervals\ninterval(qÃÇ, Z)                      # 95% marginal posterior credible intervals","category":"page"},{"location":"workflow/examples/#Gridded-data","page":"Examples","title":"Gridded data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For data collected over a regular grid, neural estimators are typically based on a convolutional neural network (CNN; see, e.g., Dumoulin and Visin, 2016). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"When using CNNs with NeuralEstimators, each data set must be stored as a multi-dimensional array. The penultimate dimension stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes), while the final dimension stores independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10times15 grid, one would construct an array of dimension 10times15times2times50.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, here we develop a neural Bayes estimator for the (univariate) spatial Gaussian process model with exponential covariance function and unknown range parameter theta  0. The spatial domain is taken to be the unit square, and we adopt the prior theta sim U(0 05). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Simulation from Gaussian processes typically involves the computation of an expensive intermediate object, namely, the Cholesky factor of a covariance matrix. Storing intermediate objects can enable the fast simulation of new data sets when the parameters are held fixed. Hence, in this example, we define a custom type Parameters subtyping ParameterConfigurations for storing the matrix of parameters and the corresponding Cholesky factors: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters{T} <: ParameterConfigurations\n\tŒ∏::Matrix{T}\n\tL\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Further, we define two constructors for our custom type: one that accepts an integer K, and another that accepts a dtimes K matrix of parameters. The former constructor will be useful during the training stage for sampling from the prior distribution, while the latter constructor will be useful for parametric bootstrap (since this involves repeated simulation from the fitted model):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\t# Sample parameters from the prior \n\tŒ∏ = 0.5 * rand(1, K)\n\n\t# Pass to matrix constructor\n\tParameters(Œ∏)\nend\n\nfunction Parameters(Œ∏::Matrix)\n\t# Spatial locations, a 16x16 grid over the unit square\n\tpts = range(0, 1, length = 16)\n\tS = expandgrid(pts, pts)\n\n\t# Distance matrix, covariance matrices, and Cholesky factors\n\tD = pairwise(Euclidean(), S, dims = 1)\n\tK = size(Œ∏, 2)\n\tL = Folds.map(1:K) do k\n\t\tŒ£ = exp.(-D ./ Œ∏[k])\n\t\tcholesky(Symmetric(Œ£)).L\n\tend\n\n\tParameters(Œ∏, L)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define the model simulator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m = 1) \n\tZ = Folds.map(parameters.L) do L\n\t\tn = size(L, 1)\n\t\tz = L * randn(n, m)\n\t\tz = reshape(z, 16, 16, 1, m) # reshape to 16x16 images\n\t\tz\n\tend\n\tZ\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"A possible architecture is as follows. Note that deeper architectures that employ residual connections (see ResidualBlock) often lead to improved performance, and certain pooling layers (e.g., GlobalMeanPool) allow the neural network to accommodate grids of varying dimension; for further discussion and an illustration, see Sainsbury-Dale et al. (2025, Sec. S3, S4). ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Inner network \nœà = Chain(\n      Conv((3, 3), 1 => 32, relu),   # 3x3 convolutional filter, 1 input channel to 32 output channels\n      MaxPool((2, 2)),               # 2x2 max pooling for dimension reduction\n      Conv((3, 3), 32 => 64, relu),  # 3x3 convolutional filter, 32 input channels to 64 output channels\n      MaxPool((2, 2)),               # 2x2 max pooling for dimension reduction\n      Flux.flatten                   # flatten output to feed into a fully connected layer\n  )\n\n# Outer network \nœï = Chain(Dense(256, 64, relu), Dense(64, 1))\n\n# DeepSet object\nnetwork = DeepSet(œà, œï)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we initialise a point estimator and a posterior credible-interval estimator:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ = PointEstimator(network)\nqÃÇ = IntervalEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Now we train the estimators, here using fixed parameter instances to avoid repeated Cholesky factorisations (see Storing expensive intermediate objects for data simulation and On-the-fly and just-in-time simulation for further discussion):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"K = 10000  # number of training parameter vectors\nm = 1      # number of independent replicates in each data set\nŒ∏_train = sample(K)\nŒ∏_val = sample(K √∑ 10)\nŒ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulate, m = m)\nqÃÇ = train(qÃÇ, Œ∏_train, Œ∏_val, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the estimators have been trained, we assess them using empirical simulation-based methods:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏_test = sample(1000)\nZ_test = simulate(Œ∏_test)\nassessment = assess([Œ∏ÃÇ, qÃÇ], Œ∏_test, Z_test)\n\nbias(assessment)       # 0.005\nrmse(assessment)       # 0.032\ncoverage(assessment)   # 0.953\nplot(assessment)       ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Gridded spatial Gaussian process example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, we can apply our estimators to observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ = sample(1)                          # true parameter\nZ = simulate(Œ∏)                        # \"observed\" data\nestimate(Œ∏ÃÇ, Z)                         # point estimate\ninterval(qÃÇ, Z)                         # 95% marginal posterior credible intervals","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that missing data (e.g., due to cloud cover) can be accommodated using the missing-data methods implemented in the package.","category":"page"},{"location":"workflow/examples/#Irregular-spatial-data","page":"Examples","title":"Irregular spatial data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To cater for spatial data collected over arbitrary spatial locations, one may construct a neural estimator with a graph neural network (GNN; see Sainsbury-Dale, Zammit-Mangion, Richards, and Huser, 2025). The overall workflow remains as given in previous examples, with two key additional steps:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Sampling spatial configurations during the training phase, possibly using an appropriately chosen spatial point process; see, for example, maternclusterprocess().\nStoring the spatial data as a graph; see spatialgraph().","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For illustration, we again consider a spatial Gaussian process model with exponential covariance function, and we define a type for storing expensive intermediate objects needed for data simulation. In this example, these objects include Cholesky factors, and spatial graphs which store the adjacency matrices needed to perform graph convolution: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters <: ParameterConfigurations\n\tŒ∏::Matrix      # true parameters  \n\tL              # Cholesky factors\n\tg              # spatial graphs\n\tS              # spatial locations \nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Again, we define two constructors, which will be convenient for sampling parameters from the prior during training and assessment, and for parametric bootstrap sampling when making inferences from observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K::Integer)\n\t# Sample parameters from the prior \n\tŒ∏ = 0.5 * rand(1, K)\n\n\t# Sample spatial configurations from Matern cluster process on [0, 1]¬≤\n\tn = rand(200:300, K)\n\tŒª = rand(Uniform(10, 50), K)\n\tS = [maternclusterprocess(Œª = Œª[k], Œº = n[k]/Œª[k]) for k ‚àà 1:K]\n\n\t# Pass to constructor\n\tParameters(Œ∏, S)\nend\n\nfunction Parameters(Œ∏::Matrix, S)\n\t# Compute covariance matrices and Cholesky factors \n\tL = Folds.map(axes(Œ∏, 2)) do k\n\t\tD = pairwise(Euclidean(), S[k], dims = 1)\n\t\tŒ£ = Symmetric(exp.(-D ./ Œ∏[k]))\n\t\tcholesky(Œ£).L\n\tend\n\n\t# Construct spatial graphs\n\tg = spatialgraph.(S)\n\n\t# Store in Parameters object\n\tParameters(Œ∏, L, g, S)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define a function for simulating from the model given an object of type Parameters. The code below enables simulation of an arbitrary number of independent replicates m, and one may provide a single integer for m, or any object that can be sampled using rand(m, K) (e.g., an integer range or some distribution over the possible sample sizes):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m)\n\tK = size(parameters, 2)\n\tm = rand(m, K)\n\tmap(1:K) do k\n\t\tL = parameters.L[k]\n\t\tg = parameters.g[k]\n\t\tn = size(L, 1)\n\t\tZ = L * randn(n, m[k])      \n\t\tspatialgraph(g, Z)            \n\tend\nend\nsimulate(parameters::Parameters, m::Integer = 1) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we construct our GNN architecture. Here, we use an architecture tailored to isotropic spatial dependence models; for further details, see Sainsbury-Dale et al. (2025, Sec. 2.2). We also employ a sparse approximation of the empirical variogram as an expert summary statistic (Gerber and Nychka, 2021).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In this example our goal is to construct a point estimator, however any other kind of estimator (see Estimators) can be constructed by simply substituting the appropriate estimator class in the final line below:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Spatial weight functions: continuous surrogates for 0-1 basis functions \nh_max = 0.15 # maximum distance to consider \nq = 10       # output dimension of the spatial weights\nw = KernelWeights(h_max, q)\n\n# Propagation module\npropagation = GNNChain(\n\tSpatialGraphConv(1 => q, relu, w = w, w_out = q),\n\tSpatialGraphConv(q => q, relu, w = w, w_out = q)\n)\n\n# Readout module\nreadout = GlobalPool(mean)\n\n# Inner network\nœà = GNNSummary(propagation, readout)\n\n# Expert summary statistics, the empirical variogram\nS = NeighbourhoodVariogram(h_max, q)\n\n# Outer network\nœï = Chain(\n\tDense(2q => 128, relu), \n\tDense(128 => 128, relu), \n\tDense(128 => 1, identity)\n)\n\n# DeepSet object\nnetwork = DeepSet(œà, œï; S = S)\n\n# Point estimator\nestimator = PointEstimator(network)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the estimator. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 1\nK = 5000\nŒ∏_train = sample(K)\nŒ∏_val   = sample(K√∑5)\nestimator = train(estimator, Œ∏_train, Œ∏_val, simulate, m = m, epochs = 20)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that the computations in GNNs are performed in parallel, making them particularly well-suited for GPUs, which typically contain thousands of cores. If you have access to an NVIDIA GPU, you can utilise it by simply loading the Julia package CUDA. ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we assess our trained estimator: ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏_test = sample(1000)\nZ_test = simulate(Œ∏_test, m)\nassessment = assess(estimator, Œ∏_test, Z_test)\nbias(assessment)   \nrmse(assessment)    \nrisk(assessment)   \nplot(assessment)   ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Estimates from a graph neural network (GNN) based neural Bayes estimator)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, once the estimator has been assessed, it may be applied to observed data, with bootstrap-based uncertainty quantification facilitated by bootstrap and interval. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"parameters = sample(1)             # sample a parameter vector and spatial locations              \nŒ∏ = parameters.Œ∏                   # true parameters\nS = parameters.S                   # \"observed\" locations\nZ = simulate(parameters)           # \"observed\" data    \nŒ∏ÃÇ = estimate(estimator, Z)         # point estimate\nps = Parameters(Œ∏ÃÇ, S)              # construct Parameters object from point estimate\nbs = bootstrap(estimator, ps, simulate, m)  # parametric bootstrap estimates\ninterval(bs)                       # parametric bootstrap confidence interval              ","category":"page"},{"location":"methodology/#Methodology","page":"Methodology","title":"Methodology","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Here, we provide an overview of the amortised neural inferential methods supported by the package; for further details, see the review paper by Zammit-Mangion et al. (2025) and the references therein.","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Notation: We denote model parameters of interest by boldsymboltheta equiv (theta_1 dots theta_d) in Theta, where Theta subseteq mathbbR^d is the parameter space. We denote data by boldsymbolZ equiv (Z_1 dots Z_n) in mathcalZ, where mathcalZ subseteq mathbbR^n is the sample space. We denote neural-network parameters by boldsymbolgamma. For simplicity, we assume that all measures admit densities with respect to the Lebesgue measure. We use pi(cdot) to denote the prior density function of the parameters. The input argument to a generic density function p(cdot) serves to specify both the random variable associated with the density and its evaluation point.","category":"page"},{"location":"methodology/#Neural-Bayes-estimators","page":"Methodology","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The goal of parametric point estimation is to estimate boldsymboltheta from data boldsymbolZ using an estimator, hatboldsymboltheta  mathcalZtoTheta. Estimators can be constructed intuitively within a decision-theoretic framework based on average-risk optimality. Specifically, consider a loss function L Theta times Theta to 0 infty). Then the Bayes risk of the estimator hatboldsymboltheta(cdot) is  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"int_Theta int_mathcalZ  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ))p(boldsymbolZ mid boldsymboltheta) pi(boldsymboltheta) textrmd boldsymbolZ textrmdboldsymboltheta  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Any minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdot cdot) and pi(cdot). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Bayes estimators are functionals of the posterior distribution (e.g., the Bayes estimator under quadratic loss is the posterior mean), and are therefore often unavailable in closed form. A way forward is to assume a flexible parametric function for hatboldsymboltheta(cdot), and to optimise the parameters within that function in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are fast to evaluate. Let hatboldsymboltheta_boldsymbolgamma  mathcalZtoTheta denote a neural network parameterised by boldsymbolgamma. Then a Bayes estimator may be approximated by hatboldsymboltheta_boldsymbolgamma^*(cdot), where ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin frac1K sum_k = 1^K L(boldsymboltheta hatboldsymboltheta_boldsymbolgamma(boldsymbolZ^(k)))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"with boldsymboltheta^(k) sim pi(boldsymboltheta) and, independently for each k, boldsymbolZ^(k) sim p(boldsymbolZ mid  boldsymboltheta^(k)). The process of obtaining boldsymbolgamma^* is referred to as \"training the network\", and this can be performed efficiently using back-propagation and stochastic gradient descent. The trained neural network hatboldsymboltheta_boldsymbolgamma^*(cdot) approximately minimises the Bayes risk, and therefore it is called a neural Bayes estimator (Sainsbury-Dale at al., 2024). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Once trained, a neural Bayes estimator can be applied repeatedly to real data sets at a fraction of the computational cost of conventional inferential methods. It is therefore ideal to use a neural Bayes estimator in settings where inference needs to be made repeatedly; in this case, the initial training cost is said to be amortised over time. ","category":"page"},{"location":"methodology/#Uncertainty-quantification-with-neural-Bayes-estimators","page":"Methodology","title":"Uncertainty quantification with neural Bayes estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Uncertainty quantification with neural Bayes estimators often proceeds through the bootstrap distribution (e.g., Lenzi et al., 2023; Richards et al., 2024; Sainsbury-Dale et al., 2024). Bootstrap-based approaches are particularly attractive when nonparametric bootstrap is possible (e.g., when the data are independent replicates), or when simulation from the fitted model is fast, in which case parametric bootstrap is also computationally efficient. However, these conditions are not always met and, although bootstrap-based approaches are often considered to be fairly accurate and favourable to methods based on asymptotic normality, there are situations where bootstrap procedures are not reliable (see, e.g., Canty et al., 2006, pg. 6). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Alternatively, by leveraging ideas from (Bayesian) quantile regression, one may construct a neural Bayes estimator that approximates a set of marginal posterior quantiles (Fisher et al., 2023; Sainsbury-Dale et al., 2025), which can then be used to construct credible intervals for each parameter. Inference then remains fully amortised since, once the estimators are trained, both point estimates and credible intervals can be obtained with virtually zero computational cost. Specifically, posterior quantiles can be targeted by training a neural Bayes estimator under the loss function","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"L(boldsymboltheta hatboldsymboltheta tau) equiv sum_j=1^d (hattheta_j - theta_j)mathbbI(hattheta_j - theta_j) - tau quad 0  tau  1","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where mathbbI(cdot) denotes the indicator function, since the Bayes estimator under this loss function is the vector of marginal posterior tau-quantiles (Sainsbury-Dale et al., 2025, Sec. 2.2.4). ","category":"page"},{"location":"methodology/#Neural-posterior-estimators","page":"Methodology","title":"Neural posterior estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"We now describe amortised approximate posterior inference through the minimisation of an expected Kullback‚ÄìLeibler (KL) divergence. Throughout, we let q(boldsymboltheta boldsymbolkappa) denote a parametric approximation to the posterior distribution p(boldsymboltheta mid boldsymbolZ), where the approximate-distribution parameters boldsymbolkappa belong to a space mathcalK. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"We first consider the non-amortised case, where the optimal parameters boldsymbolkappa^* for a single data set boldsymbolZ are found by minimising the KL divergence between p(boldsymboltheta mid boldsymbolZ) and q(boldsymboltheta boldsymbolkappa): ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"  boldsymbolkappa^* \n  equiv undersetboldsymbolkappamathrmargmin textrmKLp(boldsymboltheta mid boldsymbolZ)    q(boldsymboltheta boldsymbolkappa)\n  = undersetboldsymbolkappamathrmargmin -int_Theta log q(boldsymboltheta boldsymbolkappa) p(boldsymboltheta mid boldsymbolZ) textrmdboldsymboltheta\n  ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The resulting approximate posterior q(boldsymboltheta boldsymbolkappa^*) targets the true posterior in the sense that the KL divergence is zero if and only if q(boldsymboltheta boldsymbolkappa^*) = p(boldsymboltheta mid boldsymbolZ) for all boldsymboltheta in Theta. However, solving this optimisation problem is often computationally demanding even for a single data set boldsymbolZ, and solving it for many different data sets can be computationally prohibitive. The optimisation problem can be amortised by treating the parameters boldsymbolkappa as a function boldsymbolkappa  mathcalZ to mathcalK, and then choosing the function boldsymbolkappa^*(cdot) that minimises an expected KL divergence: ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolkappa^*(cdot) equiv undersetboldsymbolkappa(cdot)mathrmargmin mathbbE_boldsymbolZtextrmKLp(boldsymboltheta mid boldsymbolZ)    q(boldsymboltheta boldsymbolkappa(boldsymbolZ))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"In practice, we approximate boldsymbolkappa^*(cdot) using a neural network boldsymbolkappa_boldsymbolgamma  mathcalZ to mathcalK parameterised by boldsymbolgamma, fit by minimising a Monte Carlo approximation of the expected KL divergence above: ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin -sum_k=1^K log q(boldsymboltheta^(k) boldsymbolkappa_boldsymbolgamma(boldsymbolZ^(k)))","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Once trained, the neural network boldsymbolkappa_boldsymbolgamma^*(cdot) may be used to estimate the optimal approximate-distribution parameters boldsymbolkappa^* given data boldsymbolZ at almost no computational cost. The neural network boldsymbolkappa_boldsymbolgamma^*(cdot), together with the corresponding approximate distribution q(cdot boldsymbolkappa), is collectively referred to as a neural posterior estimator. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"There are numerous options for the approximate distribution q(cdot boldsymbolkappa). For instance, q(cdotboldsymbolkappa) can be modelled as a Gaussian distribution (e.g., Chan et al., 2018; see GaussianDistribution), where the parameters boldsymbolkappa = (boldsymbolmu textrmvech(boldsymbolL)) consist of a d-dimensional mean parameter boldsymbolmu and the d(d+1)2 non-zero elements of the lower Cholesky factor boldsymbolL of a covariance matrix, and the half-vectorisation operator textrmvech(cdot) vectorises the lower triangle of its matrix argument. One may also consider Gaussian mixtures (e.g., Papamakarios & Murray, 2016) or trans-Gaussian distributions (e.g., Maceda et al., 2024). However, the most widely adopted approach is to model q(cdot boldsymbolkappa) using a normalising flow (e.g., Ardizzone et al., 2019; Radev et al., 2022), excellent reviews for which are given by Kobyzev et al. (2020) and Papamakarios (2021). A particularly popular class of normalising flow is the affine coupling flow (e.g., Dinh et al., 2016; Kingma & Dhariwal, 2018; Ardizzone et al., 2019). Since affine coupling flows are universal density approximators (Teshima et al., 2020), they serve as the default and recommended choice for approximate distributions in this package; for further details, see NormalisingFlow. ","category":"page"},{"location":"methodology/#Neural-ratio-estimators","page":"Methodology","title":"Neural ratio estimators","text":"","category":"section"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Finally, we describe amortised inference by approximation of the likelihood-to-evidence ratio, ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"r(boldsymbolZ boldsymboltheta) equiv p(boldsymbolZ mid boldsymboltheta)p(boldsymbolZ)","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where p(boldsymbolZ mid boldsymboltheta) is the likelihood and p(boldsymbolZ) is the marginal likelihood (also known as the model evidence). ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"The likelihood-to-evidence ratio is ubiquitous in statistical inference. For example, likelihood ratios of the form p(boldsymbolZmid boldsymboltheta_0)p(boldsymbolZmid boldsymboltheta_1)=r(boldsymbolZ boldsymboltheta_0)r(boldsymbolZ boldsymboltheta_1) are central to hypothesis testing and model comparison, and naturally appear in the transition probabilities of most standard MCMC algorithms used for Bayesian inference. Further, since the likelihood-to-evidence ratio is a prior-free quantity, its approximation facilitates Bayesian inference that require multiple fits of the same model under different prior distributions. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Unlike the methods discussed earlier, the likelihood-to-evidence ratio might not immediately seem like a quantity well-suited for approximation by neural networks, which are typically trained by minimising empirical risk functions. However, this ratio emerges naturally as a simple transformation of the optimal solution to a standard binary classification problem, derived through the minimisation of an average risk. Specifically, consider a binary classifier c(boldsymbolZ boldsymboltheta) that distinguishes dependent data-parameter pairs (boldsymbolZ boldsymboltheta) sim p(boldsymbolZ boldsymboltheta) with class labels Y=1 from independent data-parameter pairs (tildeboldsymbolZ tildeboldsymboltheta) sim p(boldsymbolZ)p(boldsymboltheta) with class labels Y=0, and where the classes are balanced. Then, the Bayes classifier under binary cross-entropy loss is defined as ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"beginaligned\nc^*(cdot cdot) \nequiv\nundersetc(cdot cdot)mathrmargmin sum_yin0 1 textrmPr(Y = y)  int_Thetaint_mathcalZL_textrmBCEy c(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta mid Y = y)textrmd boldsymbolZ textrmd boldsymboltheta\n=\nundersetc(cdot cdot)mathrmargmin - int_Thetaint_mathcalZBiglogc(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta)  + log1 - c(boldsymbolZ boldsymboltheta)p(boldsymbolZ)p(boldsymboltheta) Bigtextrmd boldsymbolZ textrmd boldsymboltheta\nendaligned","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"where L_textrmBCE(y c) equiv -ylog(c) - (1 - y) log(1 - c). It can be shown (e.g., Hermans et al., 2020, App. B)  that the Bayes classifier is given by ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"c^*(boldsymbolZ boldsymboltheta) = fracp(boldsymbolZ boldsymboltheta)p(boldsymbolZ boldsymboltheta) + p(boldsymboltheta)p(boldsymbolZ) quad boldsymbolZ in mathcalZ boldsymboltheta in Theta","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"and, hence,","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"r(boldsymbolZ boldsymboltheta) = fracc^*(boldsymbolZ boldsymboltheta)1 - c^*(boldsymbolZ boldsymboltheta) quad boldsymbolZ in mathcalZ boldsymboltheta in Theta","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"This connection links the likelihood-to-evidence ratio to the average-risk-optimal solution of a standard binary classification problem, and consequently provides a foundation for approximating the ratio using neural networks. Specifically, let c_boldsymbolgamma mathcalZ times Theta to (0 1) denote a neural network parametrised by boldsymbolgamma. Then the Bayes classifier may be approximated by c_boldsymbolgamma^*(cdot cdot), where ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":" boldsymbolgamma^* equiv undersetboldsymbolgammamathrmargmin -sum_k=1^K Biglogc_boldsymbolgamma(boldsymbolZ^(k) boldsymboltheta^(k)) +  log1 - c_boldsymbolgamma(boldsymbolZ^(sigma(k)) boldsymboltheta^(k)) Big","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"with boldsymboltheta^(k) sim p(boldsymboltheta) simulated from a proposal distribution p(boldsymboltheta) that does not necessarily coincide with the prior distribution, boldsymbolZ^(k) sim p(boldsymbolZ mid boldsymboltheta^(k)), and sigma(cdot) a random permutation of 1 dots K. Once the neural network is trained, r_boldsymbolgamma^*(boldsymbolZ boldsymboltheta) equiv c_boldsymbolgamma^*(boldsymbolZ boldsymboltheta)1 - c_boldsymbolgamma^*(boldsymbolZ boldsymboltheta)^-1, boldsymbolZ in mathcalZ boldsymboltheta in Theta, may be used to quickly approximate the likelihood-to-evidence ratio, and therefore it is called a neural ratio estimator. ","category":"page"},{"location":"methodology/","page":"Methodology","title":"Methodology","text":"Inference based on a neural ratio estimator may proceed in a frequentist setting via maximum likelihood and likelihood ratios (e.g., Walchessen et al., 2024), and in a Bayesian setting by facilitating the computation of transition probabilities in Hamiltonian Monte Carlo and MCMC algorithms (e.g., Hermans et al., 2020). Further, an approximate posterior distribution can be obtained via the identity p(boldsymboltheta mid boldsymbolZ) = pi(boldsymboltheta) r(boldsymboltheta boldsymbolZ), and sampled from using standard sampling techniques (e.g., Thomas et al., 2022).","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Saving-and-loading-neural-estimators","page":"Advanced usage","title":"Saving and loading neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In regards to saving and loading, neural estimators behave in the same manner as regular Flux models. Therefore, the examples and recommendations outlined in the Flux documentation also apply directly to neural estimators. For example, to save the model state of the neural estimator estimator, run:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using Flux\nusing BSON: @save, @load\nmodel_state = Flux.state(estimator)\n@save \"estimator.bson\" model_state","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, to load it in a new session, one may initialise a neural estimator with the same architecture used previously, and load the saved model state as follows:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"@load \"estimator.bson\" model_state\nFlux.loadmodel!(estimator, model_state)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"It is also straightforward to save the entire neural estimator, including its architecture (see here). However, the first approach outlined above is recommended for long-term storage.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For convenience, the function train() allows for the automatic saving of the model state during the training stage, via the argument savepath.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution may be stored in two ways. Most simply, they can be stored as a d times K matrix, where d is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. Alternatively, they can be stored in a user-defined subtype of ParameterConfigurations, whose only requirement is a field Œ∏ that stores the d times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, the training data set, mathcalZ_texttrain, can be simulated continuously during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Further, this technique allows for data to be simulated \"just-in-time\", in the sense that they can be simulated in small batches, used to train the neural estimator, and then removed from memory. This can substantially reduce pressure on memory resources, particularly when working with large data sets.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh the set vartheta_texttrain of parameter vectors used during training, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models. Hybrid approaches are also possible, whereby the parameters (and possibly the data) are held fixed for several epochs (i.e., several passes through the training set when performing stochastic gradient descent) before being refreshed.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with various methods of train().","category":"page"},{"location":"workflow/advancedusage/#Regularisation","page":"Advanced usage","title":"Regularisation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The term regularisation refers to a variety of techniques aimed to reduce overfitting when training a neural network, primarily by discouraging complex models.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A popular regularisation technique is known as dropout, implemented in Flux's Dropout layer. Dropout involves temporarily dropping (\"turning off\") a randomly selected set of neurons (along with their connections) at each iteration of the training stage, which results in a computationally-efficient form of model (neural-network) averaging (Srivastava et al., 2014).","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Another class of regularisation techniques involve modifying the loss function. For instance, L‚ÇÅ regularisation (sometimes called lasso regression) adds to the loss a penalty based on the absolute value of the neural-network parameters. Similarly, L‚ÇÇ regularisation (sometimes called ridge regression) adds to the loss a penalty based on the square of the neural-network parameters. Note that these penalty terms are not functions of the data or of the statistical-model parameters that we are trying to infer. These regularisation techniques can be implemented straightforwardly by providing a custom optimiser to train() that includes a SignDecay object for L‚ÇÅ regularisation, or a WeightDecay object for L‚ÇÇ regularisation. See the Flux documentation for further details. Note that, when the training data and parameters are simulated dynamically (i.e., \"on the fly\"; see On-the-fly and just-in-time simulation), overfitting is generally not a concern, making this form of regularisation unnecessary.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For illustration, the following code constructs a neural Bayes estimator using dropout and L‚ÇÅ regularisation with penalty coefficient lambda = 10^-4:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators, Flux\n\n# Data Z|Œ∏ ~ N(Œ∏, 1) with Œ∏ ~ N(0, 1)\nd = 1     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 5     # number of independent replicates in each data set\nsampler(K) = randn32(d, K)\nsimulator(Œ∏, m) = [Œº .+ randn32(n, m) for Œº ‚àà eachcol(Œ∏)]\nK = 3000  # number of training samples\nŒ∏_train = sampler(K)\nŒ∏_val   = sampler(K)\nZ_train = simulator(Œ∏_train, m)\nZ_val   = simulator(Œ∏_val, m)\n\n# Neural network with dropout layers\nw = 128\nœà = Chain(Dense(1, w, relu), Dropout(0.1), Dense(w, w, relu), Dropout(0.5))     \nœï = Chain(Dense(w, w, relu), Dropout(0.5), Dense(w, 1))           \nnetwork = DeepSet(œà, œï)\n\n# Initialise estimator\nestimator = PointEstimator(network)\n\n# Optimiser with L‚ÇÅ regularisation\noptimiser = Flux.setup(OptimiserChain(SignDecay(1e-4), Adam()), estimator)\n\n# Train the estimator\ntrain(estimator, Œ∏_train, Œ∏_val, Z_train, Z_val; optimiser = optimiser)","category":"page"},{"location":"workflow/advancedusage/#Expert-summary-statistics","page":"Advanced usage","title":"Expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Implicitly, neural estimators involve the learning of summary statistics. However, some summary statistics are available in closed form, simple to compute, and highly informative (e.g., sample quantiles, the empirical variogram). Often, explicitly incorporating these expert summary statistics in a neural estimator can simplify the optimisation problem, and lead to a better estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The fusion of learned and expert summary statistics is facilitated by our implementation of the DeepSet framework. Note that this implementation also allows the user to construct a neural estimator using only expert summary statistics, following, for example, Gerber and Nychka (2021) and Rai et al. (2024). Note also that the user may specify arbitrary expert summary statistics, however, for convenience several standard User-defined summary statistics are provided with the package, including a fast, sparse approximation of the empirical variogram.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For an example of incorporating expert summary statistics, see Irregular spatial data, where the empirical variogram is used alongside learned graph-neural-network-based summary statistics.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as boldsymbolZ^(m) equiv (boldsymbolZ_1 dots boldsymbolZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator). For example, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*)\n=\nbegincases\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_1)  m leq m_1\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where boldsymbolgamma^* equiv (boldsymbolgamma^*_tildem_1 dots boldsymbolgamma^*_tildem_l-1), and boldsymbolgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatboldsymboltheta(cdot boldsymbolgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice and is less computationally burdensome than it first appears when used in conjunction with the technique known as pre-training (see Sainsbury-Dale at al., 2024, Sec 2.3.3), which is facilitated with trainx(). ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise estimators are implemented using the type PiecewiseEstimator. ","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case the Bayes risk becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"sum_m in mathcalM\ntextrmPr(M=m)left(\nint_Theta int_mathcalZ^m  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ^(m)))p(boldsymbolZ^(m) mid boldsymboltheta)pi(boldsymboltheta) textrmdboldsymbolZ^(m) textrmd boldsymboltheta\nright)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data during the training phase.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The following pseudocode illustrates how one may modify a general data simulator to train under a range of sample sizes, with the distribution of M defined by passing any object that can be sampled using rand(m, K) (e.g., an integer range like 1:30, an integer-valued distribution from Distributions.jl):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Method that allows m to be an object that can be sampled from\nfunction simulate(parameters, m)\n\t# Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t# Generate K sample sizes from the prior distribution for M\n\tmÃÉ = rand(m, K)\n\n\t# Pseudocode for data simulation\n\tZ = [<simulate mÃÉ[k] realisations from the model> for k ‚àà 1:K]\n\n\treturn Z\nend\n\n# Method that allows an integer to be passed for m\nsimulate(parameters, m::Integer) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/advancedusage/#Missing-data","page":"Advanced usage","title":"Missing data","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Neural networks do not naturally handle missing data, and this property can preclude their use in a broad range of applications. Here, we describe two techniques that alleviate this challenge in the context of parameter point estimation: the masking approach and the expectation-maximisation (EM) approach. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As a running example, we consider a Gaussian process model where the data are collected over a regular grid, but where some elements of the grid are unobserved. This situation often arises in, for example, remote-sensing applications, where the presence of cloud cover prevents measurement in some places. Below, we load the packages needed in this example, and define some aspects of the model that will remain constant throughout (e.g., the prior, the spatial domain). We also define types and functions for sampling from the prior distribution and for simulating marginally from the data model.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators, Flux\nusing Distributions: Uniform\nusing Distances, LinearAlgebra\nusing Statistics: mean\n\n# Set the prior and define the number of parameters in the statistical model\nŒ† = (\n\tœÑ = Uniform(0, 1.0),\n\tœÅ = Uniform(0, 0.4)\n)\nd = length(Œ†)\n\n# Define the (gridded) spatial domain and compute the distance matrix\npoints = range(0, 1, 16)\nS = expandgrid(points, points)\nD = pairwise(Euclidean(), S, dims = 1)\n\n# Store model information for later use\nŒæ = (\n\tŒ† = Œ†,\n\tS = S,\n\tD = D\n)\n\n# Struct for storing parameters+Cholesky factors\nstruct Parameters <: ParameterConfigurations\n\tŒ∏\n\tL\nend\n\n# Constructor for above struct\nfunction Parameters(K::Integer, Œæ)\n\n\t# Sample parameters from the prior\n\tŒ† = Œæ.Œ†\n\tœÑ = rand(Œ†.œÑ, K)\n\tœÅ = rand(Œ†.œÅ, K)\n\tŒΩ = 1 # fixed smoothness\n\n\t# Compute Cholesky factors  \n\tL = maternchols(Œæ.D, œÅ, ŒΩ)\n\n\t# Concatenate into matrix\n\tŒ∏ = permutedims(hcat(œÑ, œÅ))\n\n\tParameters(Œ∏, L)\nend\n\n# Marginal simulation from the data model\nfunction simulate(parameters::Parameters, m::Integer)\n\n\tK = size(parameters, 2)\n\tœÑ = parameters.Œ∏[1, :]\n\tL = parameters.L\n\tn = isqrt(size(L, 1))\n\n\tZ = map(1:K) do k\n\t\tz = simulategaussian(L[:, :, k], m)\n\t\tz = z + œÑ[k] * randn(size(z)...)\n\t\tz = Float32.(z)\n\t\tz = reshape(z, n, n, 1, :)\n\t\tz\n\tend\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/#The-masking-approach","page":"Advanced usage","title":"The masking approach","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The first missing-data technique that we consider is the so-called masking approach of Wang et al. (2024); see also the discussion by Sainsbury-Dale et al. (2025, Sec. 2.2). The strategy involves completing the data by replacing missing values with zeros, and using auxiliary variables to encode the missingness pattern, which are also passed into the network.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Let boldsymbolZ denote the complete-data vector. Then, the masking approach considers inference based on boldsymbolW, a vector of indicator variables that encode the missingness pattern (with elements equal to one or zero if the corresponding element of boldsymbolZ is observed or missing, respectively), and","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymbolU equiv boldsymbolZ odot boldsymbolW","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where odot denotes elementwise multiplication and the product of a missing element and zero is defined to be zero. Irrespective of the missingness pattern, boldsymbolU and boldsymbolW have the same fixed dimensions and hence may be processed easily using a single neural network. A neural point estimator is then trained on realisations of boldsymbolU boldsymbolW which, by construction, do not contain any missing elements.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Since the missingness pattern boldsymbolW is now an input to the neural network, it must be incorporated during the training phase. When interest lies only in making inference from a single already-observed data set, boldsymbolW is fixed and known, and the Bayes risk remains unchanged. However, amortised inference, whereby one trains a single neural network that will be used to make inference with many data sets, requires a joint model for the data boldsymbolZ and the missingness pattern boldsymbolW, which is here defined as follows:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Marginal simulation from the data model and a MCAR missingness model\nfunction simulatemissing(parameters::Parameters, m::Integer)\n\n\tZ = simulate(parameters, m)   # complete data\n\n\tUW = map(Z) do z\n\t\tprop = rand()             # sample a missingness proportion\n\t\tz = removedata(z, prop)   # randomly remove a proportion of the data\n\t\tuw = encodedata(z)        # replace missing entries with zero and encode missingness pattern\n\t\tuw\n\tend\n\n\treturn UW\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that the helper functions removedata() and encodedata() facilitate the construction of augmented data sets boldsymbolU boldsymbolW.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Next, we construct and train a masked neural Bayes estimator using a CNN architecture. Here, the first convolutional layer takes two input channels, since we store the augmented data boldsymbolU in the first channel and the missingness pattern boldsymbolW in the second. We construct a point estimator, but the masking approach is applicable with any other kind of estimator (see Estimators):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Construct DeepSet object\nœà = Chain(\n\tConv((10, 10), 2 => 16,  relu),\n\tConv((5, 5),  16 => 32,  relu),\n\tConv((3, 3),  32 => 64, relu),\n\tFlux.flatten\n\t)\nœï = Chain(Dense(64, 256, relu), Dense(256, d, exp))\nnetwork = DeepSet(œà, œï)\n\n# Initialise point estimator\nŒ∏ÃÇ = PointEstimator(network)\n\n# Train the masked neural Bayes estimator\nŒ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulatemissing, m = 1, Œæ = Œæ, K = 1000, epochs = 10)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Once trained, we can apply our masked neural Bayes estimator to (incomplete) observed data. The data must be encoded in the same manner that was done during training. Below, we use simulated data as a surrogate for real data, with a missingness proportion of 0.25:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Œ∏ = Parameters(1, Œæ)     # true parameters\nZ = simulate(Œ∏, 1)[1]    # complete data\nZ = removedata(Z, 0.25)  # \"observed\" incomplete data (i.e., with missing values)\nUW = encodedata(Z)       # augmented data {U, W}\nŒ∏ÃÇ(UW)                    # point estimate","category":"page"},{"location":"workflow/advancedusage/#The-EM-approach","page":"Advanced usage","title":"The EM approach","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Let boldsymbolZ_1 and boldsymbolZ_2 denote the observed and unobserved (i.e., missing) data, respectively, and let boldsymbolZ equiv (boldsymbolZ_1 boldsymbolZ_2) denote the complete data. A classical approach to facilitating inference when data are missing is the expectation-maximisation (EM) algorithm. The neural EM algorithm (Sainsbury-Dale et al., 2025) is an approximate version of the conventional (Bayesian) Monte Carlo EM algorithm which, at the lth iteration, updates the parameter vector through","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymboltheta^(l) = undersetboldsymbolthetamathrmargmax sum_h = 1^H ell(boldsymboltheta  boldsymbolZ_1  boldsymbolZ_2^(lh)) + log pi_H(boldsymboltheta)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where realisations of the missing-data component, boldsymbolZ_2^(lh)  h = 1 dots H, are sampled from the probability distribution of boldsymbolZ_2 given boldsymbolZ_1 and boldsymboltheta^(l-1), and where pi_H(boldsymboltheta) propto pi(boldsymboltheta)^H is a concentrated version of the original prior density. Given the conditionally simulated data, the neural EM algorithm performs the above EM update using a neural network that returns the MAP estimate (i.e., the posterior mode) using (complete) conditionally simulated data. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"First, we construct a neural approximation of the MAP estimator. In this example, we will take H=50. When H is taken to be reasonably large, one may lean on the Bernstein-von Mises theorem to train the neural Bayes estimator under linear or quadratic loss; otherwise, one should train the estimator under a continuous relaxation of the 0‚Äì1 loss (e.g., the tanhloss() in the limit kappa to 0). This is done as follows:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Construct DeepSet object\nœà = Chain(\n\tConv((10, 10), 1 => 16,  relu),\n\tConv((5, 5),  16 => 32,  relu),\n\tConv((3, 3),  32 => 64, relu),\n\tFlux.flatten\n\t)\nœï = Chain(\n\tDense(64, 256, relu),\n\tDense(256, d, exp)\n\t)\nnetwork = DeepSet(œà, œï)\n\n# Initialise point estimator\nŒ∏ÃÇ = PointEstimator(network)\n\n# Train neural Bayes estimator\nH = 50\nŒ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulate, m = H, Œæ = Œæ, K = 1000, epochs = 10)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Next, we define a function for conditional simulation (see EM for details on the required format of this function):","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulateconditional(Z::M, Œ∏, Œæ; nsims::Integer = 1) where {M <: AbstractMatrix{Union{Missing, T}}} where T\n\n\t# Save the original dimensions\n\tdims = size(Z)\n\n\t# Convert to vector\n\tZ = vec(Z)\n\n\t# Compute the indices of the observed and missing data\n\tI‚ÇÅ = findall(z -> !ismissing(z), Z) # indices of observed data\n\tI‚ÇÇ = findall(z -> ismissing(z), Z)  # indices of missing data\n\tn‚ÇÅ = length(I‚ÇÅ)\n\tn‚ÇÇ = length(I‚ÇÇ)\n\n\t# Extract the observed data and drop Missing from the eltype of the container\n\tZ‚ÇÅ = Z[I‚ÇÅ]\n\tZ‚ÇÅ = [Z‚ÇÅ...]\n\n\t# Distance matrices needed for covariance matrices\n\tD   = Œæ.D # distance matrix for all locations in the grid\n\tD‚ÇÇ‚ÇÇ = D[I‚ÇÇ, I‚ÇÇ]\n\tD‚ÇÅ‚ÇÅ = D[I‚ÇÅ, I‚ÇÅ]\n\tD‚ÇÅ‚ÇÇ = D[I‚ÇÅ, I‚ÇÇ]\n\n\t# Extract the parameters from Œ∏\n\tœÑ = Œ∏[1]\n\tœÅ = Œ∏[2]\n\n\t# Compute covariance matrices\n\tŒΩ = 1 # fixed smoothness\n\tŒ£‚ÇÇ‚ÇÇ = matern.(UpperTriangular(D‚ÇÇ‚ÇÇ), œÅ, ŒΩ); Œ£‚ÇÇ‚ÇÇ[diagind(Œ£‚ÇÇ‚ÇÇ)] .+= œÑ^2\n\tŒ£‚ÇÅ‚ÇÅ = matern.(UpperTriangular(D‚ÇÅ‚ÇÅ), œÅ, ŒΩ); Œ£‚ÇÅ‚ÇÅ[diagind(Œ£‚ÇÅ‚ÇÅ)] .+= œÑ^2\n\tŒ£‚ÇÅ‚ÇÇ = matern.(D‚ÇÅ‚ÇÇ, œÅ, ŒΩ)\n\n\t# Compute the Cholesky factor of Œ£‚ÇÅ‚ÇÅ and solve the lower triangular system\n\tL‚ÇÅ‚ÇÅ = cholesky(Symmetric(Œ£‚ÇÅ‚ÇÅ)).L\n\tx = L‚ÇÅ‚ÇÅ \\ Œ£‚ÇÅ‚ÇÇ\n\n\t# Conditional covariance matrix, cov(Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏),  and its Cholesky factor\n\tŒ£ = Œ£‚ÇÇ‚ÇÇ - x'x\n\tL = cholesky(Symmetric(Œ£)).L\n\n\t# Conditonal mean, E(Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏)\n\ty = L‚ÇÅ‚ÇÅ \\ Z‚ÇÅ\n\tŒº = x'y\n\n\t# Simulate from the distribution Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏ ‚àº N(Œº, Œ£)\n\tz = randn(n‚ÇÇ, nsims)\n\tZ‚ÇÇ = Œº .+ L * z\n\n\t# Combine the observed and missing data to form the complete data\n\tZ = map(1:nsims) do l\n\t\tz = Vector{T}(undef, n‚ÇÅ + n‚ÇÇ)\n\t\tz[I‚ÇÅ] = Z‚ÇÅ\n\t\tz[I‚ÇÇ] = Z‚ÇÇ[:, l]\n\t\tz\n\tend\n\tZ = stackarrays(Z, merge = false)\n\n\t# Convert Z to an array with appropriate dimensions\n\tZ = reshape(Z, dims..., 1, nsims)\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Now we can use the neural EM algorithm to get parameter point estimates from data containing missing values. The algorithm is implemented with the type EM. Again, here we use simulated data as a surrogate for real data:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Œ∏ = Parameters(1, Œæ)            # true parameters\nZ = simulate(Œ∏, 1)[1][:, :]     # complete data\nZ = removedata(Z, 0.25)         # \"observed\" incomplete data (i.e., with missing values)\nŒ∏‚ÇÄ = mean.([Œ†...])              # initial estimate, the prior mean\n\nneuralem = EM(simulateconditional, Œ∏ÃÇ)\nneuralem(Z, Œ∏‚ÇÄ, Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)","category":"page"},{"location":"workflow/advancedusage/#Censored-data","page":"Advanced usage","title":"Censored data","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Neural estimators can be constructed to handle censored data as input, by exploiting The masking approach detailed above for the case of missing data. The key difference is that, unlike the data missingness pattern, the censoring pattern is assumed to be known a priori and must be user specified. For simplicity, we here describe methdology for left censored data (i.e., we observe only data that exceed some threshold), but extensions to right or interval censoring are possible. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Richards et al. (2024) discuss neural point estimation from censored data in the context of peaks-over-threshold extreme value models, whereby artifical censoring of data is imposed to reduce estimation bias in the presence of non-extreme marginal events. In peaks-over-threshold modelling, observed data are treated as censored if they exceed their corresponding marginal tau-quantile, for tau in (01) close to one. We present two approaches to censoring data: a General setting, where users specifiy their own deterministic \"censoring\", and Peaks-over-threshold censoring, where users supply a (censoring) quantile level tau that can be treated as random and features in the neural network architecture.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As a running example, we consider a bivariate random scale Gaussian mixture; see Engelke, Opitiz, and Wadsworth (2019) and Huser and Wadsworth (2018). We consider the task of estimating boldsymboltheta=(rhodelta), where rho in 01) is a correlation parameter and delta in 01 is a shape parameter. Data boldsymbolZ_1dotsboldsymbolZ_m are independent and identically distributed according to the random scale construction","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"boldsymbolZ_i = delta R_i + (1-delta)  boldsymbolX_i","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where R_i is a unit exponential random variable and boldsymbolX_i is a bivariate random vector with unit exponential margins and a Gaussian copula with correlation rho. ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below, we construct a neural point estimator for fully observed data generated from this model.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators, Flux, Folds\nusing LinearAlgebra: Symmetric, cholesky\nusing Distributions: Uniform, Normal\nusing CairoMakie \n\nm = 200     # number of independent replicates in each data set\n\nfunction sample(K)\n\t# Sample parameters from the prior \n\tœÅ = rand(Uniform(0.0, 0.99),1, K)\n\tŒ¥ = rand(Uniform(0.0, 1.0),1, K)\n\treturn vcat(œÅ, Œ¥)\nend\n\nfunction simulate(Œ∏, m) \n\n\tK = size(Œ∏, 2)\n\tZ = Folds.map(1:K) do k\n\t\tœÅ = Œ∏[1, k]\n\t\tŒ¥ = Œ∏[2, k]\n\t\tŒ£ = [1 œÅ; œÅ 1]\n\t\tL = cholesky(Symmetric(Œ£)).L\n\n\t\tX = L * randn(2, m) # Standard Gaussian margins\n\t\tX = cdf.(Normal(), X)  # Uniform margins\n\t\tX = - log.(1 .- X) # Unit exponential margins\n\n\t\tR = -log.(1 .- rand(Uniform(0.0, 1.0), 1, m)) # nit exponential margins\n\n\t\tz = Œ¥ .* R .+ (1 - Œ¥) .* X\n\n\t\tz\n\tend\n\tZ\nend\n\n\n# Hidden layer width\nw = 32  \n\nfinal_layer = Dense(w, 2, sigmoid)      # œÅ, Œ¥  ‚àà [0,1]\n\n# Inner and outer networks\nœà = Chain(Dense(2, w, relu), Dense(w, w, relu))    \nœï = Chain(Dense(w, w, relu), final_layer)          \n\n# Combine into a DeepSet\nnetwork = DeepSet(œà, œï)\n\nŒ∏ÃÇ = PointEstimator(network)\n\n# training: full simulation on-the-fly\nŒ∏ÃÇ = train(Œ∏ÃÇ, sample, simulate, m = m)\n\n\nŒ∏_test = sample(1000)\nZ_test = simulate(Œ∏_test, m)\nassessment = assess(Œ∏ÃÇ, Œ∏_test, Z_test, boot = false)     \nfigure = plot(assessment)\n\nsave(\"docs/src/assets/figures/censoring1.png\", figure, px_per_unit = 3, size = (600, 300))","category":"page"},{"location":"workflow/advancedusage/#General-setting","page":"Advanced usage","title":"General setting","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Let boldsymbolZ equiv (boldsymbolZ_1 dots boldsymbolZ_m) denote the complete-data vector. In the general censoring setting, a user must provide a function censorandaugment that constructs, from boldsymbolZ, augmented data boldsymbolA=(tildeboldsymbolZ boldsymbolW). Similarly to The masking approach, we here consider inference using a vector of indicator variables that encode the censoring pattern, denoted by boldsymbolW; here boldsymbolW has elements equal to one or zero if the corresponding element of boldsymbolZ is left censored or observed, respectively. However, unlike typical masking, we do not set censored values to missing; we instead construct tildeboldsymbolZ, which comprises the vector boldsymbolZ with censored values set to some pre-specified constant, contained within the vector boldsymbolzeta, such that","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"tildeboldsymbolZ equiv boldsymbolZ odot boldsymbolW + boldsymbolzeta odot ( boldsymbol1 - boldsymbolW)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where boldsymbol1 is a vector of ones (of equivalent dimension to boldsymbolW) and where odot denotes elementwise multiplication. Note that boldsymbolzeta and the censoring pattern can differ across replicates t=1dotsm, as well as the underlying model parameter values.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The augmented data boldsymbolA is an input to the neural network, and the inner network boldsymbolpsi should be designed as to account for its dimension. The way in which concatenation of tildeboldsymbolZ and boldsymbolW is performed may differ depending on the type of the first layer use in boldsymbolpsi: if using a Dense layer, one can concatenate tildeboldsymbolZ and boldsymbolW along the first dimension (as they are both matrices; i nthis case, with dimension (2m)); if using graph layers or Conv layers, tildeboldsymbolZ and boldsymbolW should be concatenated as if they were two separate channels in a graph/image (see encodedata()).","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that the function censorandaugment should be applied to data during both training and at evaluation time; any manipulation of data that is performed at train time should also be performed to data at test time!","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below, the function censorandaugment takes in a vector of censoring levels boldsymbolc of the same length as boldsymbolZ_i, and sets censored values to zeta_1=dots=zeta_m=zeta; in this way, the censoring mechanism and augmentation values, boldsymbolzeta, do not vary with the model parameter values or with the replicate index.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function censorandaugment(z; c,  Œ∂=0)\n    I = 1 * (z .<= c)\n    z = ifelse.(z .<= c,  Œ∂, z)\n    return vcat(z, I)\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Censoring is performed during training. To ensure this, we employ censorandaugment during simulation:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulatecensored(Œ∏, m; c, Œ∂) \n    \n\tZ = simulate(Œ∏, m)\n\tA = Folds.map(Z) do Z‚Çñ\n\n        # censor data and create augmented datasest\n        A =  mapslices(Z -> censorandaugment(Z, c = c, Œ∂ = Œ∂), Z‚Çñ, dims = 1)\n        A # augmented dataset\n\n\tend\n\tA\nend\n\n# We can now generate data, with values below 0.4 censored and set to Œ∂ = -1.\nK = 1000  # number of training samples\n\nŒ∏_train = sample(K)\nc = [0.4, 0.4]\nZ_train = simulatecensored(Œ∏_train, m;  c = c, Œ∂ = -1.0)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"To adapt the point estimator architecture to handle the augmented dataset, we change the dimension of the input from two to four.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"œà‚ÇÇ = Chain(Dense(4, w, relu), Dense(w, w, relu))   \nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, 2, sigmoid))          \n\n\n# Combine into a DeepSet\nŒ∏ÃÇ_cNBE = PointEstimator(DeepSet(œà‚ÇÇ, œï‚ÇÇ))\n\n\n# training: full simulation on-the-fly\nsimulator(Œ∏, m)  = simulatecensored(Œ∏, m; c = c, Œ∂ = -1.0) \nŒ∏ÃÇ_cNBE = train(Œ∏ÃÇ_cNBE, sample, simulator, m = m)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"We assess the estimator using the same test parameter values as for Œ∏ÃÇ above (the neural estimator designed for fully observed data). We should observe increased estimation variance, as the censoring removes information from the estmation procedure.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Z_test = simulator(Œ∏_test, m)\nassessment_censored = assess(Œ∏ÃÇ_cNBE, Œ∏_test, Z_test, boot = false)   \nfigure = plot(assessment_censored)\n\nsave(\"docs/src/assets/figures/censoring2.png\", figure, px_per_unit = 3, size = (600, 300))","category":"page"},{"location":"workflow/advancedusage/#Peaks-over-threshold-censoring","page":"Advanced usage","title":"Peaks-over-threshold censoring","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In a peak-over-threshold modelling setup, censoring of data is determined by a user-specified quantile level tau, typically taken to be close to one. During inference, artificial censoring is imposed: data that do not exceed their marginal tau-quantile are treated as left censored. For example, in the running example, we censor components of boldsymbolZ_i below the tau-quantile of the marginal distribution of the random scale mixture, i.e., F^-1(tau delta), where F(zdelta) is the (delta-dependent) marginal distribution function of boldsymbolZ_i; this has a closed form expression, see Huser and Wadsworth (2018).","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Peaks-over-threshold modelling, with tau fixed, can easilly be implemented by adapting the censorandaugment in General setting, i.e., by imposing that c is an evaluation of F(tau delta). However, Richards et al. (2024) show that one can amortise a point estimator with respect to the choice of tau, by treating tau as random and allowing it to feature as an input into the outer neural network, boldsymbolphi. Note that, in this setting, the estimator cannot be trained via simulation-on-the-fly, and a finite number of K data/parameter pairs must be sampled prior to training!","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"We also follow Richards et al. (2024) and consider inference for data on standardised margins; that is, we pre-standardise the data boldsymbolZ to have unit exponential margins, rather than delta-dependent margins. This can help to improve the numerical stability of estimator training, as well as increasing training efficiency.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Define prior for œÑ. Here we allow $\\tau$ to be Uniform(0.4,0.6).\n\nfunction priorœÑ(K)\n\n\tœÑ = rand(Uniform(0.4, 0.6), 1, K)\n\n    return œÑ\nend\n\n\nfunction simulatecensored(Œ∏, œÑ,  m;  Œ∂) \n\n    Z = simulate(Œ∏, m)\n\n\tK = size(Œ∏, 2)\n\tA = Folds.map(1:K) do k\n\n        Z‚Çñ = Z[k]\n        œÑ‚Çñ = œÑ[k]\n\n         #Transform to Uniform margins; see Huser and Wadsworth (2018)\n        if Œ¥ == 0.5 \n            ZÃÉ‚Çñ = 1 .- exp.(- 2 .* Z‚Çñ) .* (1 .+ 2 .* Z‚Çñ) \n        else \n            ZÃÉ‚Çñ = 1 .- (Œ¥ ./ (2 .* Œ¥ .- 1)) .* exp.(- Z‚Çñ ./ Œ¥) .+ ((1 .- Œ¥) ./ (2 * Œ¥ .- 1)) .* exp.( - Z‚Çñ ./ (1 - Œ¥)) \n        end\n\n        ZÃÉ‚Çñ = -  log.(1 .- ZÃÉ‚Çñ) # Unit exponential margins; H^-1() in Richards et al., 2024\n        \n        c = -log(1 - œÑ‚Çñ) # Evaluate censoring quantile\n\n        # censor data and create augmented datasest\n        A =  mapslices(Z -> censorandaugment(Z, c = c, Œ∂ = Œ∂), ZÃÉ‚Çñ, dims = 1)\n        A # augmented dataset\n\n\tend\n\n    A\n\nend\n\n# We then generate data used for training and validation.\n\nK = 3000  # number of training samples\nŒ∏_train  = sample(K)\nŒ∏_val    = sample(K)\nœÑ_train  = priorœÑ(K)\nœÑ_val    = priorœÑ(K)\n\nZ_train = simulatecensored(Œ∏_train, œÑ_train, m;  Œ∂ = -1.0)\nZ_val   = simulatecensored(Œ∏_val, œÑ_val, m;  Œ∂ = -1.0)\n","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As tau features as an input into the outer network of the DeepSet estimator, we must increase the dimension of the input to boldsymbolphi.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"# Inner and outer networks\nœà‚ÇÉ = Chain(Dense(4, w, relu), Dense(w, w, relu))    \nœï‚ÇÉ = Chain(Dense(w + 1, w, relu), Dense(w, 2, sigmoid))    \n\n# Combine into a DeepSet\nŒ∏ÃÇ_cNBE_œÑ = PointEstimator(DeepSet(œà‚ÇÉ, œï‚ÇÉ))\n\n\n# training; note we make use of set-level statistics\nŒ∏ÃÇ_cNBE_œÑ = train(Œ∏ÃÇ_cNBE_œÑ, Œ∏_train, Œ∏_val, (Z_train, œÑ_train), (Z_val, œÑ_val))\n\n\n# assessment with œÑ fixed to 0.5, but using the same test values as previously\nœÑ_test =  repeat([0.5], 1000)'\nZ_test = simulatecensored(Œ∏_test, œÑ_test, m;  Œ∂ = -1.0)\n\nassessment_tau = assess(Œ∏ÃÇ_cNBE_œÑ, Œ∏_test, (Z_test, œÑ_test), boot = false)    \nfigure = plot(assessment_tau)\n\nsave(\"docs/src/assets/figures/censoring3.png\", figure, px_per_unit = 3, size = (600, 300))\n\n\n# assessment with œÑ fixed to 0.4\nœÑ_test =  repeat([0.4], 1000)'\nZ_test = simulatecensored(Œ∏_test, œÑ_test, m;  Œ∂ = -1.0)\n                        \nassessment_tau = assess(Œ∏ÃÇ_cNBE_œÑ, Œ∏_test, (Z_test, œÑ_test), boot = false)    \nfigure = plot(assessment_tau)\n\nsave(\"docs/src/assets/figures/censoring4.png\", figure, px_per_unit = 3, size = (600, 300))\n","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Neural inferential methods have marked practical appeal, as their implementation is only loosely connected to the statistical or physical model being considered. The workflow when using the package NeuralEstimators is as follows:","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior, pi(boldsymboltheta), to form training/validation/test parameter sets. Alternatively, define a function to sample parameters dynamically during training. Parameters are stored as d times K matrices, with d the dimensionality of the parameter vector and K the number of parameter vectors in the given parameter set. \nSimulate data from the model conditional on the above parameter sets, to form training/validation/test data sets. Alternatively, define a function to simulate data dynamically during training. Data are stored as objects of type Vector{A}, where each element of the vector is associated with one parameter vector, and the subtype A depends on the multivariate structure of the data (e.g., a Matrix for unstructured multivariate data, a multidimensional Array for gridded data, or a GNNGraph for graphical or irregular spatial data).\nIf constructing a neural posterior estimator, choose an approximate posterior distribution q(boldsymboltheta boldsymbolkappa). \nDesign and initialise a suitable neural network. The architecture class (e.g., MLP, CNN, GNN) should align with the multivariate structure of the data (e.g., unstructured, grid, graph). The specific input and output spaces depend on the chosen inferential method: \nFor neural Bayes estimators, the neural network is a mapping mathcalZtoTheta, where mathcalZ denotes the sample space and Theta denotes the parameter space.\nFor neural posterior estimators, the neural network is a mapping mathcalZtomathcalK, where mathcalK denotes the space of the approximate-distribution parameters boldsymbolkappa. \nFor neural ratio estimators, the neural network is a mapping mathcalZtimesThetatomathbbR. \nAny Flux model can be used to construct the neural network. To integrate it into the workflow, one need only define a method that transforms K-dimensional objects of type Vector{A} into matrices with K columns, where the number of rows corresponds to the dimensionality of the output spaces listed above. The type DeepSet serves as a convenient wrapper for embedding standard neural networks (e.g., MLPs, CNNs, GNNs) in a framework for making inference with an arbitrary number of independent replicates, and it comes with pre-defined methods for handling the transformations from a K-dimensional vector of data to a matrix output. \nWrap the neural network (and possibly the approximate distribution) in a subtype of NeuralEstimator corresponding to the intended inferential method:\nFor neural Bayes estimators, use PointEstimator; \nFor neural posterior estimators, use PosteriorEstimator;\nFor neural ratio estimators, use RatioEstimator. \nTrain the NeuralEstimator using train() and the training set, monitoring performance and convergence using the validation set. For neural Bayes estimators, specify a loss function. \nAssess the NeuralEstimator using assess() and the test set. ","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Once the NeuralEstimator has passed our assessments and is deemed to be well calibrated, it may be used to make inference with observed data. ","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Next, see the Examples and, once familiar with the basic workflow, see Advanced usage for further practical considerations on how to most effectively construct neural estimators.","category":"page"},{"location":"API/core/#Core","page":"Core","title":"Core","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"This page documents the classes and functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these classes and functions appear in a standard implementation: from sampling parameters from the prior distribution, to making inference with observed data.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Parameters sampled from the prior distribution are stored as a d times K matrix, where d is the dimension of the parameter vector to make inference on and K is the number of sampled parameter vectors. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). The user-defined type should be a subtype of ParameterConfigurations, whose only requirement is a field Œ∏ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.\n\nThe user-defined type must have a field Œ∏ that stores the d √ó K matrix of parameters, where d is the dimension of the parameter vector to make  inference on and K is the number of sampled parameter vectors. There are no other requirements.\n\nSee subsetparameters() for the generic function for subsetting these objects. \n\nExamples\n\nstruct P <: ParameterConfigurations\n\tŒ∏\n\t# other expensive intermediate objects...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The package accommodates any model for which simulation is feasible by allowing users to define their model implicitly through simulated data.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The data are stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and the subtype A depends on the multivariate structure of the data. Common formats include:","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Unstructured data: A is typically an n times m matrix, where:\nn is the dimension of each replicate (e.g., n=1 for univariate data, n=2 for bivariate data).  \nm is the number of independent replicates in each data set (m is allowed to vary between data sets). \nData collected over a regular grid: A is typically an (N + 2)-dimensional array, where: \nThe first N dimensions correspond to the dimensions of the grid (e.g., N = 1 for time series, N = 2 for two-dimensional spatial grids). \nThe penultimate dimension stores the so-called \"channels\" (e.g., singleton for univariate processes, two for bivariate processes). \nThe final dimension stores the m independent replicates. \nSpatial data collected over irregular locations: A is typically a GNNGraph, where independent replicates (possibly with differing spatial locations) are stored as subgraphs. See the helper function spatialgraph() for constructing these graphs. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"While the formats above cover many applications, the package is flexible: the data structure simply needs to align with the chosen neural-network architecture. ","category":"page"},{"location":"API/core/#Estimators","page":"Core","title":"Estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The package provides several classes of neural estimators, organised within a type hierarchy. At the top-level of the hierarchy is NeuralEstimator, an abstract supertype for all neural estimators in the package. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Neural Bayes estimators are implemented as subtypes of the abstract supertype BayesEstimator. The simple type PointEstimator is used for constructing neural Bayes estimators under general, user-defined loss functions. Several specialised types cater for the estimation of posterior quantiles based on the quantile loss function: see IntervalEstimator and its generalisation QuantileEstimatorDiscrete for estimating posterior quantiles for a fixed set of probability levels; and see QuantileEstimatorContinuous for estimating posterior quantiles based on a continuous probability level provided as input to the neural network.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The type PosteriorEstimator can be used to approximate the posterior distribution, and RatioEstimator can be used to approximate the likelihood-to-evidence ratio.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Several types serve as wrappers around the aforementioned estimators, enhancing their functionality. PiecewiseEstimator applies different estimators based on the sample size of the data (see the discussion on Variable sample sizes). Ensemble combines multiple estimators, aggregating their estimates to improve accuracy.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimator\n\nBayesEstimator\n\nPointEstimator\n\nPosteriorEstimator\n\nRatioEstimator\n\nIntervalEstimator\n\nQuantileEstimatorDiscrete\n\nQuantileEstimatorContinuous\n\nPiecewiseEstimator\n\nEnsemble","category":"page"},{"location":"API/core/#NeuralEstimators.NeuralEstimator","page":"Core","title":"NeuralEstimators.NeuralEstimator","text":"NeuralEstimator\n\nAn abstract supertype for all neural estimators in NeuralEstimators.jl.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.BayesEstimator","page":"Core","title":"NeuralEstimators.BayesEstimator","text":"BayesEstimator <: NeuralEstimator\n\nAn abstract supertype for neural Bayes estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PointEstimator","page":"Core","title":"NeuralEstimators.PointEstimator","text":"PointEstimator <: BayesEstimator\nPointEstimator(network)\n(estimator::PointEstimator)(Z)\n\nA point estimator, where the neural network is a mapping from the sample space to the parameter space.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PosteriorEstimator","page":"Core","title":"NeuralEstimators.PosteriorEstimator","text":"PosteriorEstimator <: NeuralEstimator\nPosteriorEstimator(q::ApproximateDistribution, network)\nsampleposterior(estimator::PosteriorEstimator, Z, N::Integer)\nposteriormean(estimator::PosteriorEstimator, Z, N::Integer)\n\nA neural estimator that approximates the posterior distribution p(boldsymboltheta mid boldsymbolZ). \n\nThe neural network is a mapping from the sample space to a space that depends on the chosen approximate distribution q (see the available in-built Approximate distributions).  Often, the output space of the neural network is the space mathcalK of approximate-distribution parameters boldsymbolkappa.   However, for certain approximate distributions (notably, NormalisingFlow), the neural network should output summary statistics of some suitable dimension (e.g., the dimension d of the parameter vector). \n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ U(0, 1) and œÉ ~ U(0, 1)\nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(Œ∏, m) = [œë[1] .+ œë[2] .* randn32(n, m) for œë in eachcol(Œ∏)]\n\n# Distribution used to approximate the posterior \nq = NormalisingFlow(d, d) \n\n# Neural network (outputs d summary statistics)\nw = 128   \nœà = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, d))\nnetwork = DeepSet(œà, œï)\n\n## Alternatively, to use a Gaussian approximate distribution: \n# q = GaussianDistribution(d) \n# w = 128\n# œà = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))\n# œï = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, numdistributionalparams(q)))\n# network = DeepSet(œà, œï)\n\n# Initialise the estimator\nestimator = PosteriorEstimator(q, network)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with observed data \nŒ∏ = [0.8f0; 0.1f0]\nZ = simulate(Œ∏, m)\nsampleposterior(estimator, Z) # posterior draws \nposteriormean(estimator, Z)   # point estimate\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.RatioEstimator","page":"Core","title":"NeuralEstimators.RatioEstimator","text":"RatioEstimator <: NeuralEstimator\nRatioEstimator(network)\n(estimator::RatioEstimator)(Z, Œ∏)\nsampleposterior(estimator::RatioEstimator, Z, N::Integer)\nposteriormean(estimator::RatioEstimator, Z, N::Integer)\n\nA neural estimator that estimates the likelihood-to-evidence ratio,\n\nr(boldsymbolZ boldsymboltheta) equiv p(boldsymbolZ mid boldsymboltheta)p(boldsymbolZ)\n\nwhere p(boldsymbolZ mid boldsymboltheta) is the likelihood and p(boldsymbolZ) is the marginal likelihood, also known as the model evidence.\n\nFor numerical stability, training is done on the log-scale using the relation  log r(boldsymbolZ boldsymboltheta) = textlogit(c^*(boldsymbolZ boldsymboltheta)),  where c^*(cdot cdot) denotes the Bayes classifier as described in the Methodology section.  Hence, the neural network should be a mapping from mathcalZ times Theta to mathbbR, where mathcalZ and Theta denote the sample and parameter spaces, respectively. \n\nWhen the neural network is a DeepSet, two requirements must be met. First, the number of input neurons in the first layer of the outer network must equal d plus the number of output neurons in the final layer of the inner network.  Second, the number of output neurons in the final layer of the outer network must be one.\n\nWhen applying the estimator to data Z, by default the likelihood-to-evidence ratio r(boldsymbolZ boldsymboltheta) is returned (setting the keyword argument classifier = true will yield class probability estimates). The estimated ratio can then be used in various Bayesian (e.g., Hermans et al., 2020) or frequentist (e.g., Walchessen et al., 2024) inferential algorithms.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ U(0, 1) and œÉ ~ U(0, 1)\nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(Œ∏, m) = [œë[1] .+ œë[2] .* randn32(n, m) for œë in eachcol(Œ∏)]\n\n# Neural network\nw = 128 \nœà = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w + d, w, relu), Dense(w, w, relu), Dense(w, 1))\nnetwork = DeepSet(œà, œï)\n\n# Initialise the estimator\nrÃÇ = RatioEstimator(network)\n\n# Train the estimator\nrÃÇ = train(rÃÇ, sample, simulate, m = m)\n\n# Inference with \"observed\" data (grid-based optimisation and sampling)\nŒ∏ = sample(1)\nz = simulate(Œ∏, m)[1]\nŒ∏_grid = f32(expandgrid(0:0.01:1, 0:0.01:1))'  # fine gridding of the parameter space\nrÃÇ(z, Œ∏_grid)                                   # likelihood-to-evidence ratios over grid\nmlestimate(rÃÇ, z; Œ∏_grid = Œ∏_grid)              # maximum-likelihood estimate\nposteriormode(rÃÇ, z; Œ∏_grid = Œ∏_grid)           # posterior mode \nsampleposterior(rÃÇ, z; Œ∏_grid = Œ∏_grid)         # posterior samples\n\n# Inference with \"observed\" data (gradient-based optimisation using Optim.jl)\nusing Optim\nŒ∏‚ÇÄ = [0.5, 0.5]                                # initial estimate\nmlestimate(rÃÇ, z; Œ∏‚ÇÄ = Œ∏‚ÇÄ)                      # maximum-likelihood estimate\nposteriormode(rÃÇ, z; Œ∏‚ÇÄ = Œ∏‚ÇÄ)                   # posterior mode \n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.IntervalEstimator","page":"Core","title":"NeuralEstimators.IntervalEstimator","text":"IntervalEstimator <: BayesEstimator\nIntervalEstimator(u, v = u, c::Union{Function, Compress} = identity; probs = [0.025, 0.975], g = exp)\nIntervalEstimator(u, c::Union{Function, Compress}; probs = [0.025, 0.975], g = exp)\n(estimator::IntervalEstimator)(Z)\n\nA neural estimator that jointly estimates marginal posterior credible intervals based on the probability levels probs (by default, 95% central credible intervals).\n\nThe estimator employs a representation that prevents quantile crossing. Specifically, given data boldsymbolZ,  it constructs intervals for each parameter theta_i, i = 1 dots d  of the form,\n\nc_i(u_i(boldsymbolZ))  c_i(u_i(boldsymbolZ)) + g(v_i(boldsymbolZ)))\n\nwhere  boldsymbolu() equiv (u_1(cdot) dots u_d(cdot)) and boldsymbolv() equiv (v_1(cdot) dots v_d(cdot)) are neural networks that map from the sample space to mathbbR^d; g(cdot) is a monotonically increasing function (e.g., exponential or softplus); and each c_i() is a monotonically increasing function that maps its input to the prior support of theta_i.\n\nThe functions c_i() may be collectively defined by a d-dimensional object of type Compress. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted.  If only a single neural-network architecture is provided, it will be used for both boldsymbolu() and boldsymbolv().\n\nThe return value when applied to data using estimate() is a matrix with 2d rows, where the first and second d rows correspond to the lower and upper bounds, respectively. The function interval() can be used to format this output in a readable d √ó 2 matrix.  \n\nSee also QuantileEstimatorDiscrete and QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ U(0, 1) and œÉ ~ U(0, 1)\nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 100   # number of independent replicates\nsample(K) = rand32(d, K)\nsimulate(Œ∏, m) = [œë[1] .+ œë[2] .* randn(n, m) for œë in eachcol(Œ∏)]\n\n# Neural network\nw = 128   # width of each hidden layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, d))\nu = DeepSet(œà, œï)\n\n# Initialise the estimator\nestimator = IntervalEstimator(u)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with \"observed\" data \nŒ∏ = [0.8f0; 0.1f0]\nZ = simulate(Œ∏, m)\nestimate(estimator, Z) \ninterval(estimator, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorDiscrete","page":"Core","title":"NeuralEstimators.QuantileEstimatorDiscrete","text":"QuantileEstimatorDiscrete <: BayesEstimator\nQuantileEstimatorDiscrete(v; probs = [0.025, 0.5, 0.975], g = Flux.softplus, i = nothing)\n(estimator::QuantileEstimatorDiscrete)(Z)\n(estimator::QuantileEstimatorDiscrete)(Z, Œ∏‚Çã·µ¢)\n\nA neural estimator that jointly estimates a fixed set of marginal posterior quantiles, with probability levels tau_1 dots tau_T controlled by the keyword argument probs. This generalises IntervalEstimator to support an arbitrary number of probability levels. \n\nGiven data boldsymbolZ, by default the estimator approximates quantiles of the distributions of \n\ntheta_i mid boldsymbolZ quad i = 1 dots d \n\nfor parameters boldsymboltheta equiv (theta_1 dots theta_d). Alternatively, if initialised with i set to a positive integer, the estimator approximates quantiles of the full conditional distribution of  \n\ntheta_i mid boldsymbolZ boldsymboltheta_-i\n\nwhere boldsymboltheta_-i denotes the parameter vector with its ith element removed. \n\nThe estimator employs a representation that prevents quantile crossing, namely,\n\nbeginaligned\nboldsymbolq^(tau_1)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ)\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_1)(boldsymbolZ) + sum_j=2^t g(boldsymbolv^(tau_j)(boldsymbolZ)) quad t = 2 dots T\nendaligned\n\nwhere boldsymbolq^(tau)(boldsymbolZ) denotes the vector of tau-quantiles  for parameters boldsymboltheta equiv (theta_1 dots theta_d);  boldsymbolv^(tau_t)(cdot), t = 1 dots T, are neural networks that map from the sample space to mathbbR^d; and g(cdot) is a monotonically increasing function (e.g., exponential or softplus) applied elementwise to its arguments. If g = nothing, the quantiles are estimated independently through the representation\n\nboldsymbolq^(tau_t)(boldsymbolZ) = boldsymbolv^(tau_t)(boldsymbolZ) quad t = 1 dots T\n\nWhen the neural networks are DeepSet objects, two requirements must be met.  First, the number of input neurons in the first layer of the outer network must equal the number of neurons in the final layer of the inner network plus textdim(boldsymboltheta_-i), where we define  textdim(boldsymboltheta_-i) equiv 0 when targetting marginal posteriors of the form theta_i mid boldsymbolZ (the default behaviour).  Second, the number of output neurons in the final layer of the outer network must equal d - textdim(boldsymboltheta_-i). \n\nThe return value is a matrix with d - textdim(boldsymboltheta_-i) times T rows, where the first T rows correspond to the estimated quantiles for the first parameter, the second T rows corresponds to the estimated quantiles for the second parameter, and so on.\n\nSee also QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ U(0, 1) and œÉ ~ U(0, 1)\nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulate(Œ∏, m) = [œë[1] .+ œë[2] .* randn32(n, m) for œë in eachcol(Œ∏)]\n\n# ---- Quantiles of Œ∏·µ¢ ‚à£ ùêô, i = 1, ‚Ä¶, d ----\n\n# Neural network\nw = 64   # width of each hidden layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, d))\nv = DeepSet(œà, œï)\n\n# Initialise the estimator\nestimator = QuantileEstimatorDiscrete(v)\n\n# Train the estimator\nestimator = train(estimator, sample, simulate, m = m)\n\n# Inference with \"observed\" data \nŒ∏ = [0.8f0; 0.1f0]\nZ = simulate(Œ∏, m)\nestimate(estimator, Z) \n\n# ---- Quantiles of Œ∏·µ¢ ‚à£ ùêô, Œ∏‚Çã·µ¢ ----\n\n# Neural network\nw = 64  # width of each hidden layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w + 1, w, relu), Dense(w, d - 1))\nv = DeepSet(œà, œï)\n\n# Initialise estimators respectively targetting quantiles of Œº‚à£Z,œÉ and œÉ‚à£Z,Œº\nq‚ÇÅ = QuantileEstimatorDiscrete(v; i = 1)\nq‚ÇÇ = QuantileEstimatorDiscrete(v; i = 2)\n\n# Train the estimators\nq‚ÇÅ = train(q‚ÇÅ, sample, simulate, m = m)\nq‚ÇÇ = train(q‚ÇÇ, sample, simulate, m = m)\n\n# Estimate quantiles of Œº‚à£Z,œÉ with œÉ = 0.5 and for many data sets\nŒ∏‚Çã·µ¢ = 0.5f0\nq‚ÇÅ(Z, Œ∏‚Çã·µ¢)\n\n# Estimate quantiles of Œº‚à£Z,œÉ with œÉ = 0.5 for a single data set\nq‚ÇÅ(Z[1], Œ∏‚Çã·µ¢)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorContinuous","page":"Core","title":"NeuralEstimators.QuantileEstimatorContinuous","text":"QuantileEstimatorContinuous <: BayesEstimator\nQuantileEstimatorContinuous(network; i = nothing, num_training_probs::Integer = 1)\n(estimator::QuantileEstimatorContinuous)(Z, œÑ)\n(estimator::QuantileEstimatorContinuous)(Z, Œ∏‚Çã·µ¢, œÑ)\n\nA neural estimator that estimates marginal posterior quantiles, with the probability level œÑ given as input to the neural network.\n\nGiven data boldsymbolZ and the desired probability level  tau  (0 1), by default the estimator approximates the tau-quantile of the distributions of \n\ntheta_i mid boldsymbolZ quad i = 1 dots d \n\nfor parameters boldsymboltheta equiv (theta_1 dots theta_d). Alternatively, if initialised with i set to a positive integer, the estimator approximates the tau-quantile of the full conditional distribution of \n\ntheta_i mid boldsymbolZ boldsymboltheta_-i\n\nwhere boldsymboltheta_-i denotes the parameter vector with its ith element removed. \n\nAlthough not a requirement, one may employ a (partially) monotonic neural network to prevent quantile crossing (i.e., to ensure that the tau_1-quantile does not exceed the tau_2-quantile for any tau_2  tau_1). There are several ways to construct such a neural network: one simple yet effective approach is to ensure that all weights associated with tau are strictly positive (see, e.g., Cannon, 2018), and this can be done using the DensePositive layer as shown in the example below.\n\nWhen the neural network is a DeepSet, two requirements must be met. First, the number of input neurons in the first layer of the outer network must equal the number of neurons in the final layer of the inner network plus 1 + textdim(boldsymboltheta_-i), where we define  textdim(boldsymboltheta_-i) equiv 0 when targetting marginal posteriors of the form theta_i mid boldsymbolZ (the default behaviour).  Second, the number of output neurons in the final layer of the outer network must equal d - textdim(boldsymboltheta_-i). \n\nThe return value is a matrix with d - textdim(boldsymboltheta_-i) rows, corresponding to the estimated quantile for each parameter not in boldsymboltheta_-i.\n\nSee also QuantileEstimatorDiscrete.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ U(0, 1) and œÉ ~ U(0, 1)\nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsample(K) = rand32(d, K)\nsimulateZ(Œ∏, m) = [œë[1] .+ œë[2] .* randn32(n, m) for œë in eachcol(Œ∏)]\nsimulateœÑ(K)    = [rand32(10) for k in 1:K]\nsimulate(Œ∏, m)  = simulateZ(Œ∏, m), simulateœÑ(size(Œ∏, 2))\n\n# ---- Quantiles of Œ∏·µ¢ ‚à£ ùêô, i = 1, ‚Ä¶, d ----\n\n# Neural network: partially monotonic network to preclude quantile crossing\nw = 64  # width of each hidden layer\nœà = Chain(\n\tDense(n, w, relu),\n\tDense(w, w, relu),\n\tDense(w, w, relu)\n\t)\nœï = Chain(\n\tDensePositive(Dense(w + 1, w, relu); last_only = true),\n\tDensePositive(Dense(w, w, relu)),\n\tDensePositive(Dense(w, d))\n\t)\nnetwork = DeepSet(œà, œï)\n\n# Initialise the estimator\nqÃÇ = QuantileEstimatorContinuous(network)\n\n# Train the estimator\nqÃÇ = train(qÃÇ, sample, simulate, m = m)\n\n# Test data \nŒ∏ = sample(1000)\nZ = simulateZ(Œ∏, m)\n\n# Estimate 0.1-quantile for each parameter and for many data sets\nœÑ = 0.1f0\nqÃÇ(Z, œÑ)\n\n# Estimate multiple quantiles for each parameter and for many data sets\n# (note that œÑ is given as a row vector)\nœÑ = f32([0.1, 0.25, 0.5, 0.75, 0.9])'\nqÃÇ(Z, œÑ)\n\n# Estimate multiple quantiles for a single data set \nqÃÇ(Z[1], œÑ)\n\n# ---- Quantiles of Œ∏·µ¢ ‚à£ ùêô, Œ∏‚Çã·µ¢ ----\n\n# Neural network: partially monotonic network to preclude quantile crossing\nw = 64  # width of each hidden layer\nœà = Chain(\n\tDense(n, w, relu),\n\tDense(w, w, relu),\n\tDense(w, w, relu)\n\t)\nœï = Chain(\n\tDensePositive(Dense(w + 2, w, relu); last_only = true),\n\tDensePositive(Dense(w, w, relu)),\n\tDensePositive(Dense(w, d - 1))\n\t)\nnetwork = DeepSet(œà, œï)\n\n# Initialise the estimator targetting Œº‚à£Z,œÉ\ni = 1\nqÃÇ·µ¢ = QuantileEstimatorContinuous(network; i = i)\n\n# Train the estimator\nqÃÇ·µ¢ = train(qÃÇ·µ¢, prior, simulate, m = m)\n\n# Test data \nŒ∏ = sample(1000)\nZ = simulateZ(Œ∏, m)\n\n# Estimate quantiles of Œº‚à£Z,œÉ with œÉ = 0.5 and for many data sets\n# (can use Œ∏[InvertedIndices.Not(i), :] to determine the order in which the conditioned parameters should be given)\nŒ∏‚Çã·µ¢ = 0.5f0\nœÑ = f32([0.1, 0.25, 0.5, 0.75, 0.9])\nqÃÇ·µ¢(Z, Œ∏‚Çã·µ¢, œÑ)\n\n# Estimate quantiles of Œº‚à£Z,œÉ with œÉ = 0.5 and for a single data set\nqÃÇ·µ¢(Z[1], Œ∏‚Çã·µ¢, œÑ)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator <: NeuralEstimator\nPiecewiseEstimator(estimators, changepoints)\n\nCreates a piecewise estimator (Sainsbury-Dale et al., 2024, Sec. 2.2.2) from a collection of estimators and sample-size changepoints.\n\nSpecifically, with l estimators and sample-size changepoints m_1  m_2  dots  m_l-1, the piecewise etimator takes the form,\n\nhatboldsymboltheta(boldsymbolZ)\n=\nbegincases\nhatboldsymboltheta_1(boldsymbolZ)  m leq m_1\nhatboldsymboltheta_2(boldsymbolZ)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta_l(boldsymbolZ)  m  m_l-1\nendcases\n\nFor example, given an estimator hatboldsymboltheta_1(cdot) trained for small sample sizes (e.g., m leq 30) and an estimator hatboldsymboltheta_2(cdot) trained for moderate-to-large sample sizes (e.g., m  30), one may construct a PiecewiseEstimator that dispatches hatboldsymboltheta_1(cdot) if m leq 30 and hatboldsymboltheta_2(cdot) otherwise.\n\nSee also trainx().\n\nExamples\n\nusing NeuralEstimators, Flux\n\nn = 2    # bivariate data\nd = 3    # dimension of parameter vector \nw = 128  # width of each hidden layer\n\n# Small-sample estimator\nœà‚ÇÅ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, d));\nŒ∏ÃÇ‚ÇÅ = PointEstimator(DeepSet(œà‚ÇÅ, œï‚ÇÅ))\n\n# Large-sample estimator\nœà‚ÇÇ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, d));\nŒ∏ÃÇ‚ÇÇ = PointEstimator(DeepSet(œà‚ÇÇ, œï‚ÇÇ))\n\n# Piecewise estimator with changepoint m=30\nŒ∏ÃÇ = PiecewiseEstimator([Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ], 30)\n\n# Apply the (untrained) piecewise estimator to data\nZ = [rand(n, m) for m ‚àà (10, 50)]\nestimate(Œ∏ÃÇ, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.Ensemble","page":"Core","title":"NeuralEstimators.Ensemble","text":"Ensemble <: NeuralEstimator\nEnsemble(estimators)\nEnsemble(architecture::Function, J::Integer)\n(ensemble::Ensemble)(Z; aggr = median)\n\nDefines an ensemble¬†based on a collection of estimators which, when applied to data Z, returns the median (or another summary defined by aggr) of the estimates.\n\nThe ensemble can be initialised with a collection of trained estimators and then applied immediately to observed data. Alternatively, the ensemble can be initialised with a collection of untrained estimators (or a function defining the architecture of each estimator, and the number of estimators in the ensemble), trained with train(), and then applied to observed data. In the latter case, where the ensemble is trained directly, if savepath is specified both the ensemble and component estimators will be saved.\n\nNote that train() currently acts sequentially on the component estimators.\n\nThe ensemble components can be accessed by indexing the ensemble; the number of component estimators can be obtained using length().\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œ∏ ~ N(Œ∏, 1) with Œ∏ ~ N(0, 1)\nd = 1     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nm = 30    # number of independent replicates in each data set\nsampler(K) = randn32(d, K)\nsimulator(Œ∏, m) = [Œº .+ randn32(n, m) for Œº ‚àà eachcol(Œ∏)]\n\n# Neural-network architecture of each ensemble component\nfunction architecture()\n\tœà = Chain(Dense(n, 64, relu), Dense(64, 64, relu))\n\tœï = Chain(Dense(64, 64, relu), Dense(64, d))\n\tnetwork = DeepSet(œà, œï)\n\tPointEstimator(network)\nend\n\n# Initialise ensemble with three component estimators \nensemble = Ensemble(architecture, 3)\nensemble[1]      # access component estimators by indexing\nensemble[1:2]    # indexing with an iterable collection returns the corresponding ensemble \nlength(ensemble) # number of component estimators\n\n# Training\nensemble = train(ensemble, sampler, simulator, m = m, epochs = 5)\n\n# Assessment\nŒ∏ = sampler(1000)\nZ = simulator(Œ∏, m)\nassessment = assess(ensemble, Œ∏, Z)\nrmse(assessment)\n\n# Apply to data\nensemble(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The function train is used to train a single neural estimator, while the wrapper function trainx is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"train\n\ntrainx","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core","title":"NeuralEstimators.train","text":"train(estimator, sampler::Function, simulator::Function; ...)\ntrain(estimator, Œ∏_train::P, Œ∏_val::P, simulator::Function; ...) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\ntrain(estimator, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; ...) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\nTrains a neural estimator.\n\nThe methods cater for different variants of \"on-the-fly\" simulation. Specifically, a sampler can be provided to continuously sample new parameter vectors from the prior, and a simulator can be provided to continuously simulate new data conditional on the parameters. If provided with specific sets of parameters (Œ∏_train and Œ∏_val) and/or data (Z_train and Z_val), they will be held fixed during training.\n\nIn all methods, the validation parameters and data are held fixed to reduce noise when evaluating the validation risk.\n\nThe estimator is returned on the CPU so that it can be saved post training. \n\nKeyword arguments common to all methods:\n\nloss = mae (applicable only to PointEstimator): loss function used to train the neural network. In addition to the standard loss functions provided by Flux (e.g., mae, mse), see Loss functions for further options. \nepochs = 100: number of epochs to train the neural network. An epoch is one complete pass through the entire training data set when doing stochastic gradient descent.\nbatchsize = 32: the batchsize to use when performing stochastic gradient descent, that is, the number of training samples processed between each update of the neural-network parameters.\noptimiser = Flux.setup(Adam(), estimator): any Optimisers.jl optimisation rule for updating the neural-network parameters. When the training data and/or parameters are held fixed, one may wish to employ regularisation to prevent overfitting; for example, optimiser = Flux.setup(OptimiserChain(WeightDecay(1e-4), Adam()), estimator), which corresponds to L‚ÇÇ regularisation with penalty coefficient Œª=10‚Åª‚Å¥. \nsavepath::Union{String, Nothing} = nothing: path to save the trained estimator and other information; if nothing (default), nothing is saved. Otherwise, the neural-network parameters (i.e., the weights and biases) will be saved during training as bson files; the risk function evaluated over the training and validation sets will also be saved, in the first and second columns of loss_per_epoch.csv, respectively; the best parameters (as measured by validation risk) will be saved as best_network.bson.\nstopping_epochs = 5: cease training if the risk doesn't improve in this number of epochs.\nuse_gpu = true: flag indicating whether to use a GPU if one is available.\nverbose = true: flag indicating whether information, including empirical risk values and timings, should be printed to the console during training.\n\nKeyword arguments common to train(estimator, sampler, simulator) and train(estimator, Œ∏_train, Œ∏_val, simulator):\n\nm = nothing: arguments to the simulator (typically the number of replicates in each data set as an Integer or an Integer collection). The simulator is called as simulator(Œ∏, m) if m is given and as simulator(Œ∏) otherwise. \nepochs_per_Z_refresh = 1: the number of passes to make through the training set before the training data are refreshed.\nsimulate_just_in_time = false: flag indicating whether we should simulate just-in-time, in the sense that only a batchsize number of parameter vectors and corresponding data are in memory at a given time.\n\nKeyword arguments unique to train(estimator, sampler, simulator):\n\nK = 10000: number of parameter vectors in the training set.\nK_val = K √∑ 5 number of parameter vectors in the validation set.\nŒæ = nothing: an arbitrary collection of objects that, if provided, will be passed to the parameter sampler as sampler(K, Œæ); otherwise, the parameter sampler will be called as sampler(K). Can also be provided as xi.\nepochs_per_Œ∏_refresh = 1: the number of passes to make through the training set before the training parameters are refreshed. Must be a multiple of epochs_per_Z_refresh. Can also be provided as epochs_per_theta_refresh.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Data Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with priors Œº ~ N(0, 1) and œÉ ~ U(0, 1)\nfunction sampler(K)\n\tŒº = randn(K) # Gaussian prior\n\tœÉ = rand(K)  # Uniform prior\n\tŒ∏ = vcat(Œº', œÉ')\n\treturn Œ∏\nend\nfunction simulator(Œ∏, m)\n\t[œë[1] .+ œë[2] * randn(1, m) for œë ‚àà eachcol(Œ∏)]\nend\n\n# Neural network \nd = 2     # dimension of the parameter vector Œ∏\nn = 1     # dimension of each independent replicate of Z\nw = 128   # width of each hidden layer \nœà = Chain(Dense(n, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, d))\nnetwork = DeepSet(œà, œï)\n\n# Initialise the estimator\nestimator = PointEstimator(network)\n\n# Number of independent replicates to use during training\nm = 15\n\n# Training: simulation on-the-fly\nestimator  = train(estimator, sampler, simulator, m = m, epochs = 5)\n\n# Training: simulation on-the-fly with fixed parameters\nK = 10000\nŒ∏_train = sampler(K)\nŒ∏_val   = sampler(K)\nestimator = train(estimator, Œ∏_train, Œ∏_val, simulator, m = m, epochs = 5)\n\n# Training: fixed parameters and fixed data\nZ_train   = simulator(Œ∏_train, m)\nZ_val     = simulator(Œ∏_val, m)\nestimator = train(estimator, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 5)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.trainx","page":"Core","title":"NeuralEstimators.trainx","text":"trainx(estimator, sampler::Function, simulator::Function, m::Vector{Integer}; ...)\ntrainx(estimator, Œ∏_train, Œ∏_val, simulator::Function, m::Vector{Integer}; ...)\ntrainx(estimator, Œ∏_train, Œ∏_val, Z_train, Z_val, m::Vector{Integer}; ...)\ntrainx(estimator, Œ∏_train, Œ∏_val, Z_train::V, Z_val::V; ...) where {V <: AbstractVector{AbstractVector{Any}}}\n\nA wrapper around train() to construct neural estimators for different sample sizes.\n\nThe positional argument m specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size (see Sainsbury-Dale at al., 2024, Sec 2.3.3). For example, if m = [m‚ÇÅ, m‚ÇÇ], the estimator for sample size m‚ÇÇ is pre-trained with the estimator for sample size m‚ÇÅ.\n\nThe method for Z_train and Z_val subsets the data using subsetdata(Z, 1:m·µ¢) for each m·µ¢ ‚àà m. The method for Z_train::V and Z_val::V trains an estimator for each element of Z_train::V and Z_val::V and, hence, it does not need to invoke subsetdata(), which can be slow or difficult to define in some cases (e.g., for graphical data). Note that, in this case, m is inferred from the data.\n\nThe keyword arguments inherit from train(). The keyword arguments epochs, batchsize, stopping_epochs, and optimiser can each be given as vectors. For example, if training two estimators, one may use a different number of epochs for each estimator by providing epochs = [epoch‚ÇÅ, epoch‚ÇÇ].\n\nSee also PiecewiseEstimator.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessment/calibration","page":"Core","title":"Assessment/calibration","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"assess\n\nAssessment\n\nrisk\n\nbias\n\nrmse\n\ncoverage","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core","title":"NeuralEstimators.assess","text":"assess(estimator, Œ∏, Z)\n\nUsing an estimator (or a collection of estimators), computes estimates from data Z simulated based on true parameter vectors stored in Œ∏.\n\nThe data Z should be a Vector, with each element corresponding to a single simulated data set. If Z contains more data sets than parameter vectors, the parameter matrix Œ∏ will be recycled by horizontal concatenation via the call Œ∏ = repeat(Œ∏, outer = (1, J)) where J = length(Z) √∑ K is the number of simulated data sets and K = size(Œ∏, 2) is the number of parameter vectors.\n\nThe return value is of type Assessment. \n\nKeyword arguments\n\nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided). If Œæ is provided with a field parameter_names, those names will be used.\nŒæ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices). Can also be provided as xi.\nuse_Œæ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses Œæ: if it does, the estimator will be applied as estimator(Z, Œæ). This argument is useful when multiple estimators are provided, only some of which need Œæ; hence, if only one estimator is provided and Œæ is not nothing, use_Œæ is automatically set to true. Can also be provided as use_xi.\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nprobs = range(0.01, stop=0.99, length=100): (relevant only for estimator::QuantileEstimatorContinuous) a collection of probability levels in (0, 1).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core","title":"NeuralEstimators.Assessment","text":"Assessment(df::DataFrame, runtime::DataFrame)\n\nA type for storing the output of assess(). The field runtime contains the total time taken for each estimator. The field df is a long-form DataFrame with columns:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size (number of iid replicates) for the given data set\nk:         the index of the parameter vector\nj:         the index of the data set (in the case that multiple data sets are associated with each parameter vector)\n\nIf estimator is an IntervalEstimator, the column estimate will be replaced by the columns lower and upper, containing the lower and upper bounds of the interval, respectively.\n\nIf estimator is a QuantileEstimator, the df will also contain a column prob indicating the probability level of the corresponding quantile estimate.\n\nUse merge() to combine assessments from multiple estimators of the same type or join() to combine assessments from a PointEstimator and an IntervalEstimator.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.risk","page":"Core","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's Bayes risk,\n\nr(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K L(boldsymboltheta^(k) hatboldsymboltheta(boldsymbolZ^(k)))\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nKeyword arguments\n\nloss = (x, y) -> abs(x - y): a binary operator defining the loss function (default absolute-error loss).\naverage_over_parameters::Bool = false: if true, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the loss is averaged over all sample sizes m; otherwise, the loss is averaged over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bias","page":"Core","title":"NeuralEstimators.bias","text":"bias(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's bias,\n\ntextrmbias(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k)\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.rmse","page":"Core","title":"NeuralEstimators.rmse","text":"rmse(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's root-mean-squared error,\n\ntextrmrmse(hatboldsymboltheta(cdot))\napprox\nsqrtfrac1K sum_k=1^K (hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k))^2\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.coverage","page":"Core","title":"NeuralEstimators.coverage","text":"coverage(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an interval estimator's expected coverage, as defined in Hermans et al. (2022, Definition 2.1), and the proportion of parameters below and above the lower and upper bounds, respectively.\n\nKeyword arguments\n\naverage_over_parameters::Bool = false: if true, the coverage is averaged over all parameters; otherwise (default), it is computed over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the coverage is averaged over all sample sizes m; otherwise, it is computed over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-with-observed-data","page":"Core","title":"Inference with observed data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The following functions are intended to facilitate the use of a trained neural estimator with observed data. ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Note that most NeuralEstimators are callable and can be applied to data Z (possibly containing multiple data sets) in a call of the form estimator(Z) or similar. In these cases, one may leverage a GPU by simply moving the estimator and the data to the GPU using gpu(). See also estimate() to apply the estimator over batches of data, which can alleviate memory issues when working with large data sets (the methods below typically use this memory-safe approach where possible). ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"estimate\n\nbootstrap\n\ninterval\n\nsampleposterior\n\nposteriormean \n\nposteriormedian\n\nposteriorquantile\n\nposteriormode\n\nmlestimate","category":"page"},{"location":"API/core/#NeuralEstimators.estimate","page":"Core","title":"NeuralEstimators.estimate","text":"estimate(estimator, Z, T = nothing; batchsize::Integer = 32, use_gpu::Bool = true, kwargs...)\n\nApplies estimator to batches of Z (and optionally other set-level information T) of size batchsize.\n\nThis can prevent memory issues that can occur with large data sets, particularly on the GPU.\n\nBatching will only be done if there are multiple data sets in Z, which will be inferred by Z being a vector, or a tuple whose first element is a vector.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core","title":"NeuralEstimators.bootstrap","text":"bootstrap(estimator::PointEstimator, parameters::P, Z; use_gpu = true) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(estimator::PointEstimator, parameters::P, simulator, m::Integer; B = 400, use_gpu = true) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(estimator::PointEstimator, Z; B = 400, blocks = nothing, trim = true, use_gpu = true)\n\nGenerates B bootstrap estimates using estimator.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, Z, whose length implicitly defines B. Alternatively, one may provide a simulator and the desired sample size, in which case the data will be simulated using simulator(parameters, m).\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm generates resampled data sets by sampling blocks with replacement. If trim = true, the final block is trimmed as needed to ensure that the resampled data set matches the original size of Z. \n\nThe return type is a d √ó B matrix, where d is the dimension of the parameter vector. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.interval","page":"Core","title":"NeuralEstimators.interval","text":"interval(Œ∏::Matrix; probs = [0.05, 0.95], parameter_names = nothing)\ninterval(estimator::IntervalEstimator, Z; parameter_names = nothing, use_gpu = true)\n\nComputes a confidence/credible interval based either on a d √ó B matrix Œ∏ of parameters (typically containing bootstrap estimates or posterior draws), where d denotes the number of parameters to make inference on, or from an IntervalEstimator and data Z.\n\nWhen given Œ∏, the intervals are constructed by computing quantiles with probability levels controlled by the keyword argument probs.\n\nThe return type is a d √ó 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval. The rows of this matrix can be named by passing a vector of strings to the keyword argument parameter_names. \n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.sampleposterior","page":"Core","title":"NeuralEstimators.sampleposterior","text":"sampleposterior(estimator::PosteriorEstimator, Z, N::Integer = 1000)\nsampleposterior(estimator::RatioEstimator, Z, N::Integer = 1000; Œ∏_grid, prior::Function = Œ∏ -> 1f0)\n\nSamples from the approximate posterior distribution implied by estimator.\n\nThe positional argument N controls the size of the posterior sample.\n\nWhen sampling based on a RatioEstimator, the sampling algorithm is based on a fine-gridding of the parameter space, specified through the keyword argument Œ∏_grid (or theta_grid).  The approximate posterior density is evaluated over this grid, which is then used to draw samples. This is very effective when making inference with a small number of parameters. For models with a large number of parameters, other sampling algorithms may be needed (please feel free to contact the package maintainer for discussion). The prior distribution p(boldsymboltheta) is controlled through the keyword argument prior (by default, a uniform prior is used).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormean","page":"Core","title":"NeuralEstimators.posteriormean","text":"posteriormean(Œ∏::AbstractMatrix)\t\nposteriormean(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, N::Integer = 1000; kwargs...)\n\nComputes the posterior mean based either on a d √ó N matrix Œ∏ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nSee also posteriormedian(), posteriormode(), and mlestimate().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormedian","page":"Core","title":"NeuralEstimators.posteriormedian","text":"posteriormedian(Œ∏::AbstractMatrix)\t\nposteriormedian(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, N::Integer = 1000; kwargs...)\n\nComputes the vector of marginal posterior medians based either on a d √ó N matrix Œ∏ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nSee also posteriormean(), posteriorquantile(), and mlestimate().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriorquantile","page":"Core","title":"NeuralEstimators.posteriorquantile","text":"posteriorquantile(Œ∏::AbstractMatrix, probs)\t\nposteriormedian(estimator::Union{PosteriorEstimator, RatioEstimator}, Z, probs, N::Integer = 1000; kwargs...)\n\nComputes the vector of marginal posterior quantiles with (a collection of) probability levels probs, based either on a d √ó N matrix Œ∏ of posterior draws, where d denotes the number of parameters to make inference on, or directly from an estimator that allows for posterior sampling via sampleposterior().\n\nThe return value is a d √ó length(probs) matrix. \n\nSee also posteriormedian().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.posteriormode","page":"Core","title":"NeuralEstimators.posteriormode","text":"posteriormode(estimator::RatioEstimator, Z; Œ∏‚ÇÄ = nothing, Œ∏_grid = nothing, prior::Function = Œ∏ -> 1, use_gpu = true)\n\nComputes the (approximate) posterior mode (maximum a posteriori estimate) given data boldsymbolZ,\n\nundersetboldsymbolthetamathrmargmax ell(boldsymboltheta  boldsymbolZ) + log p(boldsymboltheta)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function implied by estimator, and p(boldsymboltheta) denotes the prior density function controlled through the keyword argument prior.\n\nIf a vector Œ∏‚ÇÄ of initial parameter estimates is given, the approximate posterior density is maximised by gradient descent (requires Optim.jl to be loaded). Otherwise, if a matrix of parameters Œ∏_grid is given, the approximate posterior density is maximised by grid search.\n\nSee also mlestimate(), posteriormedian(), and posteriormean().\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.mlestimate","page":"Core","title":"NeuralEstimators.mlestimate","text":"mlestimate(estimator::RatioEstimator, Z; Œ∏‚ÇÄ = nothing, Œ∏_grid = nothing, penalty::Function = Œ∏ -> 1, use_gpu = true)\n\nComputes the (approximate) maximum likelihood estimate given data boldsymbolZ,\n\nundersetboldsymbolthetamathrmargmax ell(boldsymboltheta  boldsymbolZ)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function implied by estimator.\n\nIf a vector Œ∏‚ÇÄ of initial parameter estimates is given, the approximate likelihood is maximised by gradient descent (requires Optim.jl to be loaded). Otherwise, if a matrix of parameters Œ∏_grid is given, the approximate likelihood is maximised by grid search.\n\nA maximum penalised likelihood estimate,\n\nundersetboldsymbolthetamathrmargmax ell(boldsymboltheta  boldsymbolZ) + log p(boldsymboltheta)\n\ncan be obtained by specifying the keyword argument penalty that defines the penalty term p(boldsymboltheta).\n\nSee also posteriormean(), posteriormedian(), and posteriormode().\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"Order = [:type, :function]\nPages   = [\"utility.md\"]","category":"page"},{"location":"API/utility/#Core","page":"Miscellaneous","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"numberreplicates\n\nsubsetdata\n\nsubsetparameters","category":"page"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Miscellaneous","title":"NeuralEstimators.numberreplicates","text":"numberreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Miscellaneous","title":"NeuralEstimators.subsetdata","text":"subsetdata(Z::V, i) where {V <: AbstractArray{A}} where {A <: Any}\nsubsetdata(Z::A, i) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, i) where {G <: AbstractGraph}\n\nReturn replicate(s) i from each data set in Z.\n\nIf the user is working with data that are not covered by the default methods, simply overload the function with the appropriate type for Z.\n\nFor graphical data, calls getgraph(), where the replicates are assumed be to stored as batched graphs. Since this can be slow, one should consider using a method of train() that does not require the data to be subsetted when working with graphical data (use numberreplicates() to check that the training and validation data sets are equally replicated, which prevents subsetting).\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nd = 1  # dimension of the response variable\nn = 4  # number of observations in each realisation\nm = 6  # number of replicates in each data set\nK = 2  # number of data sets\n\n# Array data\nZ = [rand(n, d, m) for k ‚àà 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ‚àà 1:m]) for k ‚àà 1:K]\nsubsetdata(Z, 2)   # extract second replicate from each data set\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Miscellaneous","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Downstream-inference-algorithms","page":"Miscellaneous","title":"Downstream-inference algorithms","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"EM","category":"page"},{"location":"API/utility/#NeuralEstimators.EM","page":"Miscellaneous","title":"NeuralEstimators.EM","text":"EM(simulateconditional::Function, MAP::Union{Function, NeuralEstimator}, Œ∏‚ÇÄ = nothing)\n\nImplements the (Bayesian) Monte Carlo expectation-maximisation (EM) algorithm, with lth iteration\n\nboldsymboltheta^(l) =\nundersetboldsymbolthetamathrmargmax\nsum_h = 1^H ell(boldsymboltheta  boldsymbolZ_1  boldsymbolZ_2^(lh)) + Hlog pi(boldsymboltheta)\n\nwhere ell(cdot) is the complete-data log-likelihood function, boldsymbolZ equiv (boldsymbolZ_1 boldsymbolZ_2) denotes the complete data with boldsymbolZ_1 and boldsymbolZ_2 the observed and missing components, respectively, boldsymbolZ_2^(lh), h = 1 dots H, is simulated from the distribution of boldsymbolZ_2 mid boldsymbolZ_1 boldsymboltheta^(l-1), and pi(boldsymboltheta) denotes the prior density.\n\nFields\n\nThe function simulateconditional should have a signature of the form,\n\nsimulateconditional(Z::A, Œ∏; nsims = 1) where {A <: AbstractArray{Union{Missing, T}}} where T\n\nThe output of simulateconditional should be the completed-data Z, and it should be returned in whatever form is appropriate to be passed to the MAP estimator as MAP(Z). For example, if the data are gridded and the MAP is a neural MAP estimator based on a CNN architecture, then Z should be returned as a four-dimensional array.\n\nThe field MAP can be a function (to facilitate the conventional Monte Carlo EM algorithm) or a NeuralEstimator (to facilitate the so-called neural EM algorithm).\n\nThe starting values Œ∏‚ÇÄ may be provided during initialisation (as a vector), or when applying the EM object to data (see below). The starting values given in a function call take precedence over those stored in the object.\n\nMethods\n\nOnce constructed, objects of type EM can be applied to data via the methods,\n\n(em::EM)(Z::A, Œ∏‚ÇÄ::Union{Nothing, Vector} = nothing; ...) where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n(em::EM)(Z::V, Œ∏‚ÇÄ::Union{Nothing, Vector, Matrix} = nothing; ...) where {V <: AbstractVector{A}} where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n\nwhere Z is the complete data containing the observed data and Missing values. Note that the second method caters for the case that one has multiple data sets. The keyword arguments are:\n\nnsims = 1: the number H of conditional simulations in each iteration.\nniterations = 50: the maximum number of iterations.\nnconsecutive = 3: the number of consecutive iterations for which the convergence criterion must be met.\nœµ = 0.01: tolerance used to assess convergence; the algorithm halts if the relative change in parameter values in successive iterations is less than œµ.\nreturn_iterates::Bool: if true, the estimate at each iteration of the algorithm is returned; otherwise, only the final estimate is returned.\nŒæ = nothing: model information needed for conditional simulation (e.g., distance matrices) or in the MAP estimator.\nuse_Œæ_in_simulateconditional::Bool = false: if set to true, the conditional simulator is called as simulateconditional(Z, Œ∏, Œæ; nsims = nsims).\nuse_Œæ_in_MAP::Bool = false: if set to true, the MAP estimator is called as MAP(Z, Œæ).\nuse_gpu::Bool = true\nverbose::Bool = false\n\nExamples\n\n# See the \"Missing data\" section in \"Advanced usage\"\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#Utility-functions","page":"Miscellaneous","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"adjacencymatrix\n\ncontainertype\n\nencodedata\n\nexpandgrid\n\nIndicatorWeights\n\nKernelWeights\n\nmaternchols\n\nremovedata\n\nrowwisenorm\n\nspatialgraph\n\nstackarrays\n\nvectotril","category":"page"},{"location":"API/utility/#NeuralEstimators.adjacencymatrix","page":"Miscellaneous","title":"NeuralEstimators.adjacencymatrix","text":"adjacencymatrix(S::Matrix, k::Integer; maxmin = false, combined = false)\nadjacencymatrix(S::Matrix, r::AbstractFloat)\nadjacencymatrix(S::Matrix, r::AbstractFloat, k::Integer; random = true)\nadjacencymatrix(M::Matrix; k, r, kwargs...)\n\nComputes a spatially weighted adjacency matrix from spatial locations S based  on either the k-nearest neighbours of each location; all nodes within a disc of fixed radius r; or, if both r and k are provided, a subset of k neighbours within a disc of fixed radius r.\n\nIf S is a square matrix, it is treated as a distance matrix; otherwise, it should be an n x d matrix, where n is the number of spatial locations and d is the spatial dimension (typically d = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that use of a  maxmin ordering currently requires a matrix of spatial locations (not a distance matrix).\n\nWhen using the k nearest neighbours, if maxmin=false (default) the neighbours are chosen based on all points in the graph. If maxmin=true, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the k-nearest neighbours amongst the points that have already appeared in the ordering. If combined=true, the  neighbours are defined to be the union of the k-nearest neighbours and the  k-nearest neighbours subject to a maxmin ordering. \n\nTwo subsampling strategies are implemented when choosing a subset of k neighbours within  a disc of fixed radius r. If random=true (default), the neighbours are randomly selected from  within the disc. If random=false, a deterministic algorithm is used  that aims to preserve the distribution of distances within the neighbourhood set, by choosing  those nodes with distances to the central node corresponding to the  0 frac1k frac2k dots frack-1k 1 quantiles of the empirical  distribution function of distances within the disc (this in fact yields up to k+1 neighbours,  since both the closest and furthest nodes are always included). \n\nBy convention with the functionality in GraphNeuralNetworks.jl which is based on directed graphs,  the neighbours of location i are stored in the column A[:, i] where A is the  returned adjacency matrix. Therefore, the number of neighbours for each location is given by collect(mapslices(nnz, A; dims = 1)), and the number of times each node is  a neighbour of another node is given by collect(mapslices(nnz, A; dims = 2)).\n\nBy convention, we do not consider a location to neighbour itself (i.e., the diagonal elements of the adjacency matrix are zero). \n\nExamples\n\nusing NeuralEstimators, Distances, SparseArrays\n\nn = 250\nd = 2\nS = rand(Float32, n, d)\nk = 10\nr = 0.10\n\n# Memory efficient constructors\nadjacencymatrix(S, k)\nadjacencymatrix(S, k; maxmin = true)\nadjacencymatrix(S, k; maxmin = true, combined = true)\nadjacencymatrix(S, r)\nadjacencymatrix(S, r, k)\nadjacencymatrix(S, r, k; random = false)\n\n# Construct from full distance matrix D\nD = pairwise(Euclidean(), S, dims = 1)\nadjacencymatrix(D, k)\nadjacencymatrix(D, r)\nadjacencymatrix(D, r, k)\nadjacencymatrix(D, r, k; random = false)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.containertype","page":"Miscellaneous","title":"NeuralEstimators.containertype","text":"containertype(A::Type)\ncontainertype(::Type{A}) where A <: SubArray\ncontainertype(a::A) where A\n\nReturns the container type of its argument.\n\nIf given a SubArray, returns the container type of the parent array.\n\nExamples\n\na = rand(3, 4)\ncontainertype(a)\ncontainertype(typeof(a))\n[containertype(x) for x ‚àà eachcol(a)]\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.encodedata","page":"Miscellaneous","title":"NeuralEstimators.encodedata","text":"encodedata(Z::A; c::T = zero(T)) where {A <: AbstractArray{Union{Missing, T}, N}} where T, N\n\nFor data Z with missing entries, returns an encoded data set (U, W) where W encodes the missingness pattern as an indicator vector and U is the original data Z with missing entries replaced by a fixed constant c.\n\nThe indicator vector W is stored in the second-to-last dimension of Z, which should be singleton. If the second-to-last dimension is not singleton, then two singleton dimensions will be added to the array, and W will be stored in the new second-to-last dimension.\n\nExamples\n\nusing NeuralEstimators\n\n# Generate some missing data\nZ = rand(16, 16, 1, 1)\nZ = removedata(Z, 0.25)\t # remove 25% of the data\n\n# Encode the data\nUW = encodedata(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Miscellaneous","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nGenerates a grid of all possible combinations of the elements from two input vectors, xs and ys. \n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.IndicatorWeights","page":"Miscellaneous","title":"NeuralEstimators.IndicatorWeights","text":"IndicatorWeights(h_max, n_bins::Integer)\n(w::IndicatorWeights)(h::Matrix)\n\nFor spatial locations boldsymbols and  boldsymbolu, creates a spatial weight function defined as\n\nboldsymbolw(boldsymbols boldsymbolu) equiv (mathbbI(h in B_k)  k = 1 dots K)\n\nwhere mathbbI(cdot) denotes the indicator function,  h equiv boldsymbols - boldsymbolu  is the spatial distance between boldsymbols and  boldsymbolu, and B_k  k = 1 dots K is a set of K =n_bins equally-sized distance bins covering the spatial distances between 0 and h_max. \n\nExamples\n\nusing NeuralEstimators \n\nh_max = 1\nn_bins = 10\nw = IndicatorWeights(h_max, n_bins)\nh = rand(1, 30) # distances between 30 pairs of spatial locations \nw(h)\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#NeuralEstimators.KernelWeights","page":"Miscellaneous","title":"NeuralEstimators.KernelWeights","text":"KernelWeights(h_max, n_bins::Integer)\n(w::KernelWeights)(h::Matrix)\n\nFor spatial locations boldsymbols and  boldsymbolu, creates a spatial weight function defined as\n\nboldsymbolw(boldsymbols boldsymbolu) equiv (exp(-(h - mu_k)^2  (2sigma_k^2))  k = 1 dots K)\n\nwhere h equiv boldsymbols - boldsymbolu is the spatial distance between boldsymbols and boldsymbolu, and mu_k  k = 1 dots K and sigma_k  k = 1 dots K are the means and standard deviations of the Gaussian kernels for each bin, covering the spatial distances between 0 and h_max.\n\nExamples\n\nusing NeuralEstimators \n\nh_max = 1\nn_bins = 10\nw = KernelWeights(h_max, n_bins)\nh = rand(1, 30) # distances between 30 pairs of spatial locations \nw(h)\n\n\n\n\n\n","category":"type"},{"location":"API/utility/#NeuralEstimators.maternchols","page":"Miscellaneous","title":"NeuralEstimators.maternchols","text":"maternchols(D, œÅ, ŒΩ, œÉ¬≤ = 1; stack = true)\n\nGiven a matrix D of distances, constructs the Cholesky factor of the covariance matrix under the Mat√©rn covariance function with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance œÉ¬≤.\n\nProviding vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices D may also be provided.\n\nIf stack = true, the Cholesky factors will be \"stacked\" into a three-dimensional array (this is only possible if all distance matrices in D are the same size).\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra: norm\nn  = 10\nS  = rand(n, 2)\nD  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(S), s‚±º ‚àà eachrow(S)]\nœÅ  = [0.6, 0.5]\nŒΩ  = [0.7, 1.2]\nœÉ¬≤ = [0.2, 0.4]\nmaternchols(D, œÅ, ŒΩ)\nmaternchols([D], œÅ, ŒΩ)\nmaternchols(D, œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤)\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(2n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.removedata","page":"Miscellaneous","title":"NeuralEstimators.removedata","text":"removedata(Z::Array, I·µ§::Vector{Integer})\nremovedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)\nremovedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)\n\nReplaces elements of Z with missing.\n\nThe simplest method accepts a vector of integers I·µ§ that give the specific indices of the data to be removed.\n\nAlternatively, there are two methods available to randomly generate missing data.\n\nFirst, a vector p may be given that specifies the proportion of missingness for each element in the response vector. Hence, p should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If prevent_complete_missing = true, no replicates will contain 100% missingness (note that this can slightly alter the effective values of p).\n\nSecond, if an integer n is provided, all replicates will contain n observations after the data are removed. If fixed_pattern = true, the missingness pattern is fixed for all replicates. If contiguous_pattern = true, the data will be removed in a contiguous block based on a randomly selected starting index. If variable_proportion = true, the proportion of missingness will vary across replicates, with each replicate containing between 1 and n observations after data removal, sampled uniformly (note that variable_proportion overrides fixed_pattern).\n\nThe return type is Array{Union{T, Missing}}.\n\nExamples\n\nd = 5           # dimension of each replicate\nm = 2000        # number of replicates\nZ = rand(d, m)  # simulated data\n\n# Passing a desired proportion of missingness\np = rand(d)\nremovedata(Z, p)\n\n# Passing a desired final sample size\nn = 3  # number of observed elements of each replicate: must have n <= d\nremovedata(Z, n)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.rowwisenorm","page":"Miscellaneous","title":"NeuralEstimators.rowwisenorm","text":"rowwisenorm(A)\n\nComputes the row-wise norm of a matrix A.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.spatialgraph","page":"Miscellaneous","title":"NeuralEstimators.spatialgraph","text":"spatialgraph(S)\nspatialgraph(S, Z)\nspatialgraph(g::GNNGraph, Z)\n\nGiven spatial data Z measured at spatial locations S, constructs a GNNGraph ready for use in a graph neural network that employs SpatialGraphConv layers. \n\nWhen m independent replicates are collected over the same set of n spatial locations,\n\nboldsymbols_1 dots boldsymbols_n subset mathcalD\n\nwhere mathcalD subset mathbbR^d denotes the spatial domain of interest,  Z should be given as an n times m matrix and S should be given as an n times d matrix.  Otherwise, when m independent replicates are collected over differing sets of spatial locations,\n\nboldsymbols_ij dots boldsymbols_in_i subset mathcalD quad i = 1 dots m\n\nZ should be given as an m-vector of n_i-vectors, and S should be given as an m-vector of n_i times d matrices.\n\nThe spatial information between neighbours is stored as an edge feature, with the specific  information controlled by the keyword arguments stationary and isotropic.  Specifically, the edge feature between node j and node j stores the spatial  distance boldsymbols_j - boldsymbols_j (if isotropic), the spatial  displacement boldsymbols_j - boldsymbols_j (if stationary), or the matrix of   locations (boldsymbols_j boldsymbols_j) (if !stationary).  \n\nAdditional keyword arguments inherit from adjacencymatrix() to determine the neighbourhood of each node, with the default being a randomly selected set of  k=30 neighbours within a disc of radius r=0.15 units.\n\nExamples\n\nusing NeuralEstimators\n\n# Number of replicates and spatial dimension\nm = 5  \nd = 2  \n\n# Spatial locations fixed for all replicates\nn = 100\nS = rand(n, d)\nZ = rand(n, m)\ng = spatialgraph(S, Z)\n\n# Spatial locations varying between replicates\nn = rand(50:100, m)\nS = rand.(n, d)\nZ = rand.(n)\ng = spatialgraph(S, Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Miscellaneous","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStacks a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.vectotril","page":"Miscellaneous","title":"NeuralEstimators.vectotril","text":"vectotril(v; strict = false)\nvectotriu(v; strict = false)\n\nConverts a vector v of length d(d+1)2 (a triangular number) into a d  d lower or upper triangular matrix.\n\nIf strict = true, the matrix will be strictly lower or upper triangular, that is, a (d+1)  (d+1) triangular matrix with zero diagonal.\n\nNote that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if v is a GPU array. Note also that the return type is not of type Triangular matrix (i.e., the zeros are materialised) since Triangular matrices are not always compatible with other GPU operations.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nn = d*(d+1)√∑2\nv = collect(range(1, n))\nvectotril(v)\nvectotriu(v)\nvectotril(v; strict = true)\nvectotriu(v; strict = true)\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"In addition to the standard loss functions provided by Flux (e.g., mae, mse), NeuralEstimators provides the following loss functions for training estimators of type PointEstimator. ","category":"page"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"tanhloss\n\nkpowerloss\n\nquantileloss\n\nintervalscore","category":"page"},{"location":"API/loss/#NeuralEstimators.tanhloss","page":"Loss functions","title":"NeuralEstimators.tanhloss","text":"tanhloss(Œ∏ÃÇ, Œ∏, Œ∫; agg = mean)\n\nFor Œ∫ > 0, computes the loss function given in Sainsbury-Dale et al. (2025; Eqn. 14), namely,\n\nL(hatboldsymboltheta boldsymboltheta) = tanhbighatboldsymboltheta - boldsymboltheta_1kappabig)\n\nwhich yields the 0-1 loss function in the limit Œ∫ ‚Üí 0. \n\nCompared with the kpowerloss(), which may also be used as a continuous approximation of the 0‚Äì1 loss function, the gradient of this loss is bounded as hatboldsymboltheta - boldsymboltheta_1 to 0, which can improve numerical stability during training. \n\nSee also kpowerloss().\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.kpowerloss","page":"Loss functions","title":"NeuralEstimators.kpowerloss","text":"kpowerloss(Œ∏ÃÇ, Œ∏, Œ∫; agg = mean, safeorigin = true, œµ = 0.1)\n\nFor Œ∫ > 0, the Œ∫-th power absolute-distance loss function,\n\nL(hatboldsymboltheta boldsymboltheta) = hatboldsymboltheta - boldsymboltheta_1^kappa\n\ncontains the squared-error (Œ∫ = 2), absolute-error (Œ∫ = 2), and 0‚Äì1 (Œ∫ ‚Üí 0) loss functions as special cases. It is Lipschitz continuous if Œ∫ = 1, convex if Œ∫ ‚â• 1, and strictly convex if Œ∫ > 1. It is quasiconvex for all Œ∫ > 0.\n\nIf safeorigin = true, the loss function is modified to be piecewise, continuous, and linear in the œµ-interval surrounding the origin, to avoid pathologies around the origin. \n\nSee also tanhloss().\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.quantileloss","page":"Loss functions","title":"NeuralEstimators.quantileloss","text":"quantileloss(Œ∏ÃÇ, Œ∏, œÑ; agg = mean)\nquantileloss(Œ∏ÃÇ, Œ∏, œÑ::Vector; agg = mean)\n\nThe asymmetric quantile loss function,\n\n  L(Œ∏ Œ∏ œÑ) = (Œ∏ - Œ∏)(ùïÄ(Œ∏ - Œ∏  0) - œÑ)\n\nwhere œÑ ‚àà (0, 1) is a probability level and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes œÑ as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in Œ∏ÃÇ is assumed to be dr, where d is the number of parameters and r is the number probability levels in œÑ (i.e., the length of œÑ).\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.intervalscore","page":"Loss functions","title":"NeuralEstimators.intervalscore","text":"intervalscore(l, u, Œ∏, Œ±; agg = mean)\nintervalscore(Œ∏ÃÇ, Œ∏, Œ±; agg = mean)\nintervalscore(assessment::Assessment; average_over_parameters::Bool = false, average_over_sample_sizes::Bool = true)\n\nGiven an interval [l, u] with nominal coverage 100√ó(1-Œ±)%  and true value Œ∏, the interval score (Gneiting and Raftery, 2007) is defined as\n\nS(l u Œ∏ Œ±) = (u - l) + 2Œ±¬π(l - Œ∏)ùïÄ(Œ∏  l) + 2Œ±¬π(Œ∏ - u)ùïÄ(Œ∏  u)\n\nwhere Œ± ‚àà (0, 1) and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes a single value Œ∏ÃÇ assumes that Œ∏ÃÇ is a matrix with 2d rows, where d is the dimension of the parameter vector to make inference on. The first and second sets of d rows will be used as l and u, respectively.\n\n\n\n\n\n","category":"function"},{"location":"API/approximatedistributions/#Approximate-distributions","page":"Approximate distributions","title":"Approximate distributions","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"When constructing a PosteriorEstimator, one must choose an approximate distribution q(boldsymboltheta boldsymbolkappa) for the posterior distribution p(boldsymboltheta mid boldsymbolZ). These distributions are implemented as subtypes of the abstract supertype ApproximateDistribution. ","category":"page"},{"location":"API/approximatedistributions/#Distributions","page":"Approximate distributions","title":"Distributions","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"ApproximateDistribution\n\nGaussianDistribution\n\nNormalisingFlow","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.ApproximateDistribution","page":"Approximate distributions","title":"NeuralEstimators.ApproximateDistribution","text":"ApproximateDistribution\n\nAn abstract supertype for approximate posterior distributions used in conjunction with a PosteriorEstimator.\n\nSubtypes A <: ApproximateDistribution should provide methods logdensity(q::A, Œ∏::AbstractMatrix, Z) and sampleposterior(q::A, Z, N::Integer). \n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#NeuralEstimators.GaussianDistribution","page":"Approximate distributions","title":"NeuralEstimators.GaussianDistribution","text":"GaussianDistribution <: ApproximateDistribution\nGaussianDistribution(d::Integer)\n\nA Gaussian distribution for amortised posterior inference, where d is the dimension of the parameter vector. \n\nThe approximate-distribution parameters boldsymbolkappa = (boldsymbolmu textrmvech(boldsymbolL)) consist of a d-dimensional mean parameter boldsymbolmu and the d(d+1)2 non-zero elements of the lower Cholesky factor boldsymbolL of a covariance matrix, where the half-vectorisation operator textrmvech(cdot) vectorises the lower triangle of its matrix argument.\n\nWhen using a GaussianDistribution as the approximate distribution of a PosteriorEstimator,  the neural network of the PosteriorEstimator should be a mapping from the sample space to mathbbR^mathcalK,  where mathcalK denotes the space of boldsymbolkappa. The dimension mathcalK can be determined from an object of type GaussianDistribution using numdistributionalparams(). Given the mathcalK-dimensional real-valued outputs of the neural network, a valid covariance matrix is constructed internally using CovarianceMatrix.\n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#NeuralEstimators.NormalisingFlow","page":"Approximate distributions","title":"NeuralEstimators.NormalisingFlow","text":"NormalisingFlow <: ApproximateDistribution\nNormalisingFlow(d::Integer, dstar::Integer; num_coupling_layers::Integer = 6, kwargs...)\n\nA normalising flow for amortised posterior inference (e.g., Ardizzone et al., 2019; Radev et al., 2022), where d is the dimension of  the parameter vector and dstar is the dimension of the summary statistics for the data. \n\nNormalising flows are diffeomorphisms (i.e., invertible, differentiable transformations with differentiable inverses) that map a simple base distribution (e.g., standard Gaussian) to a more complex target distribution (e.g., the posterior). They achieve this by applying a sequence of learned transformations, the forms of which are chosen to be invertible and allow for tractable density computation via the change of variables formula. This allows for efficient density evaluation during the training stage, and efficient sampling during the inference stage. For further details, see the reviews by Kobyzev et al. (2020) and Papamakarios (2021).\n\nWhen using a NormalisingFlow as the approximate distribution of a PosteriorEstimator,  the neural network of the PosteriorEstimator should be a mapping from the sample space to mathbbR^d^*,  where d^* is an appropriate number of summary statistics for the given parameter vector (e.g., d^* = d).\n\nNormalisingFlow uses affine coupling blocks (see AffineCouplingBlock), with activation normalisation (Kingma and Dhariwal, 2018) and permutations used between each block. The base distribution is taken to be a standard multivariate Gaussian distribution. \n\nKeyword arguments\n\nnum_coupling_layers::Integer = 6: number of coupling layers. \nkwargs: additional keyword arguments passed to AffineCouplingBlock. \n\n\n\n\n\n","category":"type"},{"location":"API/approximatedistributions/#Methods","page":"Approximate distributions","title":"Methods","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"numdistributionalparams","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.numdistributionalparams","page":"Approximate distributions","title":"NeuralEstimators.numdistributionalparams","text":"numdistributionalparams(q::ApproximateDistribution)\nnumdistributionalparams(estimator::PosteriorEstimator)\n\nThe number of distributional parameters (i.e., the dimension of the space mathcalK of approximate-distribution parameters boldsymbolkappa). \n\n\n\n\n\n","category":"function"},{"location":"API/approximatedistributions/#Building-blocks","page":"Approximate distributions","title":"Building blocks","text":"","category":"section"},{"location":"API/approximatedistributions/","page":"Approximate distributions","title":"Approximate distributions","text":"AffineCouplingBlock","category":"page"},{"location":"API/approximatedistributions/#NeuralEstimators.AffineCouplingBlock","page":"Approximate distributions","title":"NeuralEstimators.AffineCouplingBlock","text":"AffineCouplingBlock(Œ∫‚ÇÅ::MLP, Œ∫‚ÇÇ::MLP)\nAffineCouplingBlock(d‚ÇÅ::Integer, dstar::Integer, d‚ÇÇ; kwargs...)\n\nAn affine coupling block used in a NormalisingFlow. \n\nAn affine coupling block splits its input boldsymboltheta into two disjoint components, boldsymboltheta_1 and boldsymboltheta_2, with dimensions d_1 and d_2, respectively. The block then applies the following transformation: \n\nbeginaligned\n    tildeboldsymboltheta_1 = boldsymboltheta_1\n    tildeboldsymboltheta_2 = boldsymboltheta_2 odot expboldsymbolkappa_boldsymbolgamma1(tildeboldsymboltheta_1 boldsymbolT(boldsymbolZ)) + boldsymbolkappa_boldsymbolgamma2(tildeboldsymboltheta_1 boldsymbolT(boldsymbolZ))\nendaligned\n\nwhere boldsymbolkappa_boldsymbolgamma1(cdot) and boldsymbolkappa_boldsymbolgamma2(cdot) are generic, non-invertible multilayer perceptrons (MLPs) that are functions of both the (transformed) first input component tildeboldsymboltheta_1 and the learned d^*-dimensional summary statistics boldsymbolT(boldsymbolZ) (see PosteriorEstimator). \n\nAdditional keyword arguments kwargs are passed to the MLP constructor when creating Œ∫‚ÇÅ and Œ∫‚ÇÇ. \n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Architectures","page":"Architectures","title":"Architectures","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In principle, any Flux model can be used to construct the neural network. To integrate it into the workflow, one need only define a method that transforms K-dimensional vectors of data sets into matrices with K columns, where the number of rows corresponds to the dimensionality of the output spaces listed in the Overview. ","category":"page"},{"location":"API/architectures/#Modules","page":"Architectures","title":"Modules","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"The following high-level modules are often used when constructing the neural network. In particular, the type DeepSet serves as a convenient wrapper for embedding standard neural networks (e.g., MLPs, CNNs, GNNs) in a framework for making inference with an arbitrary number of independent replicates, and it comes with pre-defined methods for handling the transformations from a K-dimensional vector of data to a matrix output described above. ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DeepSet\n\nGNNSummary\n\nMLP","category":"page"},{"location":"API/architectures/#NeuralEstimators.DeepSet","page":"Architectures","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, a = mean; S = nothing)\n(ds::DeepSet)(Z::Vector{A}) where A <: Any\n(ds::DeepSet)(tuple::Tuple{Vector{A}, Vector{Vector}}) where A <: Any\n\nThe DeepSets representation (Zaheer et al., 2017; Sainsbury-Dale et al., 2024),\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi(mathbfT(mathbfZ)) quad\nmathbfT(mathbfZ) = mathbfa(boldsymbolpsi(mathbfZ_i)  i = 1 dots m)\n\n\nwhere ùêô ‚â° (ùêô‚ÇÅ', ‚Ä¶, ùêô‚Çò')' are independent replicates of data,  œà and œï are neural networks, and a is a permutation-invariant aggregation function. \n\nThe function a must operate on arrays and have a keyword argument dims for  specifying the dimension of aggregation (e.g., sum, mean, maximum, minimum, logsumexp).\n\nDeepSet objects act on data of type Vector{A}, where each element of the vector is associated with one data set (i.e., one set of independent replicates), and where A depends on the chosen architecture for œà. As a rule of thumb, when A is an array, replicates are stored in the final dimension. For example, with gridded spatial data and œà a CNN, A should be a 4-dimensional array, with replicates stored in the 4·µó ∞ dimension.  Note that, when using Flux, the final dimension is usually the \"batch\" dimension, but batching with DeepSet objects is done at the data-set level  (i.e., sets of replicates are always kept together). \n\nFor computational efficiency,  array data are first concatenated along their final dimension  (i.e., the replicates dimension) before being passed into the inner network œà,  thereby ensuring that œà is applied to a single large array, rather than multiple small ones. \n\nExpert summary statistics can be incorporated as\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfS(mathbfZ)))\n\nwhere S is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are provided either as a Function that returns a Vector, or as a vector of functions. In the case that œà is set to nothing, only expert summary statistics will be used. See Expert summary statistics for further discussion on their use. \n\nSet-level inputs (e.g., covariates) ùêó can be passed directly into the outer network œï in the following manner: \n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfX))\n\nor, when expert summary statistics are also used,\n\nhatboldsymboltheta(mathbfZ) = boldsymbolphi((mathbfT(mathbfZ) mathbfS(mathbfZ) mathbfX))\n\nThis is done by calling the DeepSet object on a Tuple{Vector{A}, Vector{Vector}}, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level inputs (i.e., one vector for each data set).\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Two data sets containing 3 and 4 replicates\nd = 5  # number of parameters in the model\nn = 10 # dimension of each replicate\nZ = [rand32(n, m) for m ‚àà (3, 4)]\n\n# Construct DeepSet object\nS = samplesize\nd‚Çõ = 1   # dimension of expert summary statistic\nd‚Çú = 16  # dimension of neural summary statistic\nw  = 32  # width of hidden layers\nœà  = Chain(Dense(n, w, relu), Dense(w, d‚Çú, relu))\nœï  = Chain(Dense(d‚Çú + d‚Çõ, w, relu), Dense(w, d))\nds = DeepSet(œà, œï; S = S)\n\n# Apply DeepSet object to data\nds(Z)\n\n# With set-level inputs \nd‚Çì = 2 # dimension of set-level inputs \nœï  = Chain(Dense(d‚Çú + d‚Çõ + d‚Çì, w, relu), Dense(w, d))\nds = DeepSet(œà, œï; S = S)\nX  = [rand32(d‚Çì) for _ ‚àà eachindex(Z)]\nds((Z, X))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.GNNSummary","page":"Architectures","title":"NeuralEstimators.GNNSummary","text":"GNNSummary(propagation, readout)\n\nA graph neural network (GNN) module designed to serve as the inner network œà in the DeepSet representation when the data are graphical (e.g., irregularly observed spatial data).\n\nThe propagation module transforms graph data into a set of hidden-feature graphs. The readout module aggregates these feature graphs into a single hidden feature vector of fixed length. The network œà is then defined as the composition of the propagation and readout modules.\n\nThe data should be stored as a GNNGraph or Vector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain subgraphs corresponding to independent replicates.\n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\nusing Flux: batch\nusing Statistics: mean\n\n# Propagation module\nr  = 1     # dimension of response variable\nn‚Çï = 32    # dimension of node feature vectors\npropagation = GNNChain(GraphConv(r => n‚Çï), GraphConv(n‚Çï => n‚Çï))\n\n# Readout module\nreadout = GlobalPool(mean)\n\n# Inner network\nœà = GNNSummary(propagation, readout)\n\n# Outer network\nd = 3     # output dimension \nw = 64    # width of hidden layer\nœï = Chain(Dense(n‚Çï, w, relu), Dense(w, d))\n\n# DeepSet object \nds = DeepSet(œà, œï)\n\n# Apply to data \ng‚ÇÅ = rand_graph(11, 30, ndata = rand32(r, 11)) \ng‚ÇÇ = rand_graph(13, 40, ndata = rand32(r, 13))\ng‚ÇÉ = batch([g‚ÇÅ, g‚ÇÇ])  \nds(g‚ÇÅ)                # single graph \nds(g‚ÇÉ)                # graph with subgraphs corresponding to independent replicates\nds([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])      # vector of graphs, corresponding to multiple data sets \n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.MLP","page":"Architectures","title":"NeuralEstimators.MLP","text":"MLP(in::Integer, out::Integer; kwargs...)\n\nA traditional fully-connected multilayer perceptron (MLP) with input dimension in and output dimension out.\n\nThe method (mlp::MLP)(x, y) concatenates x and y along their first dimension before passing the result through the neural network. This functionality is used in constructs such as AffineCouplingBlock. \n\nKeyword arguments\n\ndepth::Integer = 2: the number of hidden layers.\nwidth::Integer = 128: the width of each hidden layer.\nactivation::Function = relu: the (non-linear) activation function used in each hidden layer.\noutput_activation::Function = identity: the activation function used in the output layer.\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#User-defined-summary-statistics","page":"Architectures","title":"User-defined summary statistics","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"summarystatistics.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"The following functions correspond to summary statistics that are often useful as user-defined summary statistics in DeepSet objects.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"samplesize\n\nsamplecorrelation\n\nsamplecovariance\n\nNeighbourhoodVariogram","category":"page"},{"location":"API/architectures/#NeuralEstimators.samplesize","page":"Architectures","title":"NeuralEstimators.samplesize","text":"samplesize(Z::AbstractArray)\n\nComputes the sample size of a set of independent realisations Z.\n\nNote that this function is a wrapper around numberreplicates, but this function returns the number of replicates as the eltype of Z, rather than as an integer.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecorrelation","page":"Architectures","title":"NeuralEstimators.samplecorrelation","text":"samplecorrelation(Z::AbstractArray)\n\nComputes the sample correlation matrix, RÃÇ, and returns the vectorised strict lower triangle of RÃÇ.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecorrelation(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.samplecovariance","page":"Architectures","title":"NeuralEstimators.samplecovariance","text":"samplecovariance(Z::AbstractArray)\n\nComputes the sample covariance matrix, Œ£ÃÇ, and returns the vectorised lower triangle of Œ£ÃÇ.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecovariance(z)\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#NeuralEstimators.NeighbourhoodVariogram","page":"Architectures","title":"NeuralEstimators.NeighbourhoodVariogram","text":"NeighbourhoodVariogram(h_max, n_bins) \n(l::NeighbourhoodVariogram)(g::GNNGraph)\n\nComputes the empirical variogram, \n\nhatgamma(h pm delta) = frac12N(h pm delta) sum_(ij) in N(h pm delta) (Z_i - Z_j)^2\n\nwhere N(h pm delta) equiv left(ij)  boldsymbols_i - boldsymbols_j in (h-delta h+delta)right  is the set of pairs of locations separated by a distance within (h-delta h+delta), and cdot denotes set cardinality. \n\nThe distance bins are constructed to have constant width 2delta, chosen based on the maximum distance  h_max to be considered, and the specified number of bins n_bins. \n\nThe input type is a GNNGraph, and the empirical variogram is computed based on the corresponding graph structure.  Specifically, only locations that are considered neighbours will be used when computing the empirical variogram. \n\nExamples\n\nusing NeuralEstimators, Distances, LinearAlgebra\n  \n# Simulate Gaussian spatial data with exponential covariance function \nŒ∏ = 0.1                                 # true range parameter \nn = 250                                 # number of spatial locations \nS = rand(n, 2)                          # spatial locations \nD = pairwise(Euclidean(), S, dims = 1)  # distance matrix \nŒ£ = exp.(-D ./ Œ∏)                       # covariance matrix \nL = cholesky(Symmetric(Œ£)).L            # Cholesky factor \nm = 5                                   # number of independent replicates \nZ = L * randn(n, m)                     # simulated data \n\n# Construct the spatial graph \nr = 0.15                                # radius of neighbourhood set\ng = spatialgraph(S, Z, r = r)\n\n# Construct the variogram object with 10 bins\nnv = NeighbourhoodVariogram(r, 10) \n\n# Compute the empirical variogram \nnv(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Layers","page":"Architectures","title":"Layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In addition to the built-in layers provided by Flux, the following layers may be used when building a neural-network architecture.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DensePositive\n\nPowerDifference\n\nResidualBlock\n\nSpatialGraphConv","category":"page"},{"location":"API/architectures/#NeuralEstimators.DensePositive","page":"Architectures","title":"NeuralEstimators.DensePositive","text":"DensePositive(layer::Dense; g::Function = relu, last_only::Bool = false)\n\nWrapper around the standard Dense layer that ensures positive weights (biases are left unconstrained).\n\nThis layer can be useful for constucting (partially) monotonic neural networks (see, e.g., QuantileEstimatorContinuous).\n\nExamples\n\nusing NeuralEstimators, Flux\n\nl = DensePositive(Dense(5 => 2))\nx = rand32(5, 64)\nl(x)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.PowerDifference","page":"Architectures","title":"NeuralEstimators.PowerDifference","text":"PowerDifference(a, b)\n\nFunction f(x y) = ax - (1-a)y^b for trainable parameters a ‚àà [0, 1] and b > 0.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Generate some data\nd = 5\nK = 10000\nX = randn32(d, K)\nY = randn32(d, K)\nXY = (X, Y)\na = 0.2f0\nb = 1.3f0\nZ = (abs.(a .* X - (1 .- a) .* Y)).^b\n\n# Initialise layer\nf = PowerDifference([0.5f0], [2.0f0])\n\n# Optimise the layer\nloader = Flux.DataLoader((XY, Z), batchsize=32, shuffle=false)\noptim = Flux.setup(Flux.Adam(0.01), f)\nfor epoch in 1:100\n    for (xy, z) in loader\n        loss, grads = Flux.withgradient(f) do m\n            Flux.mae(m(xy), z)\n        end\n        Flux.update!(optim, f, grads[1])\n    end\nend\n\n# Estimates of a and b\nf.a\nf.b\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.ResidualBlock","page":"Architectures","title":"NeuralEstimators.ResidualBlock","text":"ResidualBlock(filter, in => out; stride = 1)\n\nBasic residual block (see here), consisting of two sequential convolutional layers and a skip (shortcut) connection that connects the input of the block directly to the output, facilitating the training of deep networks.\n\nExamples\n\nusing NeuralEstimators\nz = rand(16, 16, 1, 1)\nb = ResidualBlock((3, 3), 1 => 32)\nb(z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.SpatialGraphConv","page":"Architectures","title":"NeuralEstimators.SpatialGraphConv","text":"SpatialGraphConv(in => out, g=relu; args...)\n\nImplements a spatial graph convolution for isotropic spatial processes (Sainsbury-Dale et al., 2025), \n\n boldsymbolh^(l)_j =\n gBig(\n boldsymbolGamma_1^(l) boldsymbolh^(l-1)_j\n +\n boldsymbolGamma_2^(l) barboldsymbolh^(l)_j\n +\n boldsymbolgamma^(l)\n Big)\n quad\n barboldsymbolh^(l)_j = sum_j in mathcalN(j)boldsymbolw^(l)(boldsymbols_j - boldsymbols_j) odot f^(l)(boldsymbolh^(l-1)_j boldsymbolh^(l-1)_j)\n\nwhere boldsymbolh^(l)_j is the hidden feature vector at location boldsymbols_j at layer l, g(cdot) is a non-linear activation function applied elementwise, boldsymbolGamma_1^(l) and boldsymbolGamma_2^(l) are trainable parameter matrices, boldsymbolgamma^(l) is a trainable bias vector, mathcalN(j) denotes the indices of neighbours of boldsymbols_j, boldsymbolw^(l)(cdot) is a (learnable) spatial weighting function, odot denotes elementwise multiplication,  and f^(l)(cdot cdot) is a (learnable) function. \n\nBy default, the function f^(l)(cdot cdot) is modelled using a PowerDifference function.  One may alternatively employ a nonlearnable function, for example, f = (h·µ¢, h‚±º) -> (h·µ¢ - h‚±º).^2,  specified through the keyword argument f.  \n\nThe spatial distances between locations must be stored as an edge feature, as facilitated by spatialgraph().  The input to boldsymbolw^(l)(cdot) is a 1 times n matrix (i.e., a row vector) of spatial distances.  The output of boldsymbolw^(l)(cdot) must be either a scalar; a vector of the same dimension as the feature vectors of the previous layer;  or, if the features vectors of the previous layer are scalars, a vector of arbitrary dimension.  To promote identifiability, the weights are normalised to sum to one (row-wise) within each neighbourhood set.  By default, boldsymbolw^(l)(cdot) is taken to be a multilayer perceptron with a single hidden layer,  although a custom choice for this function can be provided using the keyword argument w. \n\nArguments\n\nin: dimension of input features.\nout: dimension of output features.\ng = relu: activation function.\nbias = true: add learnable bias?\ninit = glorot_uniform: initialiser for boldsymbolGamma_1^(l), boldsymbolGamma_2^(l), and boldsymbolgamma^(l). \nf = nothing\nw = nothing \nw_width = 128 (applicable only if w = nothing): the width of the hidden layer in the MLP used to model boldsymbolw^(l)(cdot cdot). \nw_out = in (applicable only if w = nothing): the output dimension of boldsymbolw^(l)(cdot cdot).  \n\nExamples\n\nusing NeuralEstimators, Flux, GraphNeuralNetworks\n\n# Toy spatial data\nn = 250                # number of spatial locations\nm = 5                  # number of independent replicates\nS = rand(n, 2)         # spatial locations\nZ = rand(n, m)         # data\ng = spatialgraph(S, Z) # construct the graph\n\n# Construct and apply spatial graph convolution layer\nl = SpatialGraphConv(1 => 10)\nl(g)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Output-layers","page":"Architectures","title":"Output layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"activationfunctions.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"In addition to the standard activation functions provided by Flux (e.g., relu, softplus), the following layers can be used at the end of an architecture to ensure valid estimates for certain models. Note that the Flux layer Parallel can be useful for applying several different parameter constraints, as shown in the Univariate data example.","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"note: Layers vs. activation functions\nAlthough we may conceptualise the following types as \"output activation functions\", they should be treated as separate layers included in the final stage of a Flux Chain(). In particular, they cannot be used as the activation function of a Dense layer. ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Compress\n\nCorrelationMatrix\n\nCovarianceMatrix","category":"page"},{"location":"API/architectures/#NeuralEstimators.Compress","page":"Architectures","title":"NeuralEstimators.Compress","text":"Compress(a, b, k = 1)\n\nLayer that compresses its input to be within the range a and b, where each element of a is less than the corresponding element of b.\n\nThe layer uses a logistic function,\n\nl(Œ∏) = a + fracb - a1 + e^-kŒ∏\n\nwhere the arguments a and b together combine to shift and scale the logistic function to the range (a, b), and the growth rate k controls the steepness of the curve.\n\nThe logistic function given here contains an additional parameter, Œ∏‚ÇÄ, which is the input value corresponding to the functions midpoint. In Compress, we fix Œ∏‚ÇÄ = 0, since the output of a randomly initialised neural network is typically around zero.\n\nExamples\n\nusing NeuralEstimators, Flux\n\na = [25, 0.5, -pi/2]\nb = [500, 2.5, 0]\np = length(a)\nK = 100\nŒ∏ = randn(p, K)\nl = Compress(a, b)\nl(Œ∏)\n\nn = 20\nŒ∏ÃÇ = Chain(Dense(n, p), l)\nZ = randn(n, K)\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CorrelationMatrix","page":"Architectures","title":"NeuralEstimators.CorrelationMatrix","text":"CorrelationMatrix(d)\n(object::CorrelationMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector ùêØ ‚àà ‚Ñù·µà to the parameters of an unconstrained d√ód correlation matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d√ód correlation matrix.\n\nThe expected input is a Matrix with T(d-1) = (d-1)d√∑2 rows, where T(d-1) is the (d-1)th triangular number (the number of free parameters in an unconstrained d√ód correlation matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different correlation matrices).\n\nInternally, the layer constructs a valid Cholesky factor ùêã for a correlation matrix, and then extracts the strict lower triangle from the correlation matrix ùêë = ùêãùêã'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the correlation matrix\n\nbeginbmatrix\n1    R‚ÇÅ‚ÇÇ   R‚ÇÅ‚ÇÉ \nR‚ÇÇ‚ÇÅ  1     R‚ÇÇ‚ÇÉ\nR‚ÇÉ‚ÇÅ  R‚ÇÉ‚ÇÇ  1\nendbmatrix\n\nthe rows of the matrix returned by a CorrelationMatrix layer are ordered as\n\nbeginbmatrix\nR‚ÇÇ‚ÇÅ \nR‚ÇÉ‚ÇÅ \nR‚ÇÉ‚ÇÇ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied correlation matrices using vectotril and Symmetric.\n\nSee also CovarianceMatrix.\n\nExamples\n\nusing NeuralEstimators, LinearAlgebra, Flux\n\nd  = 4\nl  = CorrelationMatrix(d)\np  = (d-1)*d√∑2\nŒ∏  = randn(p, 100)\n\n# Returns a matrix of parameters, which can be converted to correlation matrices\nR = l(Œ∏)\nR = map(eachcol(R)) do r\n\tR = Symmetric(cpu(vectotril(r, strict = true)), :L)\n\tR[diagind(R)] .= 1\n\tR\nend\n\n# Obtain the Cholesky factor directly\nL = l(Œ∏, true)\nL = map(eachcol(L)) do x\n\t# Only the strict lower diagonal elements are returned\n\tL = LowerTriangular(cpu(vectotril(x, strict = true)))\n\n\t# Diagonal elements are determined under the constraint diag(L*L') = ùüè\n\tL[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)\n\tL\nend\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CovarianceMatrix","page":"Architectures","title":"NeuralEstimators.CovarianceMatrix","text":"CovarianceMatrix(d)\n(object::CovarianceMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector ùêØ ‚àà ‚Ñù·µà to the parameters of an unconstrained d√ód covariance matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d√ód covariance matrix.\n\nThe expected input is a Matrix with T(d) = d(d+1)√∑2 rows, where T(d) is the dth triangular number (the number of free parameters in an unconstrained d√ód covariance matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different covariance matrices).\n\nInternally, the layer constructs a valid Cholesky factor ùêã and then extracts the lower triangle from the positive-definite covariance matrix ùö∫ = ùêãùêã'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the covariance matrix\n\nbeginbmatrix\nŒ£‚ÇÅ‚ÇÅ  Œ£‚ÇÅ‚ÇÇ  Œ£‚ÇÅ‚ÇÉ \nŒ£‚ÇÇ‚ÇÅ  Œ£‚ÇÇ‚ÇÇ  Œ£‚ÇÇ‚ÇÉ \nŒ£‚ÇÉ‚ÇÅ  Œ£‚ÇÉ‚ÇÇ  Œ£‚ÇÉ‚ÇÉ \nendbmatrix\n\nthe rows of the matrix returned by a CovarianceMatrix are ordered as\n\nbeginbmatrix\nŒ£‚ÇÅ‚ÇÅ \nŒ£‚ÇÇ‚ÇÅ \nŒ£‚ÇÉ‚ÇÅ \nŒ£‚ÇÇ‚ÇÇ \nŒ£‚ÇÉ‚ÇÇ \nŒ£‚ÇÉ‚ÇÉ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied covariance matrices using vectotril and Symmetric.\n\nSee also CorrelationMatrix.\n\nExamples\n\nusing NeuralEstimators, Flux, LinearAlgebra\n\nd = 4\nl = CovarianceMatrix(d)\np = d*(d+1)√∑2\nŒ∏ = randn(p, 50)\n\n# Returns a matrix of parameters, which can be converted to covariance matrices\nŒ£ = l(Œ∏)\nŒ£ = [Symmetric(cpu(vectotril(x)), :L) for x ‚àà eachcol(Œ£)]\n\n# Obtain the Cholesky factor directly\nL = l(Œ∏, true)\nL = [LowerTriangular(cpu(vectotril(x))) for x ‚àà eachcol(L)]\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"NeuralEstimators facilitates a suite of neural methods for parameter inference in scenarios where simulation from the model is feasible. These methods are likelihood-free and amortised, in the sense that, once the neural networks are trained on simulated data, they enable rapid inference across arbitrarily many observed data sets in a fraction of the time required by conventional approaches. ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The package supports neural Bayes estimators, which transform data into point summaries of the posterior distribution; neural posterior estimators, which perform approximate posterior inference via KL-divergence minimisation; and neural ratio estimators, which approximate the likelihood-to-evidence ratio and thereby enable frequentist or Bayesian inference through various downstream algorithms, such as MCMC sampling. ","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"User-friendliness is a central focus of the package, which is designed to minimise \"boilerplate\" code while preserving complete flexibility in the neural-network architecture and other workflow components. The package accommodates any model for which simulation is feasible by allowing users to define their model implicitly through simulated data. A convenient interface for R users is available on CRAN.","category":"page"},{"location":"#Getting-started","page":"NeuralEstimators","title":"Getting started","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the Methodology, see the Overview of the package workflow and the illustrative Examples.","category":"page"},{"location":"#Installation","page":"NeuralEstimators","title":"Installation","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"To install the package, please first install the current stable release of Julia. Then, one may install the current stable version of the package using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Alternatively, one may install the current development version using the command:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(url = \"https://github.com/msainsburydale/NeuralEstimators.jl\")","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use it in your research or other activities, please also use the following citations.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@Manual{,\n    title = {{NeuralEstimators}: Likelihood-Free Parameter Estimation\n      using Neural Networks},\n    author = {Matthew Sainsbury-Dale},\n    year = {2024},\n    note = {R package version 0.1-2},\n    url = {https://CRAN.R-project.org/package=NeuralEstimators},\n    doi = {10.32614/CRAN.package.NeuralEstimators},\n  }\n\n@Article{,\n    title = {Likelihood-Free Parameter Estimation with Neural {B}ayes\n      Estimators},\n    author = {Matthew Sainsbury-Dale and Andrew Zammit-Mangion and\n      Raphael Huser},\n    journal = {The American Statistician},\n    year = {2024},\n    volume = {78},\n    pages = {1--14},\n    doi = {10.1080/00031305.2023.2249522},\n  }","category":"page"},{"location":"#Contributing","page":"NeuralEstimators","title":"Contributing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"If you encounter a bug or have a suggestion, please consider opening an issue or submitting a pull request. Instructions for contributing to the documentation can be found in docs/README.md. When adding functionality to the package, you may wish to add unit tests to the file test/runtests.jl. You can then run these tests locally by executing the following command from the root folder:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"julia --project=. -e \"using Pkg; Pkg.test()\"","category":"page"},{"location":"#Papers-using-NeuralEstimators","page":"NeuralEstimators","title":"Papers using NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Likelihood-free parameter estimation with neural Bayes estimators [paper] [code]\nNeural methods for amortized inference [paper][code]\nNeural Bayes estimators for irregular spatial data using graph neural networks [paper][code]\nNeural Bayes estimators for censored inference with peaks-over-threshold models [paper] [code]\nNeural parameter estimation with incomplete data [paper][code]","category":"page"}]
}
