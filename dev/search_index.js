var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by having the user define their statistical model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code illustrates how a user could formulate code for their own model.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"See also Distributions.jl for a large range of distributions implemented in Julia, and the package RCall for calling R functions within Julia. ","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussianprocess\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Model-specific functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::Matrix, m = 1)\nsimulategaussianprocess(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from a mean-zero Gaussian process.\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or an object from the package GaussianRandomFields.\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nExamples\n\nusing NeuralEstimators\nusing Distances\nusing LinearAlgebra\n\nn = 500\nœÅ = 0.6\nŒΩ = 1.0\nS = rand(n, 2)\n\n# Passing a Cholesky factor:\nD = pairwise(Euclidean(), S, S, dims = 1)\nŒ£ = Symmetric(matern.(D, œÅ, ŒΩ))\nL = cholesky(Œ£).L\nsimulategaussianprocess(L)\n\n# Passing a GaussianRandomField with Cholesky decomposition:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(œÅ, ŒΩ))\ngrf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), S)\nsimulategaussianprocess(grf)\n\n# Passing a GaussianRandomField with circulant embedding (fast but requires regular grid):\npts = range(0.0, 1.0, 20)\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulategaussianprocess(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::Matrix, m = 1)\nsimulateschlather(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from Schlather's max-stable model using the algorithm for approximate simulation given by Schlather (2002).\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or an object from the package GaussianRandomFields.\n\nIf m is not specified, the simulated data are returned as a vector with length equal to the number of spatial locations, n; otherwise, the data are returned as an nxm matrix.\n\nKeyword arguments\n\nC = 3.5: a tuning parameter that controls the accuracy of the algorithm: small C favours computational efficiency, while large C favours accuracy. Schlather (2002) recommends the use of C = 3.\nGumbel = true: flag indicating whether the data should be log-transformed from the unit Fr√©chet scale to the Gumbel scale.\n\nExamples\n\nusing NeuralEstimators\nusing Distances\nusing LinearAlgebra\n\nn = 500\nœÅ = 0.6\nŒΩ = 1.0\nS = rand(n, 2)\n\n# Passing a Cholesky factor:\nD = pairwise(Euclidean(), S, S, dims = 1)\nŒ£ = Symmetric(matern.(D, œÅ, ŒΩ))\nL = cholesky(Œ£).L\nsimulateschlather(L)\n\n# Passing a GaussianRandomField with Cholesky decomposition:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(œÅ, ŒΩ))\ngrf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), S)\nsimulateschlather(grf)\n\n# Passing a GaussianRandomField with circulant embedding (fast but requires regular grid):\npts = range(0.0, 1.0, 20)\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulateschlather(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Spatial-point-processes","page":"Model-specific functions","title":"Spatial point processes","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"maternclusterprocess","category":"page"},{"location":"API/simulation/#NeuralEstimators.maternclusterprocess","page":"Model-specific functions","title":"NeuralEstimators.maternclusterprocess","text":"maternclusterprocess(; Œª=10, Œº=10, r=0.1, xmin=0, xmax=1, ymin=0, ymax=1)\n\nSimulates a Mat√©rn cluster process with density of parent Poisson point process Œª, mean number of daughter points Œº, and radius of cluster disk r, over the simulation window defined by {x/y}min and {x/y}max.\n\nNote that one may also use the R package spatstat using RCall.\n\nExamples\n\nusing NeuralEstimators\n\n# Simulate a realisation from a Mat√©rn cluster process\nS = maternclusterprocess()\n\n# Visualise realisation (requires UnicodePlots)\nusing UnicodePlots\nscatterplot(S[:, 1], S[:, 2])\n\n# Visualise realisations from the cluster process with varying parameters\nn = 250\nŒª = [10, 25, 50, 90]\nŒº = n ./ Œª\nplots = map(eachindex(Œª)) do i\n\tS = maternclusterprocess(Œª = Œª[i], Œº = Œº[i])\n\tscatterplot(S[:, 1], S[:, 2])\nend\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Low-level-functions","page":"Model-specific functions","title":"Low-level functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These low-level functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\nmaternchols","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nFor two points separated by h units, compute the Mat√©rn covariance function, with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance parameter œÉ¬≤.\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright)^nu K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Model-specific functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, œÅ, ŒΩ, œÉ¬≤ = 1; stack = true)\n\nGiven a distance matrix D, constructs the Cholesky factor of the covariance matrix under the Mat√©rn covariance function with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance œÉ¬≤.\n\nProviding vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices D may also be provided.\n\nIf stack = true, the Cholesky factors will be \"stacked\" into a three-dimensional array (this is only possible if all distance matrices in D are the same size).\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra: norm\nn  = 10\nS  = rand(n, 2)\nD  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(S), s‚±º ‚àà eachrow(S)]\nœÅ  = [0.6, 0.5]\nŒΩ  = [0.7, 1.2]\nœÉ¬≤ = [0.2, 0.4]\nmaternchols(D, œÅ, ŒΩ)\nmaternchols([D], œÅ, ŒΩ)\nmaternchols(D, œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤)\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(2n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in various paper, we have developed the following functions for evaluating the density function for several popular distributions. We include these in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::V, L::LT) where {V <: AbstractVector, LT <: LowerTriangular}\ngaussiandensity(y::A, L::LT) where {A <: AbstractArray, LT <: LowerTriangular}\ngaussiandensity(y::A, Œ£::M) where {A <: AbstractArray, M <: AbstractMatrix}\n\nEfficiently computes the density function for y ~ ùëÅ(0, Œ£) for covariance matrix Œ£, and where L is lower Cholesky factor of Œ£.\n\nThe method gaussiandensity(y::A, L::LT) assumes that the last dimension of y contains independent and identically distributed (iid) replicates.\n\nThe log-density is returned if the keyword argument logdensity is true (default).\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà; logdensity = true)\n\nThe bivariate density function for Schlather's max-stable model.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The following packages are used throughout these examples.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators, Flux, Distributions, NamedArrays","category":"page"},{"location":"workflow/examples/#Univariate-data","page":"Examples","title":"Univariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here we develop a neural Bayes estimator for boldsymboltheta equiv (mu sigma) from data Z_1 dots Z_m that are independent and identically distributed realisations from the distribution N(mu sigma^2).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we define a function to sample parameters from the prior. Here, we assume that the parameters are independent a priori and we adopt the marginal priors mu sim N(0 1) and sigma sim IG(3 1). The sampled parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of sampled parameter vectors. Note that below we store the parameters as a named matrix for convenience, but this is not a requirement of the package:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tŒº = rand(Normal(0, 1), K)\n\tœÉ = rand(InverseGamma(3, 1), K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\tŒ∏ = NamedArray(Œ∏)\n\tsetnames!(Œ∏, [\"Œº\", \"œÉ\"], 1)\n\treturn Œ∏\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model with simulated data. In NeuralEstimators, the data are always stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and where the type A depends on the multivariate structure of the data. Since each replicate Z_1 dots Z_m is univariate, A should be a Matrix with d=1 rows and m columns.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Below, we define our simulator given a single parameter vector, and given a matrix of parameter vectors (which simply applies the simulator to each column):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"simulate(Œ∏, m) = Œ∏[\"Œº\"] .+ Œ∏[\"œÉ\"] .* randn32(1, m)\nsimulate(Œ∏::AbstractMatrix, m) = simulate.(eachcol(Œ∏), m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design a neural-network architecture. Since our data Z_1 dots Z_m are replicated, we will use the DeepSet architecture. The outer network (also known as the inference network) is always a fully-connected network. However, the architecture of the inner network (also known as the summary network) depends on the multivariate structure of the data: with unstructured data (i.e., when there is no spatial or temporal correlation within a replicate), we use a fully-connected neural network. This architecture is then used to initialise a PointEstimator object. Note that the architecture can be defined using raw Flux code (see below) or with the helper function initialise_estimator:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"d = 1   # dimension of each replicate\np = 2   # number of parameters in the statistical model\n\nœà = Chain(Dense(d, 32, relu), Dense(32, 32, relu))     # summary network\nœï = Chain(Dense(32, 32, relu), Dense(32, p))           # inference network\narchitecture = DeepSet(œà, œï)\n\nŒ∏ÃÇ = PointEstimator(architecture)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss. We'll train the estimator using 50 independent replicates per parameter configuration. Below, we pass our user-defined functions for sampling parameters and simulating data, but one may also pass parameter or data instances, which will be held fixed during training:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 50\nŒ∏ÃÇ = train(Œ∏ÃÇ, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Since the training stage can be computationally demanding, one may wish to save a trained estimator and load it in later sessions: see Saving and loading neural estimators for details on how this can be done. See also the Regularisation methods that can be easily applied when constructing neural Bayes estimators.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The function assess can be used to assess the trained point estimator. Parametric and non-parametric bootstrap-based uncertainty quantification are facilitated by bootstrap and interval, and this can also be included in the assessment stage through the keyword argument boot:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏_test = sample(1000)\nZ_test = simulate(Œ∏_test, m)\nassessment = assess(Œ∏ÃÇ, Œ∏_test, Z_test, boot = true)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"An Assessment object contains the sampled parameters, the corresponding point estimates, and the corresponding lower and upper bounds of the bootstrap intervals. This object can be used to compute various diagnostics:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"bias(assessment)\nrmse(assessment)\nrisk(assessment)\ncoverage(assessment)\nintervalscore(assessment)\nplot(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Univariate Gaussian example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"As an alternative form of uncertainty quantification, one may approximate a set of marginal posterior quantiles by training a second neural Bayes estimator under the quantileloss function, which allows one to generate approximate marginal posterior credible intervals. This is facilitated with IntervalEstimator which, by default, targets 95% central credible intervals. Below, we use the same base architecture used for point estimation, which is wrapped in a more complex architecture that ensures that the estimated credible intervals are valid (i.e., that the estimated lower bound is always less than the estimated upper bound):","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ‚ÇÇ = IntervalEstimator(architecture)\nŒ∏ÃÇ‚ÇÇ = train(Œ∏ÃÇ‚ÇÇ, sample, simulate, m = m)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The resulting posterior credible-interval estimator can also be assessed with empirical simulation-based methods using the function assess, as we did above for the point estimator. Often, these intervals have better coverage than bootstrap-based intervals.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once a neural Bayes estimator is calibrated, it may be applied to observed data. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ = sample(1)               # true parameters\nZ = simulate(Œ∏, m)          # \"observed\" data\nŒ∏ÃÇ(Z)                        # point estimates\ninterval(bootstrap(Œ∏ÃÇ, Z))   # 95% non-parametric bootstrap intervals\ninterval(Œ∏ÃÇ‚ÇÇ, Z)             # 95% marginal posterior credible intervals","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that one may utilise the GPU above by calling Z = gpu(Z) before applying the estimator.","category":"page"},{"location":"workflow/examples/#Multivariate-data","page":"Examples","title":"Multivariate data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Suppose now that our data now consists of m replicates mathbfZ_1 dots mathbfZ_m of a d-dimensional multivariate distribution. Everything remains as given in the univariate example above, except that we now store the data as a vector of d times m matrices (previously they were stored as 1times m matrices), and the inner network of the DeepSets representation takes a d-dimensional input (previously it took a 1-dimensional input).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Note that, when estimating a full covariance matrix, one may wish to constrain the neural estimator to only produce parameters that imply a valid (i.e., positive definite) covariance matrix. This can be achieved by appending a  CovarianceMatrix layer to the end of the outer network of the DeepSets representation. However, the estimator will often learn to provide valid estimates, even if not constrained to do so.","category":"page"},{"location":"workflow/examples/#Gridded-data","page":"Examples","title":"Gridded data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For data collected over a regular grid, the neural Bayes estimator is based on a convolutional neural network (CNN).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"In these settings, each data set must be stored as a (D + 2)-dimensional array, where D is the dimension of the grid (e.g., D = 1 for time series, D = 2 for two-dimensional spatial grids, etc.). The first D dimensions of the array correspond to the dimensions of the grid; the penultimate dimension stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes, etc.); and the final dimension stores the independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10x15 grid, one would construct an array of dimension 10x15x2x50.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Below, we develop a neural Bayes estimator for the spatial Gaussian process model with exponential covariance function and unknown range parameter. The spatial domain is taken to be the unit square and we adopt the prior theta sim U(0 06).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tŒ∏ = rand(Uniform(0, 0.6), K)\n\treturn Œ∏'\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Below, we give example code for simulating from the statistical model, where the data is collected over a 16x16 grid:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using Distances\nusing LinearAlgebra\n\nfunction simulate(Œ∏, m = 1)\n\n\t# Spatial locations\n\tpts = range(0, 1, length = 16)\n\tS = expandgrid(pts, pts)\n\tn = size(S, 1)\n\n\t# Distance matrix, covariance matrix, and Cholesky factor\n\tD = pairwise(Euclidean(), S, dims = 1)\n\tŒ£ = exp.(-D ./ Œ∏)\n\tL = cholesky(Symmetric(Œ£)).L\n\n\t# Spatial field\n\tZ = L * randn(n)\n\n\t# Reshape to 16x16 image\n\tZ = reshape(Z, 16, 16, 1, 1)\n\n\treturn Z\nend\nsimulate(Œ∏::AbstractMatrix, m) = [simulate(x, m) for x ‚àà eachcol(Œ∏)]","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For data collected over a regular grid, the neural Bayes estimator is based on a convolutional neural network (CNN). For a useful introduction to CNNs, see, for example, Dumoulin and Visin (2016). For a 16x16 grid, one possible architecture is as follows:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"p = 1 # number of parameters in the statistical model\n\n# Summary network\nœà = Chain(\n\tConv((3, 3), 1 => 32, relu),\n\tMaxPool((2, 2)),\n\tConv((3, 3),  32 => 64, relu),\n\tMaxPool((2, 2)),\n\tFlux.flatten\n\t)\n\n# Inference network\nœï = Chain(Dense(256, 64, relu), Dense(64, p))\n\n# DeepSet\narchitecture = DeepSet(œà, œï)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we initialise a point estimator and a posterior credible-interval estimator using our architecture defined above:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ  = PointEstimator(architecture)\nŒ∏ÃÇ‚ÇÇ = IntervalEstimator(architecture)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Now we train the estimators. Since simulation from this statistical model involves Cholesky factorisation, which is moderately expensive with n=256 spatial locations, here we used fixed parameter and data instances during training. See Storing expensive intermediate objects for data simulation for methods that allow one to avoid repeated Cholesky factorisation when performing On-the-fly and just-in-time simulation:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"K = 20000\nŒ∏_train = sample(K)\nŒ∏_val   = sample(K √∑ 10)\nZ_train = simulate(Œ∏_train)\nZ_val   = simulate(Œ∏_val)\n\nŒ∏ÃÇ  = train(Œ∏ÃÇ,  Œ∏_train, Œ∏_val, Z_train, Z_val)\nŒ∏ÃÇ‚ÇÇ = train(Œ∏ÃÇ‚ÇÇ, Œ∏_train, Œ∏_val, Z_train, Z_val)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the estimators have been trained, we assess them using empirical simulation-based methods:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏_test = sample(100)\nZ_test = simulate(Œ∏_test, m)\nassessment = assess([Œ∏ÃÇ, Œ∏ÃÇ‚ÇÇ], Œ∏_test, Z_test)\n\nbias(assessment)\nrmse(assessment)\ncoverage(assessment)\nplot(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"(Image: Gridded spatial Gaussian process example: Estimates vs. truth)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, we can apply our neural Bayes estimators to observed data. Note that when we have a single replicate only (which is often the case in spatial statistics), non-parametric bootstrap is not possible, and we instead use parametric bootstrap:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ = sample(1)                          # true parameter\nZ = simulate(Œ∏, m)                     # \"observed\" data\nŒ∏ÃÇ(Z)                                   # point estimates\ninterval(Œ∏ÃÇ‚ÇÇ, Z)                        # 95% marginal posterior credible intervals\nbs = bootstrap(Œ∏ÃÇ, Œ∏ÃÇ(Z), simulate, m)   # parametric bootstrap intervals\ninterval(bs)                           # 95% parametric bootstrap intervals","category":"page"},{"location":"workflow/examples/#Irregular-spatial-data","page":"Examples","title":"Irregular spatial data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The methodology we illustrate here uses graph neural networks (GNNs), which are implemented in Julia in the package GraphNeuralNetworks.jl. GNN-based estimators parsimoniously model spatial dependence, and they can be applied to data collected over arbitrary spatial locations. Some key steps involve:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Sampling spatial locations to cover a wide range of spatial configurations during the training phase: see maternclusterprocess.\nComputing (spatially-weighted) adjacency matrices: see adjacencymatrix.\nStoring the data as a graph: see GNNGraph.\nConstructing an appropriate architecture: see GNN and WeightedGraphConv.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"For a concrete example, we consider a classical spatial model, the linear Gaussian-Gaussian model,","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z_j = Y(boldsymbols_j) + epsilon_j  j = 1 dots n","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"where boldsymbolZ equiv (Z_1 dots Z_n) are data observed at locations boldsymbols_1 dots boldsymbols_n subset mathcalD, where mathcalD is some spatial domain, Y(cdot) is a spatially-correlated mean-zero Gaussian process, and epsilon_j sim N(0 tau^2), j = 1 dots n is Gaussian white noise with standard deviation tau  0. Here, we use the popular isotropic Mat√©rn covariance function with fixed marginal variance sigma^2 = 1, fixed smoothness parameter nu = 05, and unknown range parameter rho  0. See matern for the specific parametrisation used in this example. Hence, we will construct a neural Bayes estimator for boldsymboltheta equiv (tau rho).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Before proceeding, we load the required packages:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux\nusing GraphNeuralNetworks\nusing Distributions: Uniform\nusing Distances: pairwise, Euclidean\nusing LinearAlgebra\nusing Statistics: mean","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we define a function to sample parameters from the prior. As before, the sampled parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of sampled parameter vectors. We use the priors tau sim U(01 1) and rho sim U(005 05), and we assume that the parameters are independent a priori. Simulation from this model involves the computation of an expensive intermediate object, namely, the Cholesky factor of the covariance matrix. Storing this Cholesky factor for re-use can enable the fast simulation of new data sets (provided that the parameters are held fixed): hence, in this example, we define a class, Parameters, which is a sub-type of ParameterConfigurations, for storing the matrix of parameters and the corresponding intermediate objects needed for data simulation.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"If one wishes to make inference from a single spatial data set only, and this data is collected before the estimator is constructed, then the data can be simulated using the observed spatial locations. However, if one wishes to construct an estimator that is (approximately) Bayes irrespective of the spatial locations, then synthetic spatial locations must be generated during the training phase. If no prior knowledge on the sampling configuration is available, then a wide variety of spatial configurations must be simulated to produce an estimator that is broadly applicable. Below, we use a Mat√©rn cluster process (see maternclusterprocess) for this task (note that the hyper-parameters of this process govern the expected number of locations in each sampled set of spatial locations, and the degree of clustering).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We define two constructors for our Parameters object: one that constructs a Parameters object given a single integer K, and another that constructs a Parameters object given a pre-specified ptimes K matrix of parameters and a set of spatial locations associated with each parameter vector. These constructors will be useful in the workflow below.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"struct Parameters{T} <: ParameterConfigurations\n\tŒ∏::Matrix{T}\n\tlocations\n\tchols\n\tgraphs\nend\n\nfunction Parameters(K::Integer)\n\n\t# Sample parameters from the prior distribution\n\tœÑ = rand(Uniform(0.1, 1.0), K)\n\tœÅ = rand(Uniform(0.05, 0.5), K)\n\n\t# Combine parameters into a pxK matrix\n\tŒ∏ = permutedims(hcat(œÑ, œÅ))\n\n\t# Simulate spatial locations from a cluster process over the unit square\n\tn = rand(Uniform(75, 200), K)\n\tŒª = rand(Uniform(10, 50), K)\n\tlocations = [maternclusterprocess(Œª = Œª[k], Œº = n[k]/Œª[k]) for k ‚àà 1:K]\n\n\tParameters(Œ∏::Matrix, locations)\nend\n\nfunction Parameters(Œ∏::Matrix, locations)\n\n\t# Compute distance matrices and construct the graphs\n\tD = pairwise.(Ref(Euclidean()), locations, locations, dims = 1)\n\tA = adjacencymatrix.(D, 0.15)\n\tgraphs = GNNGraph.(A)\n\n\t# Compute Cholesky factors using the distance matrices\n\tœÅ = Œ∏[2, :]\n\tŒΩ = 0.5\n\tœÉ = 1\n\tchols = maternchols(D, œÅ, ŒΩ, œÉ.^2; stack = false)     \n\n\tParameters(Œ∏, locations, chols, graphs)\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we define a function for simulating from the model given an object of type Parameters. Although here we are constructing an estimator for a single replicate, the code below enables simulation of an arbitrary number of independent replicates m: one may provide a single integer for m, a range of values (e.g., 1:30), or any object that can be sampled using rand(m, K) (e.g., some distribution over the possible sample sizes).","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters::Parameters, m)\n\n\tK = size(parameters, 2)\n\tmÃÉ = rand(m, K)\n\n\tœÑ      = parameters.Œ∏[1, :]\n\tchols  = parameters.chols\n\tg      = parameters.graphs\n\n\t# Z = Folds.map(1:K) do i # use this for parallel simulation\n\tZ = map(1:K) do k\n\t\tL = chols[k][:, :]\n\t\tz = simulategaussianprocess(L, mÃÉ[k])  # simulate a smooth field\n\t\tz = z + œÑ[k] * randn(size(z)...)      # add white noise\n\t\tz = batch([GNNGraph(g[k], ndata = z[:, i, :]') for i ‚àà 1:mÃÉ[k]])\n\t\tz\n\tend\n\n\treturn Z\nend\nsimulate(parameters::Parameters, m::Integer) = simulate(parameters, range(m, m))","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next we construct an appropriate architecture using GNN and WeightedGraphConv. For example, we might construct a point estimator as:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Propagation module\nd = 1      # dimension of response variable\nnh = 32    # dimension of node feature vectors\npropagation = GNNChain(\n\tWeightedGraphConv(d => nh),\n\tWeightedGraphConv(nh => nh),\n\tWeightedGraphConv(nh => nh)\n\t)\n\n# Readout module (using the elementwise average)\nno = nh    # dimension of the final summary vector for each graph\nreadout = GlobalPool(mean)\n\n# Mapping module (use exponential output activation to ensure positive estimates)\np = 2     # number of parameters in the statistical model\nw = 64    # width of layers used for the mapping network œï\nœï = Chain(Dense(no, w, relu), Dense(w, w, relu), Dense(w, p, exp))\n\n# Construct the estimator\nŒ∏ÃÇ = GNN(propagation, readout, œï)\nŒ∏ÃÇ = PointEstimator(Œ∏ÃÇ)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss. We'll train the estimator using a single realisation per parameter configuration (i.e., with m = 1). Below, we use a very small number of epochs and a small number of training parameter vectors to keep the run time of this example low, and this will of course result in a poor estimator: in practice, one may set K to some large value (say, 10,000), and leave epochs unspecified so that training halts only when the risk function ceases to decrease.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulate, m = 1, epochs = 5, K = 500)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Finally, once the neural Bayes estimator has been assessed (as illustrated using assess in the univariate example above), it may be applied to observed data, with bootstrap-based uncertainty quantification facilitated by bootstrap and interval. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"# Generate some toy data\nparameters = Parameters(1)   # sample a single parameter vector\nz = simulate(parameters, 1)  # simulate some data                  \nŒ∏ = parameters.Œ∏             # true parameters used to generate data\nS = parameters.locations     # observed locations\n\n# Point estimates\nŒ∏ÃÇ(z)\n\n# Parametric bootstrap sample and bootstrap confidence interval\nŒ∏ÃÉ = bootstrap(Œ∏ÃÇ, Parameters(Œ∏ÃÇ(z), S), simulate, 1)   \ninterval(Œ∏ÃÉ)  \t\t\t\t\t                ","category":"page"},{"location":"framework/#Framework","page":"Framework","title":"Framework","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"In this section, we provide an overview of point estimation using neural Bayes estimators. For a more detailed discussion on the framework and its implementation, see the paper Likelihood-Free Parameter Estimation with Neural Bayes Estimators","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"A parametric statistical model is a set of probability distributions on a sample space mathcalS, where the probability distributions are parameterised via some p-dimensional parameter vector boldsymboltheta on a parameter space Theta. Suppose that we have data from one such distribution, which we denote as boldsymbolZ. Then, the goal of parameter point estimation is to come up with an estimate of the unknown boldsymboltheta from boldsymbolZ using an estimator,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" hatboldsymboltheta  mathcalS to Theta","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"which is a mapping from the sample space to the parameter space.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Estimators can be constructed within a decision-theoretic framework. Assume that the sample space is mathcalS = mathbbR^n, and consider a non-negative loss function, L(boldsymboltheta hatboldsymboltheta(boldsymbolZ)), which assesses an estimator hatboldsymboltheta(cdot) for a given boldsymboltheta and data set boldsymbolZ sim f(boldsymbolz mid boldsymboltheta), where f(boldsymbolz mid boldsymboltheta) is the probability density function of the data conditional on boldsymboltheta. An estimator's Bayes risk is its loss averaged over all possible parameter values and data realisations,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatboldsymboltheta(cdot))\n equiv int_Theta int_mathcalS  L(boldsymboltheta hatboldsymboltheta(boldsymbolz))f(boldsymbolz mid boldsymboltheta) rmd boldsymbolz rmd Omega(boldsymboltheta)  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"where Omega(cdot) is a prior measure for boldsymboltheta. Any minimiser of the Bayes risk is said to be a Bayes estimator with respect to L(cdot cdot) and Omega(cdot).","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Bayes estimators are theoretically attractive: for example, unique Bayes estimators are admissible and, under suitable regularity conditions and the squared-error loss, are consistent and asymptotically efficient. Further, for a large class of prior distributions, every set of conditions that imply consistency of the maximum likelihood (ML) estimator also imply consistency of Bayes estimators. Importantly, Bayes estimators are not motivated purely by asymptotics: by construction, they are Bayes irrespective of the sample size and model class. Unfortunately, however, Bayes estimators are typically unavailable in closed form for the complex models often encountered in practice. A way forward is to assume a flexible parametric model for hatboldsymboltheta(cdot), and to optimise the parameters within that model in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are also fast to evaluate, usually involving only simple matrix-vector operations.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Let hatboldsymboltheta(boldsymbolZ boldsymbolgamma) denote a neural point estimator, that is, a neural network that returns a point estimate from data boldsymbolZ, where boldsymbolgamma contains the neural-network parameters. Bayes estimators may be approximated with hatboldsymboltheta(cdot boldsymbolgamma^*) by solving the optimisation problem,  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"boldsymbolgamma^*\nequiv\nundersetboldsymbolgammamathrmargmin  r_Omega(hatboldsymboltheta(cdot boldsymbolgamma))","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Typically, r_Omega(cdot) cannot be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of K parameter vectors sampled from the prior Omega(cdot) denoted by vartheta and, for each boldsymboltheta in vartheta, J realisations from f(boldsymbolz mid  boldsymboltheta) collected in mathcalZ_boldsymboltheta,","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":" r_Omega(hatboldsymboltheta(cdot boldsymbolgamma))\n approx\nfrac1K sum_boldsymboltheta in vartheta frac1J sum_boldsymbolz in mathcalZ_boldsymboltheta L(boldsymboltheta hatboldsymboltheta(boldsymbolz boldsymbolgamma))  ","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Note that the above approximation does not involve evaluation, or knowledge, of the likelihood function.","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"The Monte-Carlo-approximated Bayes risk can be straightforwardly minimised with respect to boldsymbolgamma using back-propagation and stochastic gradient descent. For sufficiently flexible architectures, the point estimator targets a Bayes estimator with respect to L(cdot cdot) and Omega(cdot). We therefore call the fitted neural point estimator a  neural Bayes estimator. Like Bayes estimators, neural Bayes estimators target a specific point summary of the posterior distribution. For instance, the absolute-error and squared-error loss functions lead to neural Bayes estimators that approximate the posterior median and mean, respectively.","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Framework","title":"Framework","text":"The neural Bayes estimators is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Framework","title":"Framework","text":"Define the prior, Omega(cdot).\nChoose a loss function, L(cdot cdot), typically the mean-absolute-error or mean-squared-error loss.\nDesign a suitable neural-network architecture for the neural point estimator hatboldsymboltheta(cdot boldsymbolgamma).\nSample parameters from Omega(cdot) to form training/validation/test parameter sets.\nGiven the above parameter sets, simulate data from the model, to form training/validation/test data sets.\nTrain the neural network (i.e., estimate boldsymbolgamma) by minimising the loss function averaged over the training sets. During training, monitor performance and convergence using the validation sets.\nAssess the fitted neural Bayes estimator, hatboldsymboltheta(cdot boldsymbolgamma^*), using the test set.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/#Saving-and-loading-neural-estimators","page":"Advanced usage","title":"Saving and loading neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one often trains an estimator and then saves it for later use. As discussed in the Flux documentation, there are a number of ways to do this. Perhaps the simplest approach is to save the parameters of the neural estimator (e.g., the weights and biases of the neural networks) in a BSON file:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using Flux\nusing BSON: @save, @load\nmodel_state = Flux.state(Œ∏ÃÇ)\n@save \"estimator.bson\" model_state","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, to load the neural estimator at a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"@load \"estimator.bson\" model_state\nFlux.loadmodel!(Œ∏ÃÇ, model_state)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that the estimator Œ∏ÃÇ must be already defined (i.e., only the network parameters are saved, not the architecture). That is, the saved model state should be loaded into a neural estimator with the same architecture as the estimator that we wish to load.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As a convenience, the function train allows for the automatic saving of the neural-network parameters during the training stage, via the argument savepath. Specifically, if savepath is specified, train automatically saves the neural estimator's parameters in the folder savepath; to load them, one may use the following code:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nFlux.loadparams!(Œ∏ÃÇ, loadbestweights(savepath))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, the function loadparams! loads the parameters of the best (as determined by loadbestweights) neural estimator saved in savepath.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution Omega(cdot) may be stored in two ways. Most simply, they can be stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution; this is the approach taken in the example using univariate Gaussian data. Alternatively, they can be stored in a user-defined subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the p times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, mathcalZ_texttrain can be simulated continuously during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Refreshing mathcalZ_texttrain also has an additional computational benefit; data can be simulated \"just-in-time\", in the sense that they can be simulated from a small batch of vartheta_texttrain, used to train the neural estimator, and then removed from memory. This can reduce pressure on memory resources when vartheta_texttrain is very large.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh vartheta_texttrain, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models.  ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with various methods of train.","category":"page"},{"location":"workflow/advancedusage/#Regularisation","page":"Advanced usage","title":"Regularisation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The term regularisation refers to a variety of techniques aimed to reduce overfitting when training a neural network, primarily by discouraging complex models.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One common regularisation technique is known as dropout (Srivastava et al., 2014), implemented in Flux's Dropout layer. Dropout involves temporarily dropping (\"turning off\") a randomly selected set of neurons (along with their connections) at each iteration of the training stage, and this results in a computationally-efficient form of model (neural-network) averaging.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Another class of regularisation techniques involve modifying the loss function. For instance, L‚ÇÅ regularisation (sometimes called lasso regression) adds to the loss a penalty based on the absolute value of the neural-network parameters. Similarly, L‚ÇÇ regularisation (sometimes called ridge regression) adds to the loss a penalty based on the square of the neural-network parameters. Note that these penalty terms are not functions of the data or of the statistical-model parameters that we are trying to infer, and therefore do not modify the Bayes risk or the associated Bayes estimator. These regularisation techniques can be implemented straightforwardly by providing a custom optimiser to train that includes a SignDecay object for L‚ÇÅ regularisation, or a WeightDecay object for L‚ÇÇ regularisation. See the Flux documentation for further details.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"For example, the following code constructs a neural Bayes estimator using dropout and L‚ÇÅ regularisation with penalty coefficient lambda = 10^-4:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using NeuralEstimators\nusing Flux\n\n# Generate data from the model Z ~ N(Œ∏, 1) and Œ∏ ~ N(0, 1)\np = 1       # number of unknown parameters in the statistical model\nm = 5       # number of independent replicates\nd = 1       # dimension of each independent replicate\nK = 3000    # number of training samples\nŒ∏_train = randn(1, K)\nŒ∏_val   = randn(1, K)\nZ_train = [Œº .+ randn(1, m) for Œº ‚àà eachcol(Œ∏_train)]\nZ_val   = [Œº .+ randn(1, m) for Œº ‚àà eachcol(Œ∏_val)]\n\n# Architecture with dropout layers\nœà = Chain(\n\tDense(1, 32, relu),\n\tDropout(0.1),\n\tDense(32, 32, relu),\n\tDropout(0.5)\n\t)     \nœï = Chain(\n\tDense(32, 32, relu),\n\tDropout(0.5),\n\tDense(32, 1)\n\t)           \nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Optimiser with L‚ÇÇ regularisation\noptimiser = Flux.setup(OptimiserChain(SignDecay(1e-4), Adam()), Œ∏ÃÇ)\n\n# Train the estimator\ntrain(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val; optimiser = optimiser)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Note that when the training data and/or parameters are held fixed during training, L‚ÇÇ regularisation with penalty coefficient lambda = 10^-4 is applied by default.","category":"page"},{"location":"workflow/advancedusage/#Combining-learned-and-expert-summary-statistics","page":"Advanced usage","title":"Combining learned and expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"See DeepSet.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as boldsymbolZ^(m) equiv (boldsymbolZ_1 dots boldsymbolZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*)\n=\nbegincases\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_1)  m leq m_1\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where, here, boldsymbolgamma^* equiv (boldsymbolgamma^*_tildem_1 dots boldsymbolgamma^*_tildem_l-1), and where boldsymbolgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatboldsymboltheta(cdot boldsymbolgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise neural estimators are implemented with the struct, PiecewiseEstimator, and their construction is facilitated with trainx.  ","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case, for the neural Bayes estimator, the risk function becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"R(boldsymboltheta hatboldsymboltheta(cdot boldsymbolgamma))\nequiv\nsum_m in mathcalM\nP(M=m)left(int_mathcalS^m  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma))p(boldsymbolZ^(m) mid boldsymboltheta) textd boldsymbolZ^(m)right)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a discrete uniform prior for M, the random variable corresponding to sample size.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters, m::R) where {R <: AbstractRange{I}} where I <: Integer\n\n\t## Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t## Generate K sample sizes from the prior distribution for M\n\tmÃÉ = rand(m, K)\n\n\t## Pseudocode for data simulation\n\tZ = [<simulate mÃÉ[k] iid realisations from the model> for k ‚àà 1:K]\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, setting the argument m in train to be an integer range (e.g., 1:30) will train the neural estimator with the given variable sample sizes.","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"To develop a neural estimator with NeuralEstimators.jl,","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior distribution: the parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of samples (i.e., parameter configurations) in the given parameter set (i.e., training, validation, or test set).\nSimulate data from the assumed model over the parameter sets generated above. These data are stored as a Vector{A}, with each element of the vector associated with one parameter configuration, and where A depends on the representation of the neural estimator (e.g., an Array for CNN-based estimators, or a GNNGraph for GNN-based estimators).\nInitialise a neural network, Œ∏ÃÇ, that will be trained into a neural Bayes estimator (see, e.g., convenience constructor initialise_estimator).  \nTrain Œ∏ÃÇ under the chosen loss function using train.\nAssess Œ∏ÃÇ using assess. The resulting object of class Assessment can be used to assess the estimator with respect to the entire parameter space by estimating the risk function with risk, or used to plot the empirical sampling distribution of the estimator.\nApply Œ∏ÃÇ to observed data (once its performance has been checked in the above step). Bootstrap-based uncertainty quantification is facilitated with bootstrap and interval.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"See the Examples and, once familiar with the basic workflow, see Advanced usage for practical considerations on how to most effectively construct neural estimators.","category":"page"},{"location":"API/core/#Core","page":"Core","title":"Core","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"This page documents the classes and functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these classes and functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to using a neural Bayes estimator to make inference with observed data sets.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Parameters sampled from the prior distribution are stored as a p times K matrix, where p is the number of parameters in the statistical model and K is the number of parameter vectors sampled from the prior distribution.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.\n\nThe user-defined type must have a field Œ∏ that stores the p √ó K matrix of parameters, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.\n\nSee subsetparameters for the generic function for subsetting these objects.\n\nExamples\n\nstruct P <: ParameterConfigurations\n\tŒ∏\n\t# other expensive intermediate objects...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimators facilitates neural estimation for arbitrary statistical models by having the user implicitly define their model via simulated data, either as fixed instances or via a function that simulates data from the statistical model.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The data are always stored as a Vector{A}, where each element of the vector corresponds to a data set of m independent replicates associated with one parameter vector (note that m is arbitrary), and where the type A depends on the multivariate structure of the data:","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"For univariate and unstructured multivariate data, A is a d times m matrix where d is the dimension each replicate (e.g., d=1 for univariate data).\nFor data collected over a regular grid, A is a (D + 2)-dimensional array, where D is the dimension of the grid (e.g., D = 1 for time series, D = 2 for two-dimensional spatial grids, etc.). The first D dimensions of the array correspond to the dimensions of the grid; the penultimate dimension stores the so-called \"channels\" (this dimension is singleton for univariate processes, two for bivariate processes, and so on); and the final dimension stores the independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10x15 grid, one would construct an array of dimension 10x15x2x50.\nFor spatial data collected over irregular spatial locations, A is a GNNGraph with independent replicates (possibly with differing spatial locations) stored as subgraphs using the function batch.","category":"page"},{"location":"API/core/#Estimators","page":"Core","title":"Estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Several classes of neural estimators are available in the package.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The simplest class is PointEstimator, used for constructing arbitrary mappings from the sample space to the parameter space. When constructing a generic point estimator, the user defines the loss function and therefore the Bayes estimator that will be targeted.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Several classes cater for the estimation of marginal posterior quantiles, based on the quantile loss function (see quantileloss()); in particular, see IntervalEstimator and QuantileEstimatorDiscrete for estimating marginal posterior quantiles for a fixed set of probability levels, and QuantileEstimatorContinuous for estimating marginal posterior quantiles with the probability level as an input to the neural network.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"In addition to point estimation, the package also provides the class RatioEstimator for approximating the so-called likelihood-to-evidence ratio. The binary classification problem at the heart of this approach proceeds based on the binary cross-entropy loss.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Users are free to choose the neural-network architecture of these estimators as they see fit (subject to some class-specific requirements), but the package also provides the convenience constructor initialise_estimator().","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimator\n\nPointEstimator\n\nIntervalEstimator\n\nQuantileEstimatorDiscrete\n\nQuantileEstimatorContinuous\n\nRatioEstimator\n\nPiecewiseEstimator","category":"page"},{"location":"API/core/#NeuralEstimators.NeuralEstimator","page":"Core","title":"NeuralEstimators.NeuralEstimator","text":"NeuralEstimator\n\nAn abstract supertype for neural estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PointEstimator","page":"Core","title":"NeuralEstimators.PointEstimator","text":"PointEstimator(deepset::DeepSet)\n\nA neural point estimator, a mapping from the sample space to the parameter space.\n\nThe estimator leverages the DeepSet architecture. The only requirement is that number of output neurons in the final layer of the inference network (i.e., the outer network) is equal to the number of parameters in the statistical model.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.IntervalEstimator","page":"Core","title":"NeuralEstimators.IntervalEstimator","text":"IntervalEstimator(u, v = u; probs = [0.025, 0.975], g::Function = exp)\nIntervalEstimator(u, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)\nIntervalEstimator(u, v, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)\n\nA neural interval estimator which, given data Z, jointly estimates marginal posterior credible intervals based on the probability levels probs.\n\nThe estimator employs a representation that prevents quantile crossing, namely, it constructs marginal posterior credible intervals for each parameter theta_i, i = 1 dots p  of the form,\n\nc_i(u_i(mathbfZ))  c_i(u_i(mathbfZ)) + g(v_i(mathbfZ)))\n\nwhere  mathbfu() equiv (u_1(cdot) dots u_p(cdot)) and mathbfv() equiv (v_1(cdot) dots v_p(cdot)) are neural networks that transform data into p-dimensional vectors; g(cdot) is a monotonically increasing function (e.g., exponential or softplus); and each c_i() is a monotonically increasing function that maps its input to the prior support of theta_i.\n\nThe functions c_i() may be defined by a p-dimensional object of type Compress. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted.\n\nIf only a single neural-network architecture is provided, it will be used for both mathbfu() and mathbfv().\n\nThe return value  when applied to data is a matrix with 2p rows, where the first and second p rows correspond to the lower and upper bounds, respectively.\n\nSee also QuantileEstimatorDiscrete and QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# Generate some toy data\nn = 2   # bivariate data\nm = 100 # number of independent replicates\nZ = rand(n, m)\n\n# prior\np = 3  # number of parameters in the statistical model\nmin_supp = [25, 0.5, -pi/2]\nmax_supp = [500, 2.5, 0]\ng = Compress(min_supp, max_supp)\n\n# Create an architecture\nw = 8  # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nu = DeepSet(œà, œï)\n\n# Initialise the interval estimator\nestimator = IntervalEstimator(u, g)\n\n# Apply the (untrained) interval estimator\nestimator(Z)\ninterval(estimator, Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorDiscrete","page":"Core","title":"NeuralEstimators.QuantileEstimatorDiscrete","text":"QuantileEstimatorDiscrete(v; probs = [0.05, 0.25, 0.5, 0.75, 0.95], g = Flux.softplus)\n\nA neural estimator that jointly estimates a fixed set of marginal posterior quantiles with probability levels tau_1 dots tau_T, controlled by the keyword argument probs.\n\nThe estimator employs a representation that prevents quantile crossing, namely,\n\nbeginaligned\nhatmathbfq^(tau_1)(mathbfZ) = mathbfv^(tau_1)(mathbfZ)\nhatmathbfq^(tau_t)(mathbfZ) = mathbfv^(tau_1)(mathbfZ) + sum_j=2^t g(mathbfv^(tau_j)(mathbfZ)) quad t = 2 dots T\nendaligned\n\nwhere mathbfv^(tau_t)(cdot), t = 1 dots T, are unconstrained neural networks that transform data into p-dimensional vectors, and g(cdot) is a non-negative function (e.g., exponential or softplus) applied elementwise to its arguments. In this implementation, the same neural-network architecture v is used for each mathbfv^(tau_t)(cdot), t = 1 dots T.\n\nNote that one may use a simple PointEstimator and the quantileloss to target a specific quantile.\n\nThe return value  when applied to data is a matrix with pT rows, where the first set of T rows corresponds to the estimated quantiles for the first parameter, the second set of T rows corresponds to the estimated quantiles for the second parameter, and so on.\n\nSee also IntervalEstimator and QuantileEstimatorContinuous.\n\nExamples\n\nusing NeuralEstimators, Flux, Distributions\n\n# Simple model Z|Œ∏ ~ N(Œ∏, 1) with prior Œ∏ ~ N(0, 1)\nd = 1   # dimension of each independent replicate\np = 1   # number of unknown parameters in the statistical model\nm = 30  # number of independent replicates in each data set\nprior(K) = randn32(p, K)\nsimulate(Œ∏, m) = [Œº .+ randn32(d, m) for Œº ‚àà eachcol(Œ∏)]\n\n# Architecture\nœà = Chain(Dense(d, 32, relu), Dense(32, 32, relu))\nœï = Chain(Dense(32, 32, relu), Dense(32, p))\nv = DeepSet(œà, œï)\n\n# Initialise the estimator\nœÑ = [0.05, 0.25, 0.5, 0.75, 0.95]\nqÃÇ = QuantileEstimatorDiscrete(v; probs = œÑ)\n\n# Train the estimator\nqÃÇ = train(qÃÇ, prior, simulate, m = m)\n\n# Closed-form posterior for comparison\nfunction posterior(Z; Œº‚ÇÄ = 0, œÉ‚ÇÄ = 1, œÉ¬≤ = 1)\n\n\t# Parameters of posterior distribution\n\tŒºÃÉ = (1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1 * (Œº‚ÇÄ/œÉ‚ÇÄ^2 + sum(Z)/œÉ¬≤)\n\tœÉÃÉ = sqrt((1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1)\n\n\t# Posterior\n\tNormal(ŒºÃÉ, œÉÃÉ)\nend\n\n# Estimate posterior quantiles for 1000 test data sets\nŒ∏ = prior(1000)\nZ = simulate(Œ∏, m)\nqÃÇ(Z)                                             # neural quantiles\nreduce(hcat, quantile.(posterior.(Z), Ref(œÑ)))   # true quantiles\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimatorContinuous","page":"Core","title":"NeuralEstimators.QuantileEstimatorContinuous","text":"QuantileEstimatorContinuous(deepset::DeepSet)\n\nA neural estimator that estimates marginal posterior tau-quantiles given as input the data mathbfZ and the desired probability level tau  (0 1), therefore taking the form\n\nhatmathbfq(mathbfZ tau) quad tau  (0 1)\n\nThe estimator leverages the DeepSet architecture. The first requirement is that number of input neurons in the first layer of the inference network (i.e., the outer network) is one greater than the number of output neurons in the final layer of the summary network. The second requirement is that the number of output neurons in the final layer of the inference network is equal to the number p of parameters in the statistical model.\n\nAlthough not a requirement, one may employ a (partially) monotonic neural network to prevent quantile crossing (i.e., to ensure that the tau_1-quantile does not exceed the tau_2-quantile for any tau_2  tau_1). There are several ways to construct such a neural network: one simple yet effective approach is to ensure that all weights associated with tau are strictly positive (see, e.g., Cannon, 2018), and this can be done using the DensePositive layer as illustrated in the examples below.\n\nThe return value when applied to data is a matrix with p rows, corresponding to the estimated marginal posterior quantiles for each parameter in the statistical model.\n\nExamples\n\nusing NeuralEstimators, Flux, Distributions, Statistics\n\n# Simple model Z|Œ∏ ~ N(Œ∏, 1) with prior Œ∏ ~ N(0, 1)\nd = 1         # dimension of each independent replicate\np = 1         # number of unknown parameters in the statistical model\nm = 30        # number of independent replicates in each data set\nprior(K) = randn32(p, K)\nsimulateZ(Œ∏, m) = [Œº .+ randn32(d, m) for Œº ‚àà eachcol(Œ∏)]\nsimulateœÑ(K)    = [rand32(1) for k in 1:K]\nsimulate(Œ∏, m)  = simulateZ(Œ∏, m), simulateœÑ(size(Œ∏, 2))\n\n# Architecture: partially monotonic network to preclude quantile crossing\nw = 64  # width of each hidden layer\nq = 16  # number of learned summary statistics\nœà = Chain(\n\tDense(d, w, relu),\n\tDense(w, w, relu),\n\tDense(w, q, relu)\n\t)\nœï = Chain(\n\tDensePositive(Dense(q + 1, w, relu); last_only = true),\n\tDensePositive(Dense(w, w, relu)),\n\tDensePositive(Dense(w, p))\n\t)\ndeepset = DeepSet(œà, œï)\n\n# Initialise the estimator\nqÃÇ = QuantileEstimatorContinuous(deepset)\n\n# Train the estimator\nqÃÇ = train(qÃÇ, prior, simulate, m = m)\n\n# Closed-form posterior for comparison\nfunction posterior(Z; Œº‚ÇÄ = 0, œÉ‚ÇÄ = 1, œÉ¬≤ = 1)\n\n\t# Parameters of posterior distribution\n\tŒºÃÉ = (1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1 * (Œº‚ÇÄ/œÉ‚ÇÄ^2 + sum(Z)/œÉ¬≤)\n\tœÉÃÉ = sqrt((1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1)\n\n\t# Posterior\n\tNormal(ŒºÃÉ, œÉÃÉ)\nend\n\n# Estimate the posterior 0.1-quantile for 1000 test data sets\nŒ∏ = prior(1000)\nZ = simulateZ(Œ∏, m)\nœÑ = 0.1f0\nqÃÇ(Z, œÑ)                        # neural quantiles\nquantile.(posterior.(Z), œÑ)'   # true quantiles\n\n# Estimate several quantiles for a single data set\nz = Z[1]\nœÑ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])\nreduce(vcat, qÃÇ.(Ref(z), œÑ))    # neural quantiles\nquantile.(posterior(z), œÑ)     # true quantiles\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.RatioEstimator","page":"Core","title":"NeuralEstimators.RatioEstimator","text":"RatioEstimator(deepset::DeepSet)\n\nA neural estimator that estimates the likelihood-to-evidence ratio,\n\nr(mathbfZ mathbftheta) equiv p(mathbfZ mid mathbftheta)p(mathbfZ)\n\nwhere p(mathbfZ mid mathbftheta) is the likelihood and p(mathbfZ) is the marginal likelihood, also known as the model evidence.\n\nThe estimator leverages the DeepSet architecture, subject to two requirements. First, the number of input neurons in the first layer of the inference network (i.e., the outer network) must equal the number of output neurons in the final layer of the summary network plus the number of parameters in the statistical model. Second, the number of output neurons in the final layer of the inference network must be equal to one.\n\nThe ratio estimator is trained by solving a relatively straightforward binary classification problem. Specifically, consider the problem of distinguishing dependent parameter‚Äìdata pairs (mathbftheta mathbfZ) sim p(mathbfZ mathbftheta) with class labels Y=1 from independent parameter‚Äìdata pairs (tildemathbftheta tildemathbfZ) sim p(mathbftheta)p(mathbfZ) with class labels Y=0, and where the classes are balanced. Then the Bayes classifier under binary cross-entropy loss is given by\n\nc(mathbfZ mathbftheta) = fracp(mathbfZ mathbftheta)p(mathbfZ mathbftheta) + p(mathbftheta)p(mathbfZ)\n\nand hence,\n\nr(mathbfZ mathbftheta) = fracc(mathbfZ mathbftheta)1 - c(mathbfZ mathbftheta)\n\nFor numerical stability, training is done on the log-scale using log r(mathbfZ mathbftheta) = textlogit(c(mathbfZ mathbftheta)).\n\nWhen applying the estimator to data, by default the likelihood-to-evidence ratio r(mathbfZ mathbftheta) is returned (setting the keyword argument classifier = true will yield class probability estimates). The estimated ratio can then be used in various downstream Bayesian (e.g., Hermans et al., 2020) or Frequentist (e.g., Walchessen et al., 2023) inferential algorithms.\n\nSee also mlestimate and mapestimate for obtaining approximate maximum-likelihood and maximum-a-posteriori estimates, and sampleposterior for obtaining approximate posterior samples.\n\nExamples\n\nusing NeuralEstimators, Flux, Statistics\n\n# Generate data from Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with Œº, œÉ ~ U(0, 1)\np = 2     # number of unknown parameters in the statistical model\nd = 1     # dimension of each independent replicate\nm = 100   # number of independent replicates\n\nprior(K) = rand32(p, K)\nsimulate(Œ∏, m) = Œ∏[1] .+ Œ∏[2] .* randn32(d, m)\nsimulate(Œ∏::AbstractMatrix, m) = simulate.(eachcol(Œ∏), m)\n\n# Architecture\nw = 64 # width of each hidden layer\nq = 2p # number of learned summary statistics\nœà = Chain(\n\tDense(d, w, relu),\n\tDense(w, w, relu),\n\tDense(w, q, relu)\n\t)\nœï = Chain(\n\tDense(q + p, w, relu),\n\tDense(w, w, relu),\n\tDense(w, 1)\n\t)\ndeepset = DeepSet(œà, œï)\n\n# Initialise the estimator\nrÃÇ = RatioEstimator(deepset)\n\n# Train the estimator\nrÃÇ = train(rÃÇ, prior, simulate, m = m)\n\n# Inference with \"observed\" data set\nŒ∏ = prior(1)\nz = simulate(Œ∏, m)[1]\nŒ∏‚ÇÄ = [0.5, 0.5]                           # initial estimate\nmlestimate(rÃÇ, z;  Œ∏‚ÇÄ = Œ∏‚ÇÄ)                # maximum-likelihood estimate\nmapestimate(rÃÇ, z; Œ∏‚ÇÄ = Œ∏‚ÇÄ)                # maximum-a-posteriori estimate\nŒ∏_grid = expandgrid(0:0.01:1, 0:0.01:1)'  # fine gridding of the parameter space\nrÃÇ(z, Œ∏_grid)                              # likelihood-to-evidence ratios over grid\nsampleposterior(rÃÇ, z; Œ∏_grid = Œ∏_grid)    # posterior samples\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator(estimators, changepoints)\n\nCreates a piecewise estimator (Sainsbury-Dale et al., 2024, sec. 2.2.2) from a collection of estimators and sample-size changepoints.\n\nSpecifically, with l estimators and sample-size changepoints m_1  m_2  dots  m_l-1, the piecewise etimator takes the form,\n\nhatmathbftheta(mathbfZ)\n=\nbegincases\nhatmathbftheta_1(mathbfZ)  m leq m_1\nhatmathbftheta_2(mathbfZ)  m_1  m leq m_2\nquad vdots \nhatmathbftheta_l(mathbfZ)  m  m_l-1\nendcases\n\nFor example, given an estimator  hatmathbftheta_1(cdot) trained for small sample sizes (e.g., m ‚â§ 30) and an estimator hatmathbftheta_2(cdot) trained for moderate-to-large sample sizes (e.g., m > 30), we may construct a PiecewiseEstimator that dispatches hatmathbftheta_1(cdot) if m ‚â§ 30 and hatmathbftheta_2(cdot) otherwise.\n\nSee also trainx() for training estimators for a range of sample sizes.\n\nExamples\n\nusing NeuralEstimators, Flux\n\nd = 2  # bivariate data\np = 3  # number of parameters in the statistical model\nw = 8  # width of each hidden layer\n\n# Small-sample estimator\nœà‚ÇÅ = Chain(Dense(d, w, relu), Dense(w, w, relu));\nœï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÅ = PointEstimator(DeepSet(œà‚ÇÅ, œï‚ÇÅ))\n\n# Large-sample estimator\nœà‚ÇÇ = Chain(Dense(d, w, relu), Dense(w, w, relu));\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÇ = PointEstimator(DeepSet(œà‚ÇÇ, œï‚ÇÇ))\n\n# Piecewise estimator with changepoint m=30\nŒ∏ÃÇ = PiecewiseEstimator([Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ], 30)\n\n# Apply the (untrained) piecewise estimator to data\nZ = [rand(d, 1, m) for m ‚àà (10, 50)]\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The function train is used to train a single neural estimator, while the wrapper function trainx is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"train\n\ntrainx","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, sampler::Function, simulator::Function; ...)\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, simulator::Function; ...) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; ...) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\nTrain a neural estimator Œ∏ÃÇ.\n\nThe methods cater for different variants of \"on-the-fly\" simulation. Specifically, a sampler can be provided to continuously sample new parameter vectors from the prior, and a simulator can be provided to continuously simulate new data conditional on the parameters. If provided with specific sets of parameters (Œ∏_train and Œ∏_val) and/or data (Z_train and Z_val), they will be held fixed during training.\n\nIn all methods, the validation parameters and data are held fixed to reduce noise when evaluating the validation risk.\n\nKeyword arguments common to all methods:\n\nloss = mae\nepochs::Integer = 100\nbatchsize::Integer = 32\noptimiser = ADAM()\nsavepath::String = \"\": path to save the neural-network weights during training (as bson files) and other information, such as the risk vs epoch (the risk function evaluated over the training and validation sets are saved in the first and second columns of loss_per_epoch.csv). If savepath is an empty string (default), nothing is saved.\nstopping_epochs::Integer = 5: cease training if the risk doesn't improve in this number of epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nKeyword arguments common to train(Œ∏ÃÇ, sampler, simulator) and train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulator):\n\nm: sample sizes (either an Integer or a collection of Integers). The simulator is called as simulator(Œ∏, m).\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nsimulate_just_in_time::Bool = false: flag indicating whether we should simulate just-in-time, in the sense that only a batchsize number of parameter vectors and corresponding data are in memory at a given time.\n\nKeyword arguments unique to train(Œ∏ÃÇ, sampler, simulator):\n\nK::Integer = 10000: number of parameter vectors in the training set; the size of the validation set is K √∑ 5.\nŒæ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices). If provided, the parameter sampler is called as sampler(K, Œæ); otherwise, the parameter sampler will be called as sampler(K). Can also be provided as xi.\nepochs_per_Œ∏_refresh::Integer = 1: how often to refresh the training parameters. Must be a multiple of epochs_per_Z_refresh. Can also be provided as epochs_per_theta_refresh.\n\nExamples\n\nusing NeuralEstimators, Flux\n\n# parameter sampler\nfunction sampler(K)\n\tŒº = randn(K) # Gaussian prior\n\tœÉ = rand(K)  # Uniform prior\n\tŒ∏ = hcat(Œº, œÉ)'\n\treturn Œ∏\nend\n\n# data simulator\nsimulator(Œ∏_matrix, m) = [Œ∏[1] .+ Œ∏[2] * randn32(1, m) for Œ∏ ‚àà eachcol(Œ∏_matrix)]\n\n# architecture\nd = 1   # dimension of each replicate\np = 2   # number of parameters in the statistical model\nœà = Chain(Dense(1, 32, relu), Dense(32, 32, relu))\nœï = Chain(Dense(32, 32, relu), Dense(32, p))\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# number of independent replicates to use during training\nm = 15\n\n# training: full simulation on-the-fly\nŒ∏ÃÇ = train(Œ∏ÃÇ, sampler, simulator, m = m, epochs = 5)\n\n# training: simulation on-the-fly with fixed parameters\nK = 10000\nŒ∏_train = sampler(K)\nŒ∏_val   = sampler(K √∑ 5)\nŒ∏ÃÇ \t\t = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulator, m = m, epochs = 5)\n\n# training: fixed parameters and fixed data\nZ_train = simulator(Œ∏_train, m)\nZ_val   = simulator(Œ∏_val, m)\nŒ∏ÃÇ \t\t = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 5)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.trainx","page":"Core","title":"NeuralEstimators.trainx","text":"trainx(Œ∏ÃÇ, sampler::Function, simulator::Function, m::Vector{Integer}; ...)\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulator::Function, m::Vector{Integer}; ...)\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, m::Vector{Integer}; ...)\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train::V, Z_val::V; ...) where {V <: AbstractVector{AbstractVector{Any}}}\n\nA wrapper around train() to construct neural estimators for different sample sizes.\n\nThe positional argument m specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if m = [m‚ÇÅ, m‚ÇÇ], the estimator for sample size m‚ÇÇ is pre-trained with the estimator for sample size m‚ÇÅ.\n\nThe method for Z_train and Z_val subsets the data using subsetdata(Z, 1:m·µ¢) for each m·µ¢ ‚àà m. The method for Z_train::V and Z_val::V trains an estimator for each element of Z_train::V and Z_val::V and, hence, it does not need to invoke subsetdata(), which can be slow or difficult to define in some cases (e.g., for graphical data). Note that, in this case, m is inferred from the data.\n\nThe keyword arguments inherit from train(). The keyword arguments epochs, batchsize, stopping_epochs, and optimiser can each be given as vectors. For example, if we are training two estimators, we can use a different number of epochs for each estimator by providing epochs = [epoch‚ÇÅ, epoch‚ÇÇ].\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessment/calibration","page":"Core","title":"Assessment/calibration","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"assess\n\nAssessment\n\ndiagnostics\n\nrisk\n\nbias\n\nrmse\n\ncoverage","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core","title":"NeuralEstimators.assess","text":"assess(estimators, Œ∏, Z)\n\nUsing a collection of estimators, compute estimates from data Z simulated based on true parameter vectors stored in Œ∏.\n\nThe data Z should be a Vector, with each element corresponding to a single simulated data set. If Z contains more data sets than parameter vectors, the parameter matrix Œ∏ will be recycled by horizontal concatenation via the call Œ∏ = repeat(Œ∏, outer = (1, J)) where J = length(Z) √∑ K is the number of simulated data sets and K = size(Œ∏, 2) is the number of parameter vectors.\n\nThe output is of type Assessment; see ?Assessment for details.\n\nKeyword arguments\n\nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided). If Œæ is provided with a field parameter_names, those names will be used.\nŒæ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices). Can also be provided as xi.\nuse_Œæ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses Œæ: if it does, the estimator will be applied as estimator(Z, Œæ). This argument is useful when multiple estimators are provided, only some of which need Œæ; hence, if only one estimator is provided and Œæ is not nothing, use_Œæ is automatically set to true. Can also be provided as use_xi.\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\nExamples\n\nusing NeuralEstimators, Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Generate testing parameters\nK = 100\nŒ∏ = rand32(p, K)\n\n# Data for a single sample size\nm = 30\nZ = [rand32(n, m) for _ ‚àà 1:K];\nassessment = assess(Œ∏ÃÇ, Œ∏, Z);\nrisk(assessment)\n\n# Multiple data sets for each parameter vector\nJ = 5\nZ = repeat(Z, J);\nassessment = assess(Œ∏ÃÇ, Œ∏, Z);\nrisk(assessment)\n\n# With set-level information\nq‚Çì = 2\nœï  = Chain(Dense(w + q‚Çì, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\nx = [rand(q‚Çì) for _ ‚àà eachindex(Z)]\nassessment = assess(Œ∏ÃÇ, Œ∏, (Z, x));\nrisk(assessment)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core","title":"NeuralEstimators.Assessment","text":"Assessment(df::DataFrame, runtime::DataFrame)\n\nA type for storing the output of assess(). The field runtime contains the total time taken for each estimator. The field df is a long-form DataFrame with columns:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size (number of iid replicates) for the given data set\nk:         the index of the parameter vector\nj:         the index of the data set (in the case that multiple data sets are associated with each parameter vector)\n\nNote that if estimator is an IntervalEstimator, the column estimate will be replaced by the columns lower and upper, containing the lower and upper bounds of the interval, respectively.\n\nMultiple Assessment objects can be combined with merge() (used for combining assessments from multiple point estimators) or join() (used for combining assessments from a point estimator and an interval estimator).\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.diagnostics","page":"Core","title":"NeuralEstimators.diagnostics","text":"diagnostics(assessment::Assessment; args...)\n\nComputes all applicable diagnostics.\n\nFor a PointEstimator, the relevant diagnostics are the estimator's bias, rmse, and risk, while for an IntervalEstimator the relevant diagnostics are the coverage and intervalscore.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.risk","page":"Core","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's Bayes risk,\n\nr(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K L(boldsymboltheta^(k) hatboldsymboltheta(boldsymbolZ^(k)))\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nKeyword arguments\n\nloss = (x, y) -> abs(x - y): a binary operator defining the loss function (default absolute-error loss).\naverage_over_parameters::Bool = false: if true, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the loss is averaged over all sample sizes m; otherwise, the loss is averaged over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.bias","page":"Core","title":"NeuralEstimators.bias","text":"bias(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's bias,\n\nrmbias(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_k=1^K hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k)\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.rmse","page":"Core","title":"NeuralEstimators.rmse","text":"rmse(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an estimator's root-mean-squared error,\n\nrmrmse(hatboldsymboltheta(cdot))\napprox\nsqrtfrac1K sum_k=1^K (hatboldsymboltheta(boldsymbolZ^(k)) - boldsymboltheta^(k))^2\n\nwhere boldsymboltheta^(k)  k = 1 dots K denotes a set of K parameter vectors sampled from the prior and, for each k, data boldsymbolZ^(k) are simulated from the statistical model conditional on boldsymboltheta^(k).\n\nThis function inherits the keyword arguments of risk (excluding the argument loss).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.coverage","page":"Core","title":"NeuralEstimators.coverage","text":"coverage(assessment::Assessment; ...)\n\nComputes a Monte Carlo approximation of an interval estimator's expected coverage.\n\nKeyword arguments\n\naverage_over_parameters::Bool = false: if true, the coverage is averaged over all parameters; otherwise (default), it is computed over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the coverage is averaged over all sample sizes m; otherwise, it is computed over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-with-observed-data","page":"Core","title":"Inference with observed data","text":"","category":"section"},{"location":"API/core/#Inference-using-point-estimators","page":"Core","title":"Inference using point estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Inference with a neural Bayes (point) estimator proceeds simply by applying the estimator Œ∏ÃÇ to the observed data Z (possibly containing multiple data sets) in a call of the form Œ∏ÃÇ(Z). To leverage a GPU, simply move the estimator and the data to the GPU using gpu(); see also estimateinbatches() to apply the estimator over batches of data, which can alleviate memory issues when working with a large number of data sets.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"Uncertainty quantification often proceeds through the bootstrap distribution, which is essentially available \"for free\" when bootstrap data sets can be quickly generated; this is facilitated by bootstrap() and interval(). Alternatively, one may approximate a set of low and high marginal posterior quantiles using a specially constructed neural Bayes estimator, which can then be used to construct credible intervals: see IntervalEstimator, QuantileEstimatorDiscrete, and QuantileEstimatorContinuous.  ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"bootstrap\n\ninterval","category":"page"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core","title":"NeuralEstimators.bootstrap","text":"bootstrap(Œ∏ÃÇ, parameters::P, Z) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, parameters::P, simulator, m::Integer; B = 400) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, Z; B = 400, blocks = nothing)\n\nGenerates B bootstrap estimates from an estimator Œ∏ÃÇ.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, Z, whose length implicitly defines B. Alternatively, one may provide a simulator and the desired sample size, in which case the data will be simulated using simulator(parameters, m).\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm aims to produce resampled data sets that are of a similar size to Z, but this can only be achieved exactly if all blocks are equal in length.\n\nThe keyword argument use_gpu is a flag determining whether to use the GPU, if it is available (default true).\n\nThe return type is a p √ó B matrix, where p is the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.interval","page":"Core","title":"NeuralEstimators.interval","text":"interval(Œ∏::Matrix; probs = [0.05, 0.95], parameter_names = nothing)\ninterval(estimator::IntervalEstimator, Z; parameter_names = nothing, use_gpu = true)\n\nCompute a confidence interval based either on a p √ó B matrix Œ∏ of parameters (typically containing bootstrap estimates or posterior draws) with p the number of parameters in the model, or from an IntervalEstimator and data Z.\n\nWhen given Œ∏, the intervals are constructed by compute quantiles with probability levels controlled by the keyword argument probs.\n\nThe return type is a p √ó 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval. The rows of this matrix can be named by passing a vector of strings to the keyword argument parameter_names.\n\nExamples\n\nusing NeuralEstimators\np = 3\nB = 50\nŒ∏ = rand(p, B)\ninterval(Œ∏)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators","page":"Core","title":"Inference using likelihood and likelihood-to-evidence-ratio estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"mlestimate\n\nmapestimate\n\nsampleposterior","category":"page"},{"location":"API/core/#NeuralEstimators.mlestimate","page":"Core","title":"NeuralEstimators.mlestimate","text":"mlestimate(estimator::RatioEstimator, Z; Œ∏‚ÇÄ = nothing, Œ∏_grid = nothing, penalty::Function = Œ∏ -> 1, use_gpu = true)\n\nComputes the (approximate) maximum likelihood estimate given data mathbfZ,\n\nargmax_mathbftheta ell(mathbftheta  mathbfZ)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function derived from estimator.\n\nIf a vector Œ∏‚ÇÄ of initial parameter estimates is given, the approximate likelihood is maximised by gradient descent. Otherwise, if a matrix of parameters Œ∏_grid is given, the approximate likelihood is maximised by grid search.\n\nA maximum penalised likelihood estimate,\n\nargmax_mathbftheta ell(mathbftheta  mathbfZ) + log p(mathbftheta)\n\ncan be obtained by specifying the keyword argument penalty that defines the penalty term p(mathbftheta).\n\nSee also mapestimate() for computing (approximate) maximum a posteriori estimates.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.mapestimate","page":"Core","title":"NeuralEstimators.mapestimate","text":"mapestimate(estimator::RatioEstimator, Z; Œ∏‚ÇÄ = nothing, Œ∏_grid = nothing, prior::Function = Œ∏ -> 1, use_gpu = true)\n\nComputes the (approximate) maximum a posteriori estimate given data mathbfZ,\n\nargmax_mathbftheta ell(mathbftheta  mathbfZ) + log p(mathbftheta)\n\nwhere ell(cdot  cdot) denotes the approximate log-likelihood function derived from estimator, and p(mathbftheta) denotes the prior density function controlled through the keyword argument prior (by default, a uniform prior is used).\n\nIf a vector Œ∏‚ÇÄ of initial parameter estimates is given, the approximate posterior density is maximised by gradient descent. Otherwise, if a matrix of parameters Œ∏_grid is given, the approximate posterior density is maximised by grid search.\n\nSee also mlestimate() for computing (approximate) maximum likelihood estimates.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.sampleposterior","page":"Core","title":"NeuralEstimators.sampleposterior","text":"sampleposterior(estimator::RatioEstimator, Z, N::Integer = 1000; Œ∏_grid, prior::Function = Œ∏ -> 1f0)\n\nSamples from the approximate posterior distribution p(mathbftheta mid mathbfZ) implied by estimator.\n\nThe positional argument N controls the size of the posterior sample.\n\nThe keyword agument Œ∏_grid requires a (fine) gridding of the parameter space, given as a matrix with p rows, with p the number of parameters in the statistical model.\n\nThe prior distribution p(mathbftheta) is controlled through the keyword argument prior (by default, a uniform prior is used).\n\n\n\n\n\n","category":"function"},{"location":"API/activationfunctions/#Output-activation-functions","page":"Output activation functions","title":"Output activation functions","text":"","category":"section"},{"location":"API/activationfunctions/","page":"Output activation functions","title":"Output activation functions","text":"Order = [:type, :function]\nPages   = [\"activationfunctions.md\"]","category":"page"},{"location":"API/activationfunctions/","page":"Output activation functions","title":"Output activation functions","text":"In addition to the standard activation functions provided by Flux, the following layers can be used at the end of an architecture, to act as output activation functions that ensure valid estimates for certain models. NB: Although we refer to the following objects as \"activation functions\", they should be treated as layers that are included in the final stage of a Flux Chain(). ","category":"page"},{"location":"API/activationfunctions/","page":"Output activation functions","title":"Output activation functions","text":"Compress\n\nCorrelationMatrix\n\nCovarianceMatrix","category":"page"},{"location":"API/activationfunctions/#NeuralEstimators.Compress","page":"Output activation functions","title":"NeuralEstimators.Compress","text":"Compress(a, b, k = 1)\n\nLayer that compresses its input to be within the range a and b, where each element of a is less than the corresponding element of b.\n\nThe layer uses a logistic function,\n\nl(Œ∏) = a + fracb - a1 + e^-kŒ∏\n\nwhere the arguments a and b together combine to shift and scale the logistic function to the range (a, b), and the growth rate k controls the steepness of the curve.\n\nThe logistic function given here contains an additional parameter, Œ∏‚ÇÄ, which is the input value corresponding to the functions midpoint. In Compress, we fix Œ∏‚ÇÄ = 0, since the output of a randomly initialised neural network is typically around zero.\n\nExamples\n\nusing NeuralEstimators, Flux\n\na = [25, 0.5, -pi/2]\nb = [500, 2.5, 0]\np = length(a)\nK = 100\nŒ∏ = randn(p, K)\nl = Compress(a, b)\nl(Œ∏)\n\nn = 20\nŒ∏ÃÇ = Chain(Dense(n, p), l)\nZ = randn(n, K)\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/activationfunctions/#NeuralEstimators.CorrelationMatrix","page":"Output activation functions","title":"NeuralEstimators.CorrelationMatrix","text":"CorrelationMatrix(d)\n(object::CorrelationMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector ùêØ ‚àà ‚Ñù·µà to the parameters of an unconstrained d√ód correlation matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d√ód correlation matrix.\n\nThe expected input is a Matrix with T(d-1) = (d-1)d√∑2 rows, where T(d-1) is the (d-1)th triangular number (the number of free parameters in an unconstrained d√ód correlation matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different correlation matrices).\n\nInternally, the layer constructs a valid Cholesky factor ùêã for a correlation matrix, and then extracts the strict lower triangle from the correlation matrix ùêë = ùêãùêã'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the correlation matrix\n\nbeginbmatrix\n1    R‚ÇÅ‚ÇÇ   R‚ÇÅ‚ÇÉ \nR‚ÇÇ‚ÇÅ  1     R‚ÇÇ‚ÇÉ\nR‚ÇÉ‚ÇÅ  R‚ÇÉ‚ÇÇ  1\nendbmatrix\n\nthe rows of the matrix returned by a CorrelationMatrix layer are ordered as\n\nbeginbmatrix\nR‚ÇÇ‚ÇÅ \nR‚ÇÉ‚ÇÅ \nR‚ÇÉ‚ÇÇ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied correlation matrices using vectotril and Symmetric.\n\nSee also CovarianceMatrix.\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra\nusing Flux\n\nd  = 4\nl  = CorrelationMatrix(d)\np  = (d-1)*d√∑2\nŒ∏  = randn(p, 100)\n\n# Returns a matrix of parameters, which can be converted to correlation matrices\nR = l(Œ∏)\nR = map(eachcol(R)) do r\n\tR = Symmetric(cpu(vectotril(r, strict = true)), :L)\n\tR[diagind(R)] .= 1\n\tR\nend\n\n# Obtain the Cholesky factor directly\nL = l(Œ∏, true)\nL = map(eachcol(L)) do x\n\t# Only the strict lower diagonal elements are returned\n\tL = LowerTriangular(cpu(vectotril(x, strict = true)))\n\n\t# Diagonal elements are determined under the constraint diag(L*L') = ùüè\n\tL[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)\n\tL\nend\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"API/activationfunctions/#NeuralEstimators.CovarianceMatrix","page":"Output activation functions","title":"NeuralEstimators.CovarianceMatrix","text":"CovarianceMatrix(d)\n(object::CovarianceMatrix)(x::Matrix, cholesky::Bool = false)\n\nTransforms a vector ùêØ ‚àà ‚Ñù·µà to the parameters of an unconstrained d√ód covariance matrix or, if cholesky = true, the lower Cholesky factor of an unconstrained d√ód covariance matrix.\n\nThe expected input is a Matrix with T(d) = d(d+1)√∑2 rows, where T(d) is the dth triangular number (the number of free parameters in an unconstrained d√ód covariance matrix), and the output is a Matrix of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different covariance matrices).\n\nInternally, the layer constructs a valid Cholesky factor ùêã and then extracts the lower triangle from the positive-definite covariance matrix ùö∫ = ùêãùêã'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering: for example, when modelling the covariance matrix\n\nbeginbmatrix\nŒ£‚ÇÅ‚ÇÅ  Œ£‚ÇÅ‚ÇÇ  Œ£‚ÇÅ‚ÇÉ \nŒ£‚ÇÇ‚ÇÅ  Œ£‚ÇÇ‚ÇÇ  Œ£‚ÇÇ‚ÇÉ \nŒ£‚ÇÉ‚ÇÅ  Œ£‚ÇÉ‚ÇÇ  Œ£‚ÇÉ‚ÇÉ \nendbmatrix\n\nthe rows of the matrix returned by a CovarianceMatrix are ordered as\n\nbeginbmatrix\nŒ£‚ÇÅ‚ÇÅ \nŒ£‚ÇÇ‚ÇÅ \nŒ£‚ÇÉ‚ÇÅ \nŒ£‚ÇÇ‚ÇÇ \nŒ£‚ÇÉ‚ÇÇ \nŒ£‚ÇÉ‚ÇÉ \nendbmatrix\n\nwhich means that the output can easily be transformed into the implied covariance matrices using vectotril and Symmetric.\n\nSee also CorrelationMatrix.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing LinearAlgebra\n\nd = 4\nl = CovarianceMatrix(d)\np = d*(d+1)√∑2\nŒ∏ = randn(p, 50)\n\n# Returns a matrix of parameters, which can be converted to covariance matrices\nŒ£ = l(Œ∏)\nŒ£ = [Symmetric(cpu(vectotril(x)), :L) for x ‚àà eachcol(Œ£)]\n\n# Obtain the Cholesky factor directly\nL = l(Œ∏, true)\nL = [LowerTriangular(cpu(vectotril(x))) for x ‚àà eachcol(L)]\nL[1] * L[1]'\n\n\n\n\n\n","category":"type"},{"location":"API/summarystatistics/#User-defined-summary-statistics","page":"User-defined summary statistics","title":"User-defined summary statistics","text":"","category":"section"},{"location":"API/summarystatistics/","page":"User-defined summary statistics","title":"User-defined summary statistics","text":"Order = [:type, :function]\nPages   = [\"summarystatistics.md\"]","category":"page"},{"location":"API/summarystatistics/","page":"User-defined summary statistics","title":"User-defined summary statistics","text":"The following functions correspond to summary statistics that are often useful as user-defined summary statistics in DeepSet objects.","category":"page"},{"location":"API/summarystatistics/","page":"User-defined summary statistics","title":"User-defined summary statistics","text":"samplesize\n\nsamplecorrelation\n\nsamplecovariance","category":"page"},{"location":"API/summarystatistics/#NeuralEstimators.samplesize","page":"User-defined summary statistics","title":"NeuralEstimators.samplesize","text":"samplesize(Z::AbstractArray)\n\nComputes the sample size of a set of independent realisations Z.\n\nNote that this function is a wrapper around numberreplicates, but this function returns the number of replicates as the eltype of Z, rather than as an integer.\n\n\n\n\n\n","category":"function"},{"location":"API/summarystatistics/#NeuralEstimators.samplecorrelation","page":"User-defined summary statistics","title":"NeuralEstimators.samplecorrelation","text":"samplecorrelation(Z::AbstractArray)\n\nComputes the sample correlation matrix, RÃÇ, and returns the vectorised strict lower triangle of RÃÇ.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecorrelation(z)\n\n\n\n\n\n","category":"function"},{"location":"API/summarystatistics/#NeuralEstimators.samplecovariance","page":"User-defined summary statistics","title":"NeuralEstimators.samplecovariance","text":"samplecovariance(Z::AbstractArray)\n\nComputes the sample covariance matrix, Œ£ÃÇ, and returns the vectorised lower triangle of Œ£ÃÇ.\n\nExamples\n\n# 5 independent replicates of a 3-dimensional vector\nz = rand(3, 5)\nsamplecovariance(z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"Order = [:type, :function]\nPages   = [\"utility.md\"]","category":"page"},{"location":"API/utility/#Core","page":"Miscellaneous","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"numberreplicates\n\nsubsetdata\n\nsubsetparameters","category":"page"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Miscellaneous","title":"NeuralEstimators.numberreplicates","text":"numberofreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Miscellaneous","title":"NeuralEstimators.subsetdata","text":"subsetdata(Z::V, m) where {V <: AbstractArray{A}} where {A <: Any}\nsubsetdata(Z::A, m) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, m) where {G <: AbstractGraph}\n\nSubsets m replicates from data Z.\n\nNote that subsetdata is slow for graphical data, and one should consider using a method of train that does not require the data to be subsetted when working with graphical data: use numberreplicates to check that the training and validation data sets are equally replicated, which prevents the invocation of subsetdata. Note also that subsetdata only applies to vectors of batched graphs.\n\nIf the user is working with data that is not covered by the default methods, simply overload subsetdata with the appropriate type for Z.\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nd = 1  # dimension of the response variable\nn = 5  # number of observations in each realisation\nm = 6  # number of replicates in each data set\nK = 2  # number of data sets\n\n# Array data\nZ = [rand(n, d, m) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ‚àà 1:m]) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates from each data set\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Miscellaneous","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Utility-functions","page":"Miscellaneous","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"adjacencymatrix\n\ncontainertype\n\nencodedata\n\nestimateinbatches\n\nexpandgrid\n\ninitialise_estimator\n\nloadbestweights\n\nremovedata\n\nrowwisenorm\n\nstackarrays\n\nvectotril","category":"page"},{"location":"API/utility/#NeuralEstimators.adjacencymatrix","page":"Miscellaneous","title":"NeuralEstimators.adjacencymatrix","text":"adjacencymatrix(M::Matrix, k::Integer; maxmin::Bool = false)\nadjacencymatrix(M::Matrix, r::Float)\nadjacencymatrix(M::Matrix, r::Float, k::Integer)\n\nComputes a spatially weighted adjacency matrix from M based on either the k-nearest neighbours of each location; all nodes within a disc of radius r; or, if both r and k are provided, a random set of k neighbours with a disc of radius r.\n\nIf maxmin=false (default) the k-nearest neighbours are chosen based on all points in the graph. If maxmin=true, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the k-nearest neighbours amongst the points that have already appeared in the ordering.\n\nIf M is a square matrix, it is treated as a distance matrix; otherwise, it should be an n x d matrix, where n is the number of spatial locations and d is the spatial dimension (typically d = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that the maxmin ordering currently requires a set of spatial locations (not a distance matrix).\n\nBy convention, we consider a location to neighbour itself and, hence, k-neighbour methods will yield k+1 neighbours for each location. Note that one may use dropzeros!() to remove these self-loops from the constructed adjacency matrix (see below).\n\nExamples\n\nusing NeuralEstimators\nusing Distances\nusing SparseArrays\n\nn = 100\nd = 2\nS = rand(n, d)\nk = 10\nr = 0.1\n\n# Memory efficient constructors (avoids constructing the full distance matrix D)\nadjacencymatrix(S, k)\nadjacencymatrix(S, k; maxmin = true)\nadjacencymatrix(S, r)\nadjacencymatrix(S, r, k)\n\n# Construct from full distance matrix D\nD = pairwise(Euclidean(), S, S, dims = 1)\nadjacencymatrix(D, k)\nadjacencymatrix(D, r)\nadjacencymatrix(D, r, k)\n\n# Removing self-loops so that a location is not its own neighbour\nadjacencymatrix(S, k) |> dropzeros!\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.containertype","page":"Miscellaneous","title":"NeuralEstimators.containertype","text":"containertype(A::Type)\ncontainertype(::Type{A}) where A <: SubArray\ncontainertype(a::A) where A\n\nReturns the container type of its argument.\n\nIf given a SubArray, returns the container type of the parent array.\n\nExamples\n\na = rand(3, 4)\ncontainertype(a)\ncontainertype(typeof(a))\n[containertype(x) for x ‚àà eachcol(a)]\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.encodedata","page":"Miscellaneous","title":"NeuralEstimators.encodedata","text":"encodedata(Z::A; fixed_constant::T = zero(T)) where {A <: AbstractArray{Union{Missing, T}, N}} where T, N\n\nFor data Z with missing entries, returns an augmented data set (U, W) where W encodes the missingness pattern as an indicator vector and U is the original data Z with missing entries replaced by a fixed_constant.\n\nThe indicator vector W is stored in the second-to-last dimension of Z, which should be a singleton. If the second-to-last dimension is not singleton, then two singleton dimensions will be added to the array, and W will be stored in the new second-to-last dimension.\n\nExamples\n\nusing NeuralEstimators\n\n# Generate some missing data\nZ = rand(16, 16, 1, 1)\nZ = removedata(Z, 0.25)\t # remove 25% of the data\n\n# Encode the data\nUW = encodedata(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.estimateinbatches","page":"Miscellaneous","title":"NeuralEstimators.estimateinbatches","text":"estimateinbatches(Œ∏ÃÇ, z, Œ∏ = nothing; batchsize::Integer = 32, use_gpu::Bool = true, kwargs...)\n\nApply the estimator Œ∏ÃÇ on minibatches of z (and optionally parameter vectors or other set-level information Œ∏) of size batchsize.\n\nThis can prevent memory issues that can occur with large data sets, particularly on the GPU.\n\nMinibatching will only be done if there are multiple data sets in z; this will be inferred by z being a vector, or a tuple whose first element is a vector.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Miscellaneous","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.initialise_estimator","page":"Miscellaneous","title":"NeuralEstimators.initialise_estimator","text":"initialise_estimator(p::Integer; ...)\ninitialise_estimator(p::Integer, data_type::String; ...)\n\nInitialise a neural estimator for a statistical model with p unknown parameters.\n\nThe estimator is couched in the DeepSets framework (see DeepSet) so that it can be applied to data sets containing an arbitrary number of independent replicates (including the special case of a single replicate).\n\nNote also that the user is free to initialise their neural estimator however they see fit using arbitrary Flux code; see here for Flux's API reference.\n\nFinally, the method with positional argument data_typeis a wrapper that allows one to specify the type of their data (either \"unstructured\", \"gridded\", or \"irregular_spatial\").\n\nKeyword arguments\n\narchitecture::String: for unstructured multivariate data, one may use a densely-connected neural network (\"DNN\"); for data collected over a grid, a convolutional neural network (\"CNN\"); and for graphical or irregular spatial data, a graphical neural network (\"GNN\").\nd::Integer = 1: for unstructured multivariate data (i.e., when architecture = \"DNN\"), the dimension of the data (e.g., d = 3 for trivariate data); otherwise, if architecture ‚àà [\"CNN\", \"GNN\"], the argument d controls the number of input channels (e.g., d = 1 for univariate spatial processes).\nestimator_type::String = \"point\": the type of estimator; either \"point\" or \"interval\".\ndepth = 3: the number of hidden layers; either a single integer or an integer vector of length two specifying the depth of the inner (summary) and outer (inference) network of the DeepSets framework.\nwidth = 32: a single integer or an integer vector of length sum(depth) specifying the width (or number of convolutional filters/channels) in each hidden layer.\nactivation::Function = relu: the (non-linear) activation function of each hidden layer.\nactivation_output::Function = identity: the activation function of the output layer.\nvariance_stabiliser::Union{Nothing, Function} = nothing: a function that will be applied directly to the input, usually to stabilise the variance.\nkernel_size = nothing: (applicable only to CNNs) a vector of length depth[1] containing integer tuples of length D, where D is the dimension of the convolution (e.g., D = 2 for two-dimensional convolution).\nweight_by_distance::Bool = false: (applicable only to GNNs) flag indicating whether the estimator will weight by spatial distance; if true, a WeightedGraphConv layer is used in the propagation module; otherwise, a regular GraphConv layer is used.\n\nExamples\n\n## DNN, GNN, 1D CNN, and 2D CNN for a statistical model with two parameters:\np = 2\ninitialise_estimator(p, architecture = \"DNN\")\ninitialise_estimator(p, architecture = \"GNN\")\ninitialise_estimator(p, architecture = \"CNN\", kernel_size = [10, 5, 3])\ninitialise_estimator(p, architecture = \"CNN\", kernel_size = [(10, 10), (5, 5), (3, 3)])\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Miscellaneous","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nReturns the weights of the neural network saved as 'best_network.bson' in the given path.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.removedata","page":"Miscellaneous","title":"NeuralEstimators.removedata","text":"removedata(Z::Array, I·µ§::Vector{Integer})\nremovedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)\nremovedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)\n\nReplaces elements of Z with missing.\n\nThe simplest method accepts a vector of integers I·µ§ that give the specific indices of the data to be removed.\n\nAlterntivaly, there are two methods available to generate data that are missing completely at random (MCAR).\n\nFirst, a vector p may be given that specifies the proportion of missingness for each element in the response vector. Hence, p should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If prevent_complete_missing = true, no replicates will contain 100% missingness (note that this can slightly alter the effective values of p).\n\nSecond, if an integer n is provided, all replicates will contain n observations after the data are removed. If fixed_pattern = true, the missingness pattern is fixed for all replicates. If contiguous_pattern = true, the data will be removed in a contiguous block. If variable_proportion = true, the proportion of missingness will vary across replicates, with each replicate containing between 1 and n observations after data removal, sampled uniformly (note that variable_proportion overrides fixed_pattern).\n\nThe return type is Array{Union{T, Missing}}.\n\nExamples\n\nd = 5           # dimension of each replicate\nm = 2000        # number of replicates\nZ = rand(d, m)  # simulated data\n\n# Passing a desired proportion of missingness\np = rand(d)\nremovedata(Z, p)\n\n# Passing a desired final sample size\nn = 3  # number of observed elements of each replicate: must have n <= d\nremovedata(Z, n)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.rowwisenorm","page":"Miscellaneous","title":"NeuralEstimators.rowwisenorm","text":"rowwisenorm(A)\n\nComputes the row-wise norm of a matrix A.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Miscellaneous","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.vectotril","page":"Miscellaneous","title":"NeuralEstimators.vectotril","text":"vectotril(v; strict = false)\nvectotriu(v; strict = false)\n\nConverts a vector v of length d(d+1)2 (a triangular number) into a d  d lower or upper triangular matrix.\n\nIf strict = true, the matrix will be strictly lower or upper triangular, that is, a (d+1)  (d+1) triangular matrix with zero diagonal.\n\nNote that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if v is a GPU array. Note also that the return type is not of type Triangular matrix (i.e., the zeros are materialised) since Traingular matrices are not always compatible with other GPU operations.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nn = d*(d+1)√∑2\nv = collect(range(1, n))\nvectotril(v)\nvectotriu(v)\nvectotril(v; strict = true)\nvectotriu(v; strict = true)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Other","page":"Miscellaneous","title":"Other","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"EM","category":"page"},{"location":"API/utility/#NeuralEstimators.EM","page":"Miscellaneous","title":"NeuralEstimators.EM","text":"EM(simulateconditional::Function, MAP::Function, Œ∏‚ÇÄ = nothing)\n\nA type that implements the Monte Carlo variant of the expectation-maximisation (EM) algorithm, which at lth iteration finds the value of ùõâ that maximises\n\nŒ£‚Çï·¥¥ ‚Ñì(ùõâ  ùêô‚ÇÅ  ùêô‚ÇÇÀ° ∞) + log œÄ·¥¥(ùõâ)\n\nwhere ‚Ñì(‚ãÖ) is the complete-data log-likelihood function, ùêô ‚â° (ùêô‚ÇÅ', ùêô‚ÇÇ')' denotes the complete data with ùêô‚ÇÅ and ùêô‚ÇÇ the observed and missing components, respectively, the replicate ùêô‚ÇÇÀ° ∞, h = 1, ‚Ä¶, H, is sampled from the conditional probability distribution of ùêô‚ÇÇ given ùêô‚ÇÅ and the previous estimates ùõâÀ°‚Åª¬π, and œÄ·¥¥(‚ãÖ) ‚â° {œÄ(‚ãÖ)}·¥¥  is a concentrated version of the original prior density.\n\nFields\n\nThe function simulateconditional should be of the form,\n\nsimulateconditional(Z::A, Œ∏; nsims::Integer = 1) where {A <: AbstractArray{Union{Missing, T}}} where T\n\nand the completed-data Z should be returned in whatever form is appropriate to be passed to the MAP estimator as MAP(Z). For example, if the data are gridded and the MAP is a neural MAP estimator based on a CNN architecture, then Z should be returned as a four-dimensional array.\n\nNote that the MAP estimator should return the joint posterior mode; therefore, a neural MAP estimator should be trained under (a surrogate for) the joint 0-1 loss function (see kpowerloss).\n\nThe starting values Œ∏‚ÇÄ should be a vector, which can be provided either during construction of the EM object, or when applying the EM object to data (see below). The starting values given in a function call take precedence over those stored in the object.\n\nMethods\n\nOnce constructed, obects of type EM can be applied to data via the methods,\n\n(em::EM)(Z::A, Œ∏‚ÇÄ::Union{Nothing, Vector} = nothing; ...) where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n(em::EM)(Z::V, Œ∏‚ÇÄ::Union{Nothing, Vector, Matrix} = nothing; ...) where {V <: AbstractVector{A}} where {A <: AbstractArray{Union{Missing, T}, N}} where {T, N}\n\nwhere Z is the complete data containing the observed data and Missing values. Note that the second method caters for the case that one has multiple data sets. The keyword arguments are:\n\nniterations::Integer = 50: the maximum number of iterations.\nnsims::Integer = 1: the number of conditional replicates used to approximate the conditional expectation.\nŒæ = nothing: model information needed for conditional simulation (e.g., distance matrices) or in the MAP estimator.\nuse_Œæ_in_simulateconditional::Bool = false: if set to true, the conditional simulator is called as simulateconditional(Z, Œ∏, Œæ; nsims = nsims).\nuse_Œæ_in_MAP::Bool = false: if set to true, the MAP estimator is applied to the conditionally-completed data as MAP(Z, Œæ).\nœµ = 0.01: tolerance used to assess convergence; The algorithm if the relative change in parameter values from successive iterations is less than œµ.\nreturn_iterates: if true, the estimate at each iteration of the algorithm is returned; otherwise, only the final estimate is returned.\nuse_gpu::Bool = true\nverbose::Bool = false\n\n\n\n\n\n","category":"type"},{"location":"API/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"In addition to the standard loss functions provided by Flux (e.g., mae, mse, etc.), NeuralEstimators provides the following loss functions.","category":"page"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"kpowerloss\n\nquantileloss\n\nintervalscore","category":"page"},{"location":"API/loss/#NeuralEstimators.kpowerloss","page":"Loss functions","title":"NeuralEstimators.kpowerloss","text":"kpowerloss(Œ∏ÃÇ, y, k; agg = mean, joint = true, safeorigin = true, œµ = 0.1)\n\nFor k ‚àà (0, ‚àû), the k-th power absolute-distance loss,\n\nL(Œ∏ Œ∏) = Œ∏ - Œ∏·µè\n\ncontains the squared-error, absolute-error, and 0-1 loss functions as special cases (the latter obtained in the limit as k ‚Üí 0). It is Lipschitz continuous iff k = 1, convex iff k ‚â• 1, and strictly convex iff k > 1: it is quasiconvex for all k > 0.\n\nIf joint = true, the L‚ÇÅ norm is computed over each parameter vector, so that the resulting Bayes estimator is the mode of the joint posterior distribution; otherwise, the Bayes estimator is the vector containing the modes of the marginal posterior distributions.\n\nIf safeorigin = true, the loss function is modified to avoid pathologies around the origin, so that the resulting loss function behaves similarly to the absolute-error loss in the œµ-interval surrounding the origin.\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.quantileloss","page":"Loss functions","title":"NeuralEstimators.quantileloss","text":"quantileloss(Œ∏ÃÇ, Œ∏, œÑ; agg = mean)\nquantileloss(Œ∏ÃÇ, Œ∏, œÑ::Vector; agg = mean)\n\nThe asymmetric quantile loss function,\n\n  L(Œ∏ Œ∏ œÑ) = (Œ∏ - Œ∏)(ùïÄ(Œ∏ - Œ∏  0) - œÑ)\n\nwhere œÑ ‚àà (0, 1) is a probability level and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes œÑ as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in Œ∏ÃÇ is assumed to be pr, where p is the number of parameters and r is the number probability levels in œÑ (i.e., the length of œÑ).\n\nExamples\n\np = 1\nK = 10\nŒ∏ = rand(p, K)\nŒ∏ÃÇ = rand(p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, 0.1)\n\nŒ∏ÃÇ = rand(3p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, [0.1, 0.5, 0.9])\n\np = 2\nŒ∏ = rand(p, K)\nŒ∏ÃÇ = rand(p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, 0.1)\n\nŒ∏ÃÇ = rand(3p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, [0.1, 0.5, 0.9])\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.intervalscore","page":"Loss functions","title":"NeuralEstimators.intervalscore","text":"intervalscore(l, u, Œ∏, Œ±; agg = mean)\nintervalscore(Œ∏ÃÇ, Œ∏, Œ±; agg = mean)\nintervalscore(assessment::Assessment; average_over_parameters::Bool = false, average_over_sample_sizes::Bool = true)\n\nGiven an interval [l, u] with nominal coverage 100√ó(1-Œ±)%  and true value Œ∏, the interval score is defined by\n\nS(l u Œ∏ Œ±) = (u - l) + 2Œ±¬π(l - Œ∏)ùïÄ(Œ∏  l) + 2Œ±¬π(Œ∏ - u)ùïÄ(Œ∏  u)\n\nwhere Œ± ‚àà (0, 1) and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes a single value Œ∏ÃÇ assumes that Œ∏ÃÇ is a matrix with 2p rows, where p is the number of parameters in the statistical model. Then, the first and second set of p rows will be used as l and u, respectively.\n\nFor further discussion, see Section 6 of Gneiting, T. and Raftery, A. E. (2007), \"Strictly proper scoring rules, prediction, and estimation\", Journal of the American statistical Association, 102, 359‚Äì378.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#Architectures","page":"Architectures","title":"Architectures","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Order = [:type, :function]\nPages   = [\"architectures.md\"]","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"Although the user is free to construct their neural estimator however they see fit (i.e., using arbitrary Flux code), NeuralEstimators provides several useful architectures described below that are specifically relevant to neural estimation. See also the convenience constructor initialise_estimator.  ","category":"page"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DeepSet\n\nGNN","category":"page"},{"location":"API/architectures/#NeuralEstimators.DeepSet","page":"Architectures","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, a; S = nothing)\nDeepSet(œà, œï; a::String = \"mean\", S = nothing)\n\nThe DeepSets representation,\n\nŒ∏(ùêô) = œï(ùêì(ùêô))\t‚ÄÇ\t‚ÄÇùêì(ùêô) = ùêö(œà(ùêô·µ¢)  i = 1  m)\n\nwhere ùêô ‚â° (ùêô‚ÇÅ', ‚Ä¶, ùêô‚Çò')' are independent replicates from the statistical model, œà and œï are neural networks, and a is a permutation-invariant function. Expert summary statistics can be incorporated as,\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùêí(ùêô)))\n\nwhere S is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are typically provided either as a Function that returns a Vector, or a vector of such functions.\n\nTo ensure that the architecture is agnostic to the sample size m, the aggregation function a must aggregate over the replicates. It can be specified as a positional argument of type Function, or as a keyword argument with permissible values \"mean\", \"sum\", and \"logsumexp\".\n\nDeepSet objects act on data of type Vector{A}, where each element of the vector is associated with one data set (i.e., one set of independent replicates from the statistical model), and where the type A depends on the form of the data and the chosen architecture for œà. As a rule of thumb, when A is an array, the replicates are stored in the final dimension. The final dimension is usually the 'batch' dimension, but batching with DeepSet objects is done at the data set level (i.e., sets of replicates are batched together). For example, with gridded spatial data and œà a CNN, A should be a 4-dimensional array, with the replicates stored in the 4·µó ∞ dimension.\n\nSet-level information, ùê±, that is not a function of the data can be passed directly into the outer network œï in the following manner,\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùê±))\t‚ÄÇ\t‚ÄÇ\n\nor, in the case that expert summary statistics are also used,\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùêí(ùêô) ùê±))\t‚ÄÇ\n\nThis is done by providing a Tuple{Vector{A}, Vector{Vector}}, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level information (i.e., one vector for each data set).\n\nExamples\n\nusing NeuralEstimators, Flux\n\nn = 10 # dimension of each replicate\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nS = samplesize\nq‚Çõ = 1  # dimension of expert summary statistic\nq‚Çú = 16 # dimension of neural summary statistic\nw = 16 # width of each hidden layer\nœà = Chain(Dense(n, w, relu), Dense(w, q‚Çú, relu));\nœï = Chain(Dense(q‚Çú + q‚Çõ, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï, S = S)\n\n# Toy data\nZ = [rand32(n, m) for m ‚àà (3, 4)]; # two data sets containing 3 and 4 replicates\n\n# Apply the data\nŒ∏ÃÇ(Z)\n\n# Inference with set-level information\nq‚Çì = 2 # dimension of set-level vector\nœï  = Chain(Dense(q‚Çú + q‚Çõ + q‚Çì, w, relu), Dense(w, p));\nŒ∏ÃÇ  = DeepSet(œà, œï; S = S)\nx  = [rand32(q‚Çì) for _ ‚àà eachindex(Z)]\nŒ∏ÃÇ((Z, x))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.GNN","page":"Architectures","title":"NeuralEstimators.GNN","text":"GNN(propagation, readout, œï, a; S = nothing)\nGNN(propagation, readout, œï; a::String = \"mean\", S = nothing)\n\nA graph neural network (GNN) designed for parameter point estimation.\n\nThe propagation module transforms graphical input data into a set of hidden-feature graphs; the readout module aggregates these feature graphs into a single hidden feature vector of fixed length; the function a(‚ãÖ) is a permutation-invariant aggregation function, and œï is a neural network. Expert, user-defined summary statistics S can also be utilised, as described in DeepSet.\n\nThe data should be stored as a GNNGraph or Vector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing Flux: batch\nusing GraphNeuralNetworks\nusing Statistics: mean\n\n# Propagation module\nd = 1      # dimension of response variable\nnh = 32    # dimension of node feature vectors\npropagation = GNNChain(GraphConv(d => nh), GraphConv(nh => nh), GraphConv(nh => nh))\n\n# Readout module (using \"universal pooling\")\nnt = 64   # dimension of the summary vector for each node\nno = 128  # dimension of the final summary vector for each graph\nreadout = UniversalPool(Dense(nh, nt), Dense(nt, no))\n\n# Alternative readout module (using the elementwise average)\n# readout = GlobalPool(mean); no = nh\n\n# Mapping module\np = 3     # number of parameters in the statistical model\nw = 64    # width of layers used for the mapping network œï\nœï = Chain(Dense(no, w, relu), Dense(w, w, relu), Dense(w, p))\n\n# Construct the estimator\nŒ∏ÃÇ = GNN(propagation, readout, œï)\n\n# Apply the estimator to:\n# \t1. a single graph,\n# \t2. a single graph with sub-graphs (corresponding to independent replicates), and\n# \t3. a vector of graphs (corresponding to multiple spatial data sets).\ng‚ÇÅ = rand_graph(11, 30, ndata=rand(d, 11))\ng‚ÇÇ = rand_graph(13, 40, ndata=rand(d, 13))\ng‚ÇÉ = batch([g‚ÇÅ, g‚ÇÇ])\nŒ∏ÃÇ(g‚ÇÅ)\nŒ∏ÃÇ(g‚ÇÉ)\nŒ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Layers","page":"Architectures","title":"Layers","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures","title":"Architectures","text":"DensePositive\n\nWeightedGraphConv\n\nUniversalPool","category":"page"},{"location":"API/architectures/#NeuralEstimators.DensePositive","page":"Architectures","title":"NeuralEstimators.DensePositive","text":"DensePositive(layer::Dense, g::Function)\nDensePositive(layer::Dense; g::Function = Flux.relu)\n\nWrapper around the standard Dense layer that ensures positive weights (biases are left unconstrained).\n\nThis layer can be useful for constucting (partially) monotonic neural networks (see, e.g., [QuantileEstimatorContinuous])(@ref).\n\nExamples\n\nusing NeuralEstimators, Flux\n\nlayer = DensePositive(Dense(5 => 2))\nx = rand32(5, 64)\nlayer(x)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.WeightedGraphConv","page":"Architectures","title":"NeuralEstimators.WeightedGraphConv","text":"WeightedGraphConv(in => out, œÉ=identity; aggr=mean, bias=true, init=glorot_uniform)\n\nSame as regular GraphConv layer, but where the neighbours of a node are weighted by their spatial distance to that node.\n\nArguments\n\nin: The dimension of input features.\nout: The dimension of output features.\nœÉ: Activation function.\naggr: Aggregation operator for the incoming messages (e.g. +, *, max, min, and mean).\nbias: Add learnable bias.\ninit: Weights' initializer.\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\n\n# Construct a spatially-weighted adjacency matrix based on k-nearest neighbours\n# with k = 5, and convert to a graph with random (uncorrelated) dummy data:\nn = 100\nS = rand(n, 2)\nd = 1 # dimension of each observation (univariate data here)\nA = adjacencymatrix(S, 5)\nZ = GNNGraph(A, ndata = rand(d, n))\n\n# Construct the layer and apply it to the data to generate convolved features\nlayer = WeightedGraphConv(d => 16)\nlayer(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.UniversalPool","page":"Architectures","title":"NeuralEstimators.UniversalPool","text":"UniversalPool(œà, œï)\n\nPooling layer (i.e., readout layer) from the paper 'Universal Readout for Graph Convolutional Neural Networks'. It takes the form,\n\nmathbfV = œï(G¬π sum_sin G œà(mathbfh_s))\n\nwhere mathbfV denotes the summary vector for graph G, mathbfh_s denotes the vector of hidden features for node s in G, and œà and œï are dense neural networks.\n\nSee also the pooling layers available from GraphNeuralNetworks.jl.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing GraphNeuralNetworks\nusing Graphs: random_regular_graph\n\n# Construct an input graph G\nn_h     = 16  # dimension of each feature node\nn_nodes = 10\nn_edges = 4\nG = GNNGraph(random_regular_graph(n_nodes, n_edges), ndata = rand(n_h, n_nodes))\n\n# Construct the pooling layer\nn_t = 32  # dimension of the summary vector for each node\nn_v = 64  # dimension of the final summary vector V\nœà = Dense(n_h, n_t)\nœï = Dense(n_t, n_v)\npool = UniversalPool(œà, œï)\n\n# Apply the pooling layer\npool(G)\n\n\n\n\n\n","category":"type"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural estimators are neural networks that transform data into parameter point estimates. They are likelihood free, substantially faster than classical methods, and can be designed to be approximate Bayes estimators. Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is essentially available \"for free\" with a neural estimator, or by training a neural estimator to approximate a set of marginal posterior quantiles.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The Julia package NeuralEstimators facilitates the development of neural estimators in a user-friendly manner. It caters for arbitrary models by having the user implicitly define their model via simulated data. This makes the development of neural estimators particularly straightforward for models with existing implementations (possibly in other programming languages, e.g., R or python). A convenient interface for R users is available here.","category":"page"},{"location":"#Getting-started","page":"NeuralEstimators","title":"Getting started","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Install NeuralEstimators using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(url = \"https://github.com/msainsburydale/NeuralEstimators.jl\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the details of the Framework, see the Examples.","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use it in your research or other activities, please also use the following citation.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@article{,\n\tauthor = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Rapha√´l},\n\ttitle = {Likelihood-Free Parameter Estimation with Neural {B}ayes Estimators},\n\tjournal = {The American Statistician},\n\tyear = {2024},\n\tvolume = {78},\n\tpages = {1--14},\n\tdoi = {10.1080/00031305.2023.2249522},\n\turl = {https://doi.org/10.1080/00031305.2023.2249522}\n}","category":"page"},{"location":"#Papers-using-NeuralEstimators","page":"NeuralEstimators","title":"Papers using NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Likelihood-free parameter estimation with neural Bayes estimators [paper]\nNeural Bayes estimators for censored inference with peaks-over-threshold models [paper]\nNeural Bayes estimators for irregular spatial data using graph neural networks [paper]\nModern extreme value statistics for Utopian extremes [paper]","category":"page"}]
}
