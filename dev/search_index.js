var documenterSearchIndex = {"docs":
[{"location":"API/simulation/#Model-specific-functions","page":"Model-specific functions","title":"Model-specific functions","text":"","category":"section"},{"location":"API/simulation/#Data-simulators","page":"Model-specific functions","title":"Data simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"The philosophy of NeuralEstimators is to cater for arbitrary statistical models by having the user define their statistical model implicitly through simulated data. However, the following functions have been included as they may be helpful to others, and their source code provide an example for how a user could formulate code for their own model. If you've developed similar functions that you think may be helpful to others, please get in touch or make a pull request.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"simulategaussianprocess\n\nsimulateschlather","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Model-specific functions","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::Matrix, m = 1)\nsimulategaussianprocess(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from a mean-zero Gaussian process.\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or a GaussianRandomField object grf.\n\nExamples\n\nusing NeuralEstimators\n\nn  = 500\nS  = rand(n, 2)\nœÅ  = 0.6\nŒΩ  = 1.0\n\n# Passing GaussianRandomField object:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(œÅ, ŒΩ))\ngrf = GaussianRandomField(cov, Cholesky(), S)\nsimulategaussianprocess(grf)\n\n# Passing Cholesky factors directly as matrices:\nL = grf.data\nsimulategaussianprocess(L)\n\n# Circulant embedding, which is fast but can on only be used on grids:\npts = 1.0:50.0\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulategaussianprocess(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Model-specific functions","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::Matrix, m = 1)\nsimulateschlather(grf::GaussianRandomField, m = 1)\n\nSimulates m independent and identically distributed (i.i.d.) realisations from Schlather's max-stable model using the algorithm for approximate simulation given by Schlather (2002), \"Models for stationary max-stable random fields\", Extremes, 5:33-44.\n\nAccepts either the lower Cholesky factor L associated with a Gaussian process or a GaussianRandomField object grf.\n\nKeyword arguments\n\nC = 3.5: a tuning parameter that controls the accuracy of the algorithm: small C favours computational efficiency, while large C favours accuracy. Schlather (2002) recommends the use of C = 3.\nGumbel = true: flag indicating whether the data should be log-transformed from the unit Fr√©chet scale to the Gumbel scale.\n\nExamples\n\nusing NeuralEstimators\n\nn  = 500\nS  = rand(n, 2)\nœÅ  = 0.6\nŒΩ  = 1.0\n\n# Passing GaussianRandomField object:\nusing GaussianRandomFields\ncov = CovarianceFunction(2, Matern(œÅ, ŒΩ))\ngrf = GaussianRandomField(cov, Cholesky(), S)\nsimulateschlather(grf)\n\n# Passing Cholesky factors directly as matrices:\nL = grf.data\nsimulateschlather(L)\n\n# Circulant embedding, which is fast but can on only be used on grids:\npts = 1.0:50.0\ngrf = GaussianRandomField(cov, CirculantEmbedding(), pts, pts, minpadding = 100)\nsimulateschlather(grf)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Low-level-functions","page":"Model-specific functions","title":"Low-level functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"These low-level functions may be of use for various models.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"matern\n\nmaternchols","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Model-specific functions","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nFor two points separated by h units, compute the Mat√©rn covariance function, with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance parameter œÉ¬≤.\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright)^nu K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.maternchols","page":"Model-specific functions","title":"NeuralEstimators.maternchols","text":"maternchols(D, œÅ, ŒΩ, œÉ¬≤ = 1; stack = true)\n\nGiven a distance matrix D, constructs the Cholesky factor of the covariance matrix under the Mat√©rn covariance function with range parameter œÅ, smoothness parameter ŒΩ, and marginal variance œÉ¬≤.\n\nProviding vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices D may also be provided.\n\nIf stack = true, the Cholesky factors will be \"stacked\" into a three-dimensional array (this is only possible if all distance matrices in D are the same size).\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra: norm\nn  = 10\nS  = rand(n, 2)\nD  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(S), s‚±º ‚àà eachrow(S)]\nœÅ  = [0.6, 0.5]\nŒΩ  = [0.7, 1.2]\nœÉ¬≤ = [0.2, 0.4]\nmaternchols(D, œÅ, ŒΩ)\nmaternchols(D, œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤)\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\nSÃÉ  = rand(2n, 2)\nDÃÉ  = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(SÃÉ), s‚±º ‚àà eachrow(SÃÉ)]\nmaternchols([D, DÃÉ], œÅ, ŒΩ, œÉ¬≤; stack = false)\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Density-functions","page":"Model-specific functions","title":"Density functions","text":"","category":"section"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"Density functions are not needed in the workflow of NeuralEstimators. However, as part of a series of comparison studies between neural estimators and likelihood-based estimators given in the manuscript, we have developed the following density functions, and we include them in NeuralEstimators to cater for the possibility that they may be of use in future comparison studies.","category":"page"},{"location":"API/simulation/","page":"Model-specific functions","title":"Model-specific functions","text":"gaussiandensity\n\nschlatherbivariatedensity","category":"page"},{"location":"API/simulation/#NeuralEstimators.gaussiandensity","page":"Model-specific functions","title":"NeuralEstimators.gaussiandensity","text":"gaussiandensity(y::V, L; logdensity = true) where {V <: AbstractVector{T}} where T\ngaussiandensity(y::A, Œ£; logdensity = true) where {A <: AbstractArray{T, N}} where {T, N}\n\nEfficiently computes the density function for y ~ ùëÅ(0, Œ£), with L the lower Cholesky factor of the covariance matrix Œ£.\n\nThe method gaussiandensity(y::A, Œ£) assumes that the last dimension of y corresponds to the independent-replicates dimension, and it exploits the fact that we need to compute the Cholesky factor L for these independent replicates once only.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.schlatherbivariatedensity","page":"Model-specific functions","title":"NeuralEstimators.schlatherbivariatedensity","text":"schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà; logdensity = true)\n\nThe bivariate density function for Schlather's max-stable model, as given in Huser (2013, pg. 231‚Äì232).\n\nHuser, R. (2013). Statistical Modeling and Inference for Spatio-Temporal Ex- tremes. PhD thesis, Swiss Federal Institute of Technology, Lausanne, Switzerland.\n\n\n\n\n\n","category":"function"},{"location":"workflow/examples/#Examples","page":"Examples","title":"Examples","text":"","category":"section"},{"location":"workflow/examples/#Univariate-Gaussian-data","page":"Examples","title":"Univariate Gaussian data","text":"","category":"section"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Here, we develop a neural Bayes estimator for boldsymboltheta equiv (mu sigma) from data Z_1 dots Z_m that are independent and identically distributed according to a N(mu sigma^2) distribution. We'll use the priors mu sim N(0 1) and sigma sim U(01 1), and we assume that the parameters are independent a priori.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Before proceeding, we load the required packages:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"using NeuralEstimators\nusing Flux\nusing Distributions\nimport NeuralEstimators: simulate","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"First, we define a function to sample parameters from the prior. The sampled parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of sampled parameter vectors.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function sample(K)\n\tŒº = rand(Normal(0, 1), K)\n\tœÉ = rand(Uniform(0.1, 1), K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\tŒ∏ = Float32.(Œ∏)\n\treturn Œ∏\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we implicitly define the statistical model with simulated data. The data are stored as a Vector{A}, where each element of the vector is associated with one parameter vector, and where A depends on the representation of the neural estimator. Since our data is replicated, we will use the Deep Sets framework and, since each replicate is univariate, we will use a dense neural network (DNN) for the inner network. Since the inner network is a DNN, A should be a sub-type of AbstractArray, with the independent replicates stored in the final dimension.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"function simulate(parameters, m)\n\tZ = [Œ∏[1] .+ Œ∏[2] .* randn(Float32, 1, m) for Œ∏ ‚àà eachcol(parameters)]\n\treturn Z\nend","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"We now design architectures for the inner and outer neural networks, boldsymbolpsi(cdot) and boldsymbolphi(cdot) respectively, in the Deep Sets framework, and initialise the neural estimator as a PointEstimator object.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"p = 2   # number of parameters in the statistical model\nw = 32  # width of each layer\n\nœà = Chain(Dense(1, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p))\narchitecture = DeepSet(œà, œï)\n\nŒ∏ÃÇ = PointEstimator(architecture)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Next, we train the neural estimator using train, here using the default absolute-error loss. We'll train the estimator using 15 independent replicates per parameter configuration. Below, we pass our user-defined functions for sampling parameters and simulating data, but one may also pass parameter or data instances, which will be held fixed during training; see train.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"m = 15\nŒ∏ÃÇ = train(Œ∏ÃÇ, sample, simulate, m = m, epochs = 30)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"To test the accuracy of the resulting neural Bayes estimator, we use the function assess, which can be used to assess the performance of the estimator (or multiple estimators) over a range of sample sizes. Note that, in this example, we trained the neural estimator using a single sample size, m = 15, and hence the estimator will not necessarily be optimal for other sample sizes; see Variable sample sizes for approaches that one could adopt if data sets with varying sample size are envisaged.","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Œ∏     = sample(1000)\nZ     = [simulate(Œ∏, m) for m ‚àà (5, 10, 15, 20, 30)]\nassessment = assess([Œ∏ÃÇ], Œ∏, Z)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"The returned object is an object of type Assessment, which contains the true parameters and their corresponding estimates, and the time taken to compute the estimates for each sample size and each estimator. The risk function may be computed using the function risk:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"risk(assessment)","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"It is often helpful to visualise the empirical sampling distribution of an estimator for a particular parameter configuration and a particular sample size. This can be done by providing assess with J data sets simulated under a particular parameter configuration (below facilitated with the pre-defined method simulate(parameters, m, J::Integer), which wraps the method of simulate that we defined earlier), and then plotting the estimates contained in the long-form DataFrame in the resulting Assessment object:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"J = 100\nŒ∏ = sample(1)\nZ = [simulate(Œ∏, m, J)]\nassessment = assess([Œ∏ÃÇ], Œ∏, Z)  ","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Once the neural Bayes estimator has been assessed, it may then be applied to observed data, with parametric/non-parametric bootstrap-based uncertainty quantification facilitated by bootstrap and interval. Below, we use simulated data as a substitute for observed data:","category":"page"},{"location":"workflow/examples/","page":"Examples","title":"Examples","text":"Z = simulate(Œ∏, m)     # pretend that this is observed data\nŒ∏ÃÇ(Z)                   # point estimates from the observed data\nŒ∏ÃÉ = bootstrap(Œ∏ÃÇ, Z)    # non-parametric bootstrap estimates\ninterval(Œ∏ÃÉ)  # confidence interval from the bootstrap estimates","category":"page"},{"location":"framework/#Theoretical-framework","page":"Theoretical framework","title":"Theoretical framework","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"In this section, we provide an overview of point estimation using neural Bayes estimators. For a more detailed discussion on the framework and its implementation, see Sainsbury-Dale et al. (2022; arxiv:2208.12942).","category":"page"},{"location":"framework/#Neural-Bayes-estimators","page":"Theoretical framework","title":"Neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"A parametric statistical model is a set of probability distributions on a sample space mathcalS, where the probability distributions are parameterised via some p-dimensional parameter vector boldsymboltheta on a parameter space Theta. Suppose that we have data from one such distribution, which we denote as boldsymbolZ. Then, the goal of parameter point estimation is to come up with an estimate of the unknown boldsymboltheta from boldsymbolZ using an estimator,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" hatboldsymboltheta  mathcalS to Theta","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"which is a mapping from the sample space to the parameter space.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Estimators can be constructed within a decision-theoretic framework. Assume that the sample space is mathcalS = mathbbR^n, and consider a non-negative loss function, L(boldsymboltheta hatboldsymboltheta(boldsymbolZ)), which assesses an estimator hatboldsymboltheta(cdot) for a given boldsymboltheta and data set boldsymbolZ sim f(boldsymbolz mid boldsymboltheta), where f(boldsymbolz mid boldsymboltheta) is the probability density function of the data conditional on boldsymboltheta. boldsymboltheta. An estimator's risk function is its loss averaged over all possible data realisations,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" R(boldsymboltheta hatboldsymboltheta(cdot)) equiv int_mathcalS  L(boldsymboltheta hatboldsymboltheta(boldsymbolz))f(boldsymbolz mid boldsymboltheta) rmd boldsymbolz","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"So-called Bayes estimators minimise the Bayes risk,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" r_Omega(hatboldsymboltheta(cdot))\n equiv int_Theta R(boldsymboltheta hatboldsymboltheta(cdot)) rmd Omega(boldsymboltheta)  ","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"where Omega(cdot) is a prior measure for boldsymboltheta.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Bayes estimators are theoretically attractive: for example, unique Bayes estimators are admissible and, under suitable regularity conditions and the squared-error loss, are consistent and asymptotically efficient. Further, for a large class of prior distributions, every set of conditions that imply consistency of the maximum likelihood (ML) estimator also imply consistency of Bayes estimators. Importantly, Bayes estimators are not motivated purely by asymptotics: by construction, they are Bayes irrespective of the sample size and model class. Unfortunately, however, Bayes estimators are typically unavailable in closed form for the complex models often encountered in practice. A way forward is to assume a flexible parametric model for hatboldsymboltheta(cdot), and to optimise the parameters within that model in order to approximate the Bayes estimator. Neural networks are ideal candidates, since they are universal function approximators, and because they are also fast to evaluate, usually involving only simple matrix-vector operations.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Let hatboldsymboltheta(boldsymbolZ boldsymbolgamma) denote a neural point estimator, that is, a neural network that returns a point estimate from data boldsymbolZ, where boldsymbolgamma contains the neural-network parameters. Bayes estimators may be approximated with hatboldsymboltheta(cdot boldsymbolgamma^*) by solving the optimisation problem,  ","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"boldsymbolgamma^*\nequiv\nundersetboldsymbolgammamathrmargmin  r_Omega(hatboldsymboltheta(cdot boldsymbolgamma))","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Typically, r_Omega(cdot) cannot be directly evaluated, but it can be approximated using Monte Carlo methods. Specifically, given a set of K parameter vectors sampled from the prior Omega(cdot) denoted by vartheta and, for each boldsymboltheta in vartheta, J realisations from f(boldsymbolz mid  boldsymboltheta) collected in mathcalZ_boldsymboltheta,","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":" r_Omega(hatboldsymboltheta(cdot boldsymbolgamma))\n approx\nfrac1K sum_boldsymboltheta in vartheta frac1J sum_boldsymbolz in mathcalZ_boldsymboltheta L(boldsymboltheta hatboldsymboltheta(boldsymbolz boldsymbolgamma))  ","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Note that the above approximation does not involve evaluation, or knowledge, of the likelihood function.","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"The Monte-Carlo-approximated Bayes risk can be straightforwardly minimised with respect to boldsymbolgamma using back-propagation and stochastic gradient descent. For sufficiently flexible architectures, the point estimator targets a Bayes estimator with respect to L(cdot cdot) and Omega(cdot). We therefore call the fitted neural point estimator a  neural Bayes estimator. Like Bayes estimators, neural Bayes estimators target a specific point summary of the posterior distribution. For instance, the absolute-error and squared-error loss functions lead to neural Bayes estimators that approximate the posterior median and mean, respectively.","category":"page"},{"location":"framework/#Construction-of-neural-Bayes-estimators","page":"Theoretical framework","title":"Construction of neural Bayes estimators","text":"","category":"section"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"The neural Bayes estimators is conceptually simple and can be used in a wide range of problems where other approaches, such as maximum-likelihood estimation, are computationally infeasible. The estimator also has marked practical appeal, as the general workflow for its construction is only loosely connected to the statistical or physical model being considered. The workflow is as follows:","category":"page"},{"location":"framework/","page":"Theoretical framework","title":"Theoretical framework","text":"Define the prior, Omega(cdot).\nChoose a loss function, L(cdot cdot), typically the absolute-error or squared-error loss.\nDesign a suitable neural-network architecture for the neural point estimator hatboldsymboltheta(cdot boldsymbolgamma).\nSample parameters from Omega(cdot) to form training/validation/test parameter sets.\nGiven the above parameter sets, simulate data from the model, to form training/validation/test data sets.\nTrain the neural network (i.e., estimate boldsymbolgamma) by minimising the loss function averaged over the training sets. During training, monitor performance and convergence using the validation sets.\nAssess the fitted neural Bayes estimator, hatboldsymboltheta(cdot boldsymbolgamma^*), using the test set.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"workflow/advancedusage/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"In this section, we discuss practical considerations on how to construct neural estimators most effectively.","category":"page"},{"location":"workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation","page":"Advanced usage","title":"Storing expensive intermediate objects for data simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Parameters sampled from the prior distribution Omega(cdot) may be stored in two ways. Most simply, they can be stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution; this is the approach taken in the example using univariate Gaussian data. Alternatively, they can be stored in a user-defined subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the p times K matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting \"on-the-fly\" simulation, which is discussed below.","category":"page"},{"location":"workflow/advancedusage/#On-the-fly-and-just-in-time-simulation","page":"Advanced usage","title":"On-the-fly and just-in-time simulation","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"When data simulation is (relatively) computationally inexpensive, mathcalZ_texttrain can be simulated continuously during training, a technique coined \"simulation-on-the-fly\". Regularly refreshing mathcalZ_texttrain leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when mathcalZ_texttrain is fixed. Refreshing mathcalZ_texttrain also has an additional computational benefit; data can be simulated \"just-in-time\", in the sense that they can be simulated from a small batch of vartheta_texttrain, used to train the neural estimator, and then removed from memory. This can reduce pressure on memory resources when vartheta_texttrain is very large.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"One may also regularly refresh vartheta_texttrain, and doing so leads to similar benefits. However, fixing vartheta_texttrain allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models.  ","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"The above strategies are facilitated with various methods of train.","category":"page"},{"location":"workflow/advancedusage/#Variable-sample-sizes","page":"Advanced usage","title":"Variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, m, and is not necessarily a Bayes estimator for m^* ne m. Denote a data set comprising m replicates as boldsymbolZ^(m) equiv (boldsymbolZ_1 dots boldsymbolZ_m). There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying m are envisaged, which we describe below.","category":"page"},{"location":"workflow/advancedusage/#Piecewise-estimators","page":"Advanced usage","title":"Piecewise estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If data sets with varying m are envisaged, one could train l neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints m_1, m_2, dots, m_l-1, one could construct a piecewise neural Bayes estimator,","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*)\n=\nbegincases\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_1)  m leq m_1\nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_2)  m_1  m leq m_2\nquad vdots \nhatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma^*_tildem_l)  m  m_l-1\nendcases","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"where, here, boldsymbolgamma^* equiv (boldsymbolgamma^*_tildem_1 dots boldsymbolgamma^*_tildem_l-1), and where boldsymbolgamma^*_tildem are the neural-network parameters optimised for sample size tildem chosen so that hatboldsymboltheta(cdot boldsymbolgamma^*_tildem) is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Piecewise neural estimators are implemented with the struct, PiecewiseEstimator, and their construction is facilitated with trainx.  ","category":"page"},{"location":"workflow/advancedusage/#Training-with-variable-sample-sizes","page":"Advanced usage","title":"Training with variable sample sizes","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Alternatively, one could treat the sample size as a random variable, M, with support over a set of positive integers, mathcalM, in which case, for the neural Bayes estimator, the risk function becomes","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"R(boldsymboltheta hatboldsymboltheta(cdot boldsymbolgamma))\nequiv\nsum_m in mathcalM\nP(M=m)left(int_mathcalS^m  L(boldsymboltheta hatboldsymboltheta(boldsymbolZ^(m) boldsymbolgamma))p(boldsymbolZ^(m) mid boldsymboltheta) textd boldsymbolZ^(m)right)","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a discrete uniform prior for M, the random variable corresponding to sample size.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"function simulate(parameters, m::R) where {R <: AbstractRange{I}} where I <: Integer\n\n\t## Number of parameter vectors stored in parameters\n\tK = size(parameters, 2)\n\n\t## Generate K sample sizes from the prior distribution for M\n\tmÃÉ = rand(m, K)\n\n\t## Pseudocode for data simulation\n\tZ = [<simulate mÃÉ[k] iid realisations from the model> for k ‚àà 1:K]\n\n\treturn Z\nend","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Then, setting the argument m in train to be an integer range (e.g., 1:30) will train the neural estimator with the given variable sample sizes.","category":"page"},{"location":"workflow/advancedusage/#Combining-neural-and-expert-summary-statistics","page":"Advanced usage","title":"Combining neural and expert summary statistics","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"See DeepSetExpert.","category":"page"},{"location":"workflow/advancedusage/#Loading-previously-saved-neural-estimators","page":"Advanced usage","title":"Loading previously saved neural estimators","text":"","category":"section"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the parameters of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator at a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"If the argument savepath is specified, train automatically saves the neural estimator's parameters; to load them, one may use the following code, or similar:","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"using Flux: loadparams!\n\nŒ∏ÃÇ = architecture()\nloadparams!(Œ∏ÃÇ, loadbestweights(savepath))","category":"page"},{"location":"workflow/advancedusage/","page":"Advanced usage","title":"Advanced usage","text":"Above, architecture() is a user-defined function that returns a neural estimator with the same architecture as the estimator that we wish to load, but with randomly initialised parameters, and the function loadparams! loads the parameters of the best (as determined by loadbestweights) neural estimator saved in savepath.","category":"page"},{"location":"workflow/overview/#Overview","page":"Overview","title":"Overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"To develop a neural estimator with NeuralEstimators.jl,","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Sample parameters from the prior distribution: the parameters are stored as p times K matrices, with p the number of parameters in the model and K the number of samples (i.e., parameter configurations) in the given parameter set (i.e., training, validation, or test set).\nSimulate data from the assumed model over the parameter sets generated above. These data are stored as a Vector{A}, with each element of the vector associated with one parameter configuration, and where A depends on the representation of the neural estimator (e.g., an Array for CNN-based estimators, or a GNNGraph for GNN-based estimators).\nInitialise a neural network, Œ∏ÃÇ, that will be trained into a neural Bayes estimator.  \nTrain Œ∏ÃÇ under the chosen loss function using train.\nAssess Œ∏ÃÇ using assess. The resulting object of class Assessment can be used to assess the estimator with respect to the entire parameter space by estimating the risk function with risk, or used to plot the empirical sampling distribution of the estimator.\nApply Œ∏ÃÇ to observed data (once its performance has been checked in the above step). Bootstrap-based uncertainty quantification is facilitated with bootstrap and interval. ","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"See the Examples and, once familiar with the basic workflow, see Advanced usage for practical considerations on how to most effectively construct neural estimators.","category":"page"},{"location":"API/core/#Core","page":"Core","title":"Core","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"This page documents the functions that are central to the workflow of NeuralEstimators. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.","category":"page"},{"location":"API/core/#Sampling-parameters","page":"Core","title":"Sampling parameters","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"Parameters sampled from the prior distribution Omega(cdot) are stored as a p times K matrix, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type ParameterConfigurations, whose only requirement is a field Œ∏ that stores the matrix of parameters. See Storing expensive intermediate objects for data simulation for further discussion.   ","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"ParameterConfigurations","category":"page"},{"location":"API/core/#NeuralEstimators.ParameterConfigurations","page":"Core","title":"NeuralEstimators.ParameterConfigurations","text":"ParameterConfigurations\n\nAn abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.\n\nThe user-defined type must have a field Œ∏ that stores the p √ó K matrix of parameters, where p is the number of parameters in the model and K is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.\n\nSee subsetparameters for the generic function for subsetting these objects.\n\nExamples\n\nstruct P <: ParameterConfigurations\n\tŒ∏\n\t# other expensive intermediate objects...\nend\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Simulating-data","page":"Core","title":"Simulating data","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimators facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"The data should be stored as a Vector{A}, where each element of the vector is associated with one parameter configuration, and where A depends on the architecture of the neural estimator.","category":"page"},{"location":"API/core/#Types-of-estimators","page":"Core","title":"Types of estimators","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"See also Architectures and activations functions that are often used when constructing neural estimators.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"NeuralEstimator\n\nPointEstimator\n\nIntervalEstimator\n\nQuantileEstimator\n\nPiecewiseEstimator","category":"page"},{"location":"API/core/#NeuralEstimators.NeuralEstimator","page":"Core","title":"NeuralEstimators.NeuralEstimator","text":"NeuralEstimator\n\nAn abstract supertype for neural estimators.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PointEstimator","page":"Core","title":"NeuralEstimators.PointEstimator","text":"PointEstimator(arch)\n\nA simple point estimator, that is, a mapping from the sample space to the parameter space, defined by the given architecture arch.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.IntervalEstimator","page":"Core","title":"NeuralEstimators.IntervalEstimator","text":"IntervalEstimator(arch_lower, arch_upper)\nIntervalEstimator(arch)\n\nA neural estimator that produces credible intervals constructed as,\n\nl(Z) l(Z) + mathrmexp(u(Z))\n\nwhere l() and u() are the neural networks arch_lower and arch_upper, both of which should transform data into p-dimensional vectors, where p is the number of parameters in the model. If only a single neural network architecture arch is provided, it will be used for both arch_lower and arch_upper.\n\nInternally, the output from arch_lower and arch_upper are concatenated, so that IntervalEstimator objects transform data into 2p-dimensional vectors.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\n# Generate some toy data\nn = 2  # bivariate data\nm = 10 # number of independent replicates\nZ = rand(n, m)\n\n# Create an architecture\np = 3  # parameters in the model\nw = 8  # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\narchitecture = DeepSet(œà, œï)\n\n# Initialise the interval estimator\nestimator = IntervalEstimator(architecture)\n\n# Apply the interval estimator\nestimator(Z)\ninterval(estimator, Z, parameter_names = [\"œÅ\", \"œÉ\", \"œÑ\"])\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.QuantileEstimator","page":"Core","title":"NeuralEstimators.QuantileEstimator","text":"QuantileEstimator()\n\nComing soon: this structure will allow for the simultaneous estimation of an arbitrary number of marginal quantiles of the posterior distribution.\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.PiecewiseEstimator","page":"Core","title":"NeuralEstimators.PiecewiseEstimator","text":"PiecewiseEstimator(estimators, breaks)\n\nCreates a piecewise estimator from a collection of estimators, based on the collection of changepoints, breaks, which should contain one element fewer than the number of estimators.\n\nAny estimator can be included in estimators, including any of the subtypes of NeuralEstimator exported with the package NeuralEstimators (e.g., PointEstimator, IntervalEstimator, etc.).\n\nExamples\n\n# Suppose that we've trained two neural estimators. The first, Œ∏ÃÇ‚ÇÅ, is trained\n# for small sample sizes (e.g., m ‚â§ 30), and the second, `Œ∏ÃÇ‚ÇÇ`, is trained for\n# moderate-to-large sample sizes (e.g., m > 30). We construct a piecewise\n# estimator with a sample-size changepoint of 30, which dispatches Œ∏ÃÇ‚ÇÅ if m ‚â§ 30\n# and Œ∏ÃÇ‚ÇÇ if m > 30.\n\nusing NeuralEstimators\nusing Flux\n\nn = 2  # bivariate data\np = 3  # number of parameters in the model\nw = 8  # width of each layer\n\nœà‚ÇÅ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÅ = DeepSet(œà‚ÇÅ, œï‚ÇÅ)\n\nœà‚ÇÇ = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ‚ÇÇ = DeepSet(œà‚ÇÇ, œï‚ÇÇ)\n\nŒ∏ÃÇ = PiecewiseEstimator([Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ], [30])\nZ = [rand(n, 1, m) for m ‚àà (10, 50)]\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#Training","page":"Core","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"The function train is used to train a single neural estimator, while the wrapper function trainx is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.","category":"page"},{"location":"API/core/","page":"Core","title":"Core","text":"train\n\ntrainx","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, sampler, simulator; )\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, simulator; ) where {P <: Union{AbstractMatrix, ParameterConfigurations}}\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; ) where {T, P <: Union{AbstractMatrix, ParameterConfigurations}}\n\nTrain a neural estimator with architecture Œ∏ÃÇ.\n\nThe methods cater for different forms of on-the-fly simulation. The method that takes functions sampler and simulator for sampling parameters and simulating data, respectively, allows for both the parameters and the data to be simulated on-the-fly. Note that simulator is called as simulator(Œ∏, m), where Œ∏ is a set of parameters and m is the sample size (see keyword arguments below). If provided with specific instances of parameters (Œ∏_train and Œ∏_val) or data (Z_train and Z_val), they will be held fixed during training.\n\nIn all methods, the validation set is held fixed to reduce noise when evaluating the validation risk function, which is used to monitor the performance of the estimator during training.\n\nIf the number of replicates in Z_train is a multiple of the number of replicates for each element of Z_val, the training data will be recycled throughout training. For example, if each element of Z_train consists of 50 replicates, and each element of Z_val consists of 10 replicates, the first epoch uses the first 10 replicates in Z_train, the second epoch uses the next 10 replicates, and so on, until the sixth epoch again uses the first 10 replicates. Note that this requires the data to be subsettable with the function subsetdata.\n\nKeyword arguments\n\nArguments common to all methods:\n\nloss = mae: the loss function, which should return the average loss when applied to multiple replicates.\nepochs::Integer = 100\nbatchsize::Integer = 32\noptimiser = ADAM(1e-4)\nsavepath::String = \"\": path to save the trained estimator and other information; if savepath is an empty string (default), nothing is saved.\nstopping_epochs::Integer = 5: cease training if the risk doesn't improve in this number of epochs.\nuse_gpu::Bool = true\nverbose::Bool = true\n\nArguments common to train(Œ∏ÃÇ, P, simulator) and train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulator):\n\nm: sample sizes (either an Integer or a collection of Integers).\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nsimulate_just_in_time::Bool = false: flag indicating whether we should simulate just-in-time, in the sense that only a batchsize number of parameter vectors and corresponding data are in memory at a given time.\n\nArguments unique to train(Œ∏ÃÇ, P, simulator):\n\nK::Integer = 10000: number of parameter vectors in the training set; the size of the validation set is K √∑ 5.\nŒæ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices); if Œæ is provided, the parameter sampler is called as sampler(K, Œæ).\nepochs_per_Œ∏_refresh::Integer = 1: how often to refresh the training parameters. Must be a multiple of epochs_per_Z_refresh.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing Distributions\nimport NeuralEstimators: simulate\n\n# data simulator\nm = 15\nfunction simulate(Œ∏_set, m)\n\t[Float32.(rand(Normal(Œ∏[1], Œ∏[2]), 1, m)) for Œ∏ ‚àà eachcol(Œ∏_set)]\nend\n\n# parameter sampler\nK = 10000\nŒ© = (Œº = Normal(0, 1), œÉ = Uniform(0.1, 1)) # prior\nstruct sampler\n\tŒº\n\tœÉ\nend\nfunction (s::sampler)(K)\n\tŒº = rand(s.Œº, K)\n\tœÉ = rand(s.œÉ, K)\n\tŒ∏ = hcat(Œº, œÉ)'\n\treturn Œ∏\nend\nsmplr = sampler(Œ©.Œº, Œ©.œÉ)\n\n# architecture\np = length(Œ©)   # number of parameters in the statistical model\nw = 32          # width of each layer\nœà = Chain(Dense(1, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p))\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# training: full simulation on-the-fly\nŒ∏ÃÇ = train(Œ∏ÃÇ, smplr, simulate, m = m, K = K, epochs = 5)\nŒ∏ÃÇ = train(Œ∏ÃÇ, smplr,  simulate, m = m, K = K, epochs = 5)\nŒ∏ÃÇ = train(Œ∏ÃÇ, smplr,  simulate, m = m, K = K, epochs = 10, epochs_per_Œ∏_refresh = 4, epochs_per_Z_refresh = 2)\n\n# training: fixed parameters\nŒ∏_train = smplr(K)\nŒ∏_val   = smplr(K √∑ 5)\nŒ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulate, m = m, epochs = 5)\nŒ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulate, m = m, epochs = 5, epochs_per_Z_refresh = 2)\n\n# training: fixed parameters and fixed data\nZ_train = simulate(Œ∏_train, m)\nZ_val   = simulate(Œ∏_val, m)\nŒ∏ÃÇ = train(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train, Z_val, epochs = 5)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.trainx","page":"Core","title":"NeuralEstimators.trainx","text":"trainx(Œ∏ÃÇ, P, simulator, M; )\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, simulator, M; )\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train::T, Z_val::T, M; )\ntrainx(Œ∏ÃÇ, Œ∏_train, Œ∏_val, Z_train::V, Z_val::V; ) where {V <: AbstractVector{AbstractVector{Any}}}\n\nA wrapper around train to construct neural estimators for different sample sizes.\n\nThe collection M specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if M = [m‚ÇÅ, m‚ÇÇ], the estimator for sample size m‚ÇÇ is pre-trained with the estimator for sample size m‚ÇÅ.\n\nThe method for Z_train::T and Z_val::T subsets the data using subsetdata(Z, 1:m·µ¢) for each m·µ¢ ‚àà M. The method for Z_train::V and Z_val::V trains an estimator for each element of Z_train and Z_val; hence, it does not need to invoke subsetdata, which can be slow or difficult to define in some cases (e.g., for graphical data).\n\nThe keyword arguments inherit from train, and certain keyword arguments can be given as vectors. For example, if we are training two estimators, we can use a different number of epochs by providing epochs = [e‚ÇÅ, e‚ÇÇ]. Other arguments that allow vectors are batchsize, stopping_epochs, and optimiser.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Assessing-a-neural-estimator","page":"Core","title":"Assessing a neural estimator","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"assess\n\nAssessment\n\nrisk","category":"page"},{"location":"API/core/#NeuralEstimators.assess","page":"Core","title":"NeuralEstimators.assess","text":"assess(estimators, Œ∏, Z; <keyword args>)\n\nUsing a collection of estimators, compute estimates from data Z simulated based on true parameter vectors stored in Œ∏.\n\nIf Z contains more data sets than parameter vectors, the parameter matrix will be recycled by horizontal concatenation.\n\nThe output is of type Assessment; see ?Assessment for details.\n\nKeyword arguments\n\nestimator_names::Vector{String}: names of the estimators (sensible defaults provided).\nparameter_names::Vector{String}: names of the parameters (sensible defaults provided). If Œæ is provided with a field parameter_names, those names will be used.\nŒæ = nothing: an arbitrary collection of objects that are fixed (e.g., distance matrices).\nuse_Œæ = false: a Bool or a collection of Bool objects with length equal to the number of estimators. Specifies whether or not the estimator uses Œæ: if it does, the estimator will be applied as estimator(Z, Œæ). This argument is useful when multiple estimators are provided, only some of which need Œæ; hence, if only one estimator is provided and Œæ is not nothing, use_Œæ is automatically set to true.\nuse_gpu = true: a Bool or a collection of Bool objects with length equal to the number of estimators.\nverbose::Bool = true\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Generate testing parameters\nK = 100\nŒ∏ = rand(p, K)\n\n# Data for a single sample size\nm = 30\nZ = [rand(n, m) for _ ‚àà 1:K];\nassessment = assess([Œ∏ÃÇ], Œ∏, Z);\nrisk(assessment)\n\n# Multiple data sets for each parameter vector\nJ = 5\nZ = repeat(Z, J);\nassessment = assess([Œ∏ÃÇ], Œ∏, Z);\nrisk(assessment)\n\n# With set-level information\nq‚Çì = 2\nœï  = Chain(Dense(w + q‚Çì, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\nx = [rand(q‚Çì) for _ ‚àà eachindex(Z)]\nassessment = assess([Œ∏ÃÇ], Œ∏, (Z, x));\nrisk(assessment)\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.Assessment","page":"Core","title":"NeuralEstimators.Assessment","text":"Assessment(df::DataFrame, runtime::DataFrame)\n\nA type for storing the output of assess(). The field runtime contains the total time taken for each estimator. The field df is a long-form DataFrame with columns:\n\nestimator: the name of the estimator\nparameter: the name of the parameter\ntruth:     the true value of the parameter\nestimate:  the estimated value of the parameter\nm:         the sample size (number of iid replicates)\nk:         the index of the parameter vector in the test set\nj:         the index of the data set\n\nMultiple Assessment objects can be combined with merge().\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.risk","page":"Core","title":"NeuralEstimators.risk","text":"risk(assessment::Assessment; loss = (x, y) -> abs(x - y), average_over_parameters = true)\n\nComputes a Monte Carlo approximation of the Bayes risk,\n\nr_Omega(hatboldsymboltheta(cdot))\napprox\nfrac1K sum_boldsymboltheta in vartheta frac1J sum_boldsymbolZ in mathcalZ_boldsymboltheta L(boldsymboltheta hatboldsymboltheta(boldsymbolZ))\n\nwhere vartheta denotes a set of K parameter vectors sampled from the prior Omega(cdot) and, for each boldsymboltheta in vartheta, we have J sets of m mutually independent realisations from the model collected in mathcalZ_boldsymboltheta.\n\nKeyword arguments\n\nloss = (x, y) -> abs(x - y): a binary operator (default absolute-error loss).\naverage_over_parameters::Bool = true: if true (default), the loss is averaged over all parameters; otherwise, the loss is averaged over each parameter separately.\naverage_over_sample_sizes::Bool = true: if true (default), the loss is averaged over all sample sizes m; otherwise, the loss is averaged over each sample size separately.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Bootstrapping","page":"Core","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core","title":"Core","text":"bootstrap\n\ninterval","category":"page"},{"location":"API/core/#NeuralEstimators.bootstrap","page":"Core","title":"NeuralEstimators.bootstrap","text":"bootstrap(Œ∏ÃÇ, parameters::P, Z) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, parameters::P, simulator, m::Integer; B = 400) where P <: Union{AbstractMatrix, ParameterConfigurations}\nbootstrap(Œ∏ÃÇ, Z; B = 400, blocks = nothing)\n\nGenerates B bootstrap estimates from an estimator Œ∏ÃÇ.\n\nParametric bootstrapping is facilitated by passing a single parameter configuration, parameters, and corresponding simulated data, Z, whose length implicitly defines B. Alternatively, one may provide a simulator and the desired sample size, in which case the data will be simulated using simulator(parameters, m).\n\nNon-parametric bootstrapping is facilitated by passing a single data set, Z. The argument blocks caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, blocks should be [1, 1, 2, 2, 2]. The resampling algorithm aims to produce resampled data sets that are of a similar size to Z, but this can only be achieved exactly if all blocks are equal in length.\n\nThe keyword argument use_gpu is a flag determining whether to use the GPU, if it is available (default true).\n\nThe return type is a p √ó B matrix, where p is the number of parameters in the model.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.interval","page":"Core","title":"NeuralEstimators.interval","text":"interval(Œ∏ÃÉ, Œ∏ÃÇ = nothing; type::String, probs = [0.05, 0.95], parameter_names)\n\nCompute a confidence interval using the p √ó B matrix of bootstrap samples, Œ∏ÃÉ, where p is the number of parameters in the model.\n\nIf type = \"quantile\", the interval is constructed by simply taking the quantiles of Œ∏ÃÉ, and if type = \"reverse-quantile\", the so-called reverse-quantile method is used. In both cases, the quantile levels are controlled by the argument probs.\n\nThe rows can be named with a vector of strings parameter_names.\n\nThe return type is a p √ó 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval.\n\nExamples\n\nusing NeuralEstimators\np = 3\nB = 50\nŒ∏ÃÉ = rand(p, B)\nŒ∏ÃÇ = rand(p)\ninterval(Œ∏ÃÉ)\ninterval(Œ∏ÃÉ, Œ∏ÃÇ, type = \"basic\")\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Miscellaneous","page":"Miscellaneous","title":"Miscellaneous","text":"","category":"section"},{"location":"API/utility/#Core","page":"Miscellaneous","title":"Core","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"These functions can appear during the core workflow, and may need to be overloaded in some applications.","category":"page"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"subsetparameters\n\nnumberreplicates\n\nsubsetdata","category":"page"},{"location":"API/utility/#NeuralEstimators.subsetparameters","page":"Miscellaneous","title":"NeuralEstimators.subsetparameters","text":"subsetparameters(parameters::M, indices) where {M <: AbstractMatrix}\nsubsetparameters(parameters::P, indices) where {P <: ParameterConfigurations}\n\nSubset parameters using a collection of indices.\n\nArrays in parameters::P with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using indices. All other fields are left unchanged. To modify this default behaviour, overload subsetparameters.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.numberreplicates","page":"Miscellaneous","title":"NeuralEstimators.numberreplicates","text":"numberofreplicates(Z)\n\nGeneric function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an Array or as a GNNGraph.\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.subsetdata","page":"Miscellaneous","title":"NeuralEstimators.subsetdata","text":"Generic function for subsetting replicates from a data set. Default methods are:\n\nsubsetdata(Z::A, m) where {A <: AbstractArray{T, N}} where {T, N}\nsubsetdata(Z::G, m) where {G <: AbstractGraph}\n\nNote that subsetdata is slow for graphical data, and one should consider using a method of train that does not require the data to be subsetted. Use numberreplicates to check that the training and validation data sets are equally replicated, which prevents the invocation of subsetdata. Note also that subsetdata only applies to vectors of batched graphs.\n\nIf the user is working with data that is not covered by the default methods, simply overload subsetdata with the appropriate type for Z.\n\nExamples\n\nusing NeuralEstimators\nusing GraphNeuralNetworks\nusing Flux: batch\n\nn = 5  # number of observations in each realisation\nm = 6  # number of replicates for each parameter vector\nd = 1  # dimension of the response variable\nK = 2  # number of parameter vectors\n\n# Array data\nZ = [rand(n, d, m) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates for each parameter vector\n\n# Graphical data\ne = 8 # number of edges\nZ = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ‚àà 1:m]) for k ‚àà 1:K]\nsubsetdata(Z, 1:3) # extract first 3 replicates for each parameter vector\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#Utility-functions","page":"Miscellaneous","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Miscellaneous","title":"Miscellaneous","text":"vectotril\n\ncontainertype\n\nloadbestweights\n\nstackarrays\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.vectotril","page":"Miscellaneous","title":"NeuralEstimators.vectotril","text":"vectotril(v; strict = false)\nvectotriu(v; strict = false)\n\nConverts a vector v of length d(d+1)2 (a triangular number) into a d  d lower or upper triangular matrix.\n\nIf strict = true, the matrix will be strictly lower or upper triangular, that is, a (d+1)  (d+1) triangular matrix with zero diagonal.\n\nNote that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if v is a GPU array. Note also that the return type is not of type Triangular matrix (i.e., the zeros are materialised) since Traingular matrices are not always compatible with other GPU operations.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nn = d*(d+1)√∑2\nv = collect(range(1, n))\nvectotril(v)\nvectotriu(v)\nvectotril(v; strict = true)\nvectotriu(v; strict = true)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.containertype","page":"Miscellaneous","title":"NeuralEstimators.containertype","text":"containertype(A::Type)\ncontainertype(::Type{A}) where A <: SubArray\ncontainertype(a::A) where A\n\nReturns the container type of its argument.\n\nIf given a SubArray, returns the container type of the parent array.\n\nExamples\n\na = rand(3, 4)\ncontainertype(a)\ncontainertype(typeof(a))\n[containertype(x) for x ‚àà eachcol(a)]\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.loadbestweights","page":"Miscellaneous","title":"NeuralEstimators.loadbestweights","text":"loadbestweights(path::String)\n\nGiven a path to a training run containing neural networks saved with names \"network_epochx.bson\" and an object saved as \"loss_per_epoch.bson\",  returns the weights of the best network (measured by validation loss).\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.stackarrays","page":"Miscellaneous","title":"NeuralEstimators.stackarrays","text":"stackarrays(v::V; merge = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same size for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstackarrays(Z)\nstackarrays(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstackarrays(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Miscellaneous","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#Loss-functions","page":"Loss functions","title":"Loss functions","text":"","category":"section"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"In addition to the standard loss functions provided by Flux (e.g., mae, mse, etc.), NeuralEstimators provides the following loss functions.","category":"page"},{"location":"API/loss/","page":"Loss functions","title":"Loss functions","text":"kpowerloss\n\nquantileloss\n\nintervalscore","category":"page"},{"location":"API/loss/#NeuralEstimators.kpowerloss","page":"Loss functions","title":"NeuralEstimators.kpowerloss","text":"kpowerloss(Œ∏ÃÇ, y, k; agg = mean, safeorigin = true, œµ = 0.1)\n\nFor k ‚àà (0, ‚àû), the k-th power absolute-distance loss,\n\nL(Œ∏ Œ∏) = Œ∏ - Œ∏·µè\n\ncontains the squared-error, absolute-error, and 0-1 loss functions as special cases (the latter obtained in the limit as k ‚Üí 0).\n\nIt is Lipschitz continuous iff k = 1, convex iff k ‚â• 1, and strictly convex iff k > 1. It is quasiconvex for all k > 0.\n\nIf safeorigin = true, the loss function is modified to avoid pathologies around the origin, so that the resulting loss function behaves similarly to the absolute-error loss in the œµ-interval surrounding the origin.\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.quantileloss","page":"Loss functions","title":"NeuralEstimators.quantileloss","text":"quantileloss(Œ∏ÃÇ, Œ∏, q; agg = mean)\nquantileloss(Œ∏ÃÇ, Œ∏, q::V; agg = mean) where {T, V <: AbstractVector{T}}\n\nThe asymmetric loss function whose minimiser is the qth posterior quantile; namely,\n\nL(Œ∏ Œ∏ q) = (Œ∏ - Œ∏)(ùïÄ(Œ∏ - Œ∏  0) - q)\n\nwhere q ‚àà (0, 1) and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes q as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in Œ∏ÃÇ is assumed to be pr, where p is the number of parameters: then, q should be an r-vector.\n\nFor further discussion on this loss function, see Equation (7) of Cressie, N. (2022), \"Decisions, decisions, decisions in an uncertain environment\", arXiv:2209.13157.\n\nExamples\n\np = 1\nK = 10\nŒ∏ = rand(p, K)\nŒ∏ÃÇ = rand(p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, 0.1)\n\nŒ∏ÃÇ = rand(3p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, [0.1, 0.5, 0.9])\n\np = 2\nŒ∏ = rand(p, K)\nŒ∏ÃÇ = rand(p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, 0.1)\n\nŒ∏ÃÇ = rand(3p, K)\nquantileloss(Œ∏ÃÇ, Œ∏, [0.1, 0.5, 0.9])\n\n\n\n\n\n","category":"function"},{"location":"API/loss/#NeuralEstimators.intervalscore","page":"Loss functions","title":"NeuralEstimators.intervalscore","text":"intervalscore(l, u, Œ∏, Œ±; agg = mean)\nintervalscore(Œ∏ÃÇ, Œ∏, Œ±; agg = mean)\n\nGiven a 100√ó(1-Œ±)% confidence interval [l, u] with true value Œ∏, the interval score is defined by\n\nS(l u Œ∏ Œ±) = (u - l) + 2Œ±¬π(l - Œ∏)ùïÄ(Œ∏  l) + 2Œ±¬π(Œ∏ - u)ùïÄ(Œ∏  u)\n\nwhere Œ± ‚àà (0, 1) and ùïÄ(‚ãÖ) is the indicator function.\n\nThe method that takes a single value Œ∏ÃÇ assumes that Œ∏ÃÇ is a matrix with 2p rows, where p is the number of parameters in the statistical model. Then, the first and second set of p rows will be used as l and u, respectively.\n\nFor further discussion, see Section 6 of Gneiting, T. and Raftery, A. E. (2007), \"Strictly proper scoring rules, prediction, and estimation\", Journal of the American statistical Association, 102, 359‚Äì378.\n\n\n\n\n\n","category":"function"},{"location":"API/architectures/#Architectures-and-activations-functions","page":"Architectures and activations functions","title":"Architectures and activations functions","text":"","category":"section"},{"location":"API/architectures/#Architectures","page":"Architectures and activations functions","title":"Architectures","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures and activations functions","title":"Architectures and activations functions","text":"Although the user is free to construct their neural estimator however they see fit, NeuralEstimators provides several useful architectures described below.","category":"page"},{"location":"API/architectures/","page":"Architectures and activations functions","title":"Architectures and activations functions","text":"DeepSet\n\nDeepSetExpert\n\nGNN\n\nPropagateReadout","category":"page"},{"location":"API/architectures/#NeuralEstimators.DeepSet","page":"Architectures and activations functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, a)\nDeepSet(œà, œï; a::String = \"mean\")\n\nThe Deep Set representation,\n\nŒ∏(ùêô) = œï(ùêì(ùêô))\t‚ÄÇ\t‚ÄÇùêì(ùêô) = ùêö(œà(ùêô·µ¢)  i = 1  m)\n\nwhere ùêô ‚â° (ùêô‚ÇÅ', ‚Ä¶, ùêô‚Çò')' are independent replicates from the model, œà and œï are neural networks, and a is a permutation-invariant aggregation function.\n\nTo make the architecture agnostic to the sample size m, the aggregation function a must aggregate over the replicates. It can be specified as a positional argument of type Function, or as a keyword argument with permissible values \"mean\", \"sum\", and \"logsumexp\".\n\nDeepSet objects act on data stored as Vector{A}, where each element of the vector is associated with one parameter vector (i.e., one set of independent replicates), and where A depends on the form of the data and the chosen architecture for œà. As a rule of thumb, when the data are stored as an array, the replicates are stored in the final dimension of the array. (This is usually the 'batch' dimension, but batching with DeepSets is done at the set level, i.e., sets of replicates are batched together.) For example, with gridded spatial data and œà a CNN, A should be a 4-dimensional array, with the replicates stored in the 4·µó ∞ dimension.\n\nNote that, internally, data stored as Vector{Arrays} are first concatenated along the replicates dimension before being passed into the inner neural network œà; this means that œà is applied to a single large array rather than many small arrays, which can substantially improve computational efficiency, particularly on the GPU.\n\nSet-level information, ùê±, that is not a function of the data can be passed directly into the outer network œï in the following manner,\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùê±))\t‚ÄÇ\t‚ÄÇùêì(ùêô) = ùêö(œà(ùêô·µ¢)  i = 1  m)\n\nThis is done by providing a Tuple{Vector{A}, Vector{B}}, where the first element of the tuple contains the vector of data sets and the second element contains the vector of set-level information.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Apply the estimator\nZ‚ÇÅ = rand(n, 3);                  # single set of 3 realisations\nZ‚ÇÇ = [rand(n, m) for m ‚àà (3, 3)]; # two sets each containing 3 realisations\nZ‚ÇÉ = [rand(n, m) for m ‚àà (3, 4)]; # two sets containing 3 and 4 realisations\nŒ∏ÃÇ(Z‚ÇÅ)\nŒ∏ÃÇ(Z‚ÇÇ)\nŒ∏ÃÇ(Z‚ÇÉ)\n\n# Repeat the above but with set-level information:\nq‚Çì = 2\nœï  = Chain(Dense(w + q‚Çì, w, relu), Dense(w, p));\nŒ∏ÃÇ  = DeepSet(œà, œï)\nx‚ÇÅ = rand(q‚Çì)\nx‚ÇÇ = [rand(q‚Çì) for _ ‚àà eachindex(Z‚ÇÇ)]\nŒ∏ÃÇ((Z‚ÇÅ, x‚ÇÅ))\nŒ∏ÃÇ((Z‚ÇÇ, x‚ÇÇ))\nŒ∏ÃÇ((Z‚ÇÉ, x‚ÇÇ))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.DeepSetExpert","page":"Architectures and activations functions","title":"NeuralEstimators.DeepSetExpert","text":"DeepSetExpert(œà, œï, S, a)\nDeepSetExpert(œà, œï, S; a::String = \"mean\")\nDeepSetExpert(deepset::DeepSet, œï, S)\n\nIdentical to DeepSet, but with additional expert summary statistics,\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùêí(ùêô)))\t‚ÄÇ\t‚ÄÇùêì(ùêô) = ùêö(œà(ùêô·µ¢)  i = 1  m)\n\nwhere S is a function that returns a vector of expert summary statistics.\n\nThe constructor DeepSetExpert(deepset::DeepSet, œï, S) inherits œà and a from deepset.\n\nSimilarly to DeepSet, set-level information can be incorporated by passing a Tuple, in which case we have\n\nŒ∏(ùêô) = œï((ùêì(ùêô) ùêí(ùêô) ùê±))\t‚ÄÇ\t‚ÄÇùêì(ùêô) = ùêö(œà(ùêô·µ¢)  i = 1  m)\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\nn = 10 # number of observations in each realisation\np = 4  # number of parameters in the statistical model\n\n# Construct the neural estimator\nS = samplesize\nq‚Çõ = 1\nq‚Çú = 32\nw = 16\nœà = Chain(Dense(n, w, relu), Dense(w, q‚Çú, relu));\nœï = Chain(Dense(q‚Çú + q‚Çõ, w), Dense(w, p));\nŒ∏ÃÇ = DeepSetExpert(œà, œï, S)\n\n# Apply the estimator\nZ‚ÇÅ = rand(n, 3);                  # single set\nZ‚ÇÇ = [rand(n, m) for m ‚àà (3, 4)]; # two sets\nŒ∏ÃÇ(Z‚ÇÅ)\nŒ∏ÃÇ(Z‚ÇÇ)\n\n# Repeat the above but with set-level information:\nq‚Çì = 2\nœï  = Chain(Dense(q‚Çú + q‚Çõ + q‚Çì, w, relu), Dense(w, p));\nŒ∏ÃÇ  = DeepSetExpert(œà, œï, S)\nx‚ÇÅ = rand(q‚Çì)\nx‚ÇÇ = [rand(q‚Çì) for _ ‚àà eachindex(Z‚ÇÇ)]\nŒ∏ÃÇ((Z‚ÇÅ, x‚ÇÅ))\nŒ∏ÃÇ((Z‚ÇÇ, x‚ÇÇ))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.GNN","page":"Architectures and activations functions","title":"NeuralEstimators.GNN","text":"GNN(propagation, readout, deepset)\n\nA graph neural network (GNN) designed for parameter estimation from independent replicates of the data gnerating process.\n\nThe propagation module transforms graphical input data into a set of hidden-feature graphs; the readout module aggregates these feature graphs (graph-wise) into a single hidden feature vector of fixed length; and the deepset module maps the hidden feature vector onto the output space.\n\nThe data should be a GNNGraph or AbstractVector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model. In cases where the independent replicates are stored over a fixed set of nodes, one may store the replicated data in the ndata field of a graph as a three-dimensional array with dimensions d √ó m √ó n, where d is the dimension of the response variable (i.e, d = 1 for univariate data), m is the number of replicates of the graph, and n is the number of nodes in the graph.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing Flux: batch\nusing GraphNeuralNetworks\nusing Statistics: mean\nusing Test\n\n# propagation and readout modules\nd = 1; w = 5; o = 7\npropagation = GNNChain(GraphConv(d => w), GraphConv(w => w), GraphConv(w => o))\nreadout     = GlobalPool(mean)\n\n# DeepSet module\nw = 32\np = 3\nœà = Chain(Dense(o, w, relu), Dense(w, w, relu), Dense(w, w, relu))\nœï = Chain(Dense(w, w, relu), Dense(w, p))\ndeepset = DeepSet(œà, œï)\n\n# GNN estimator\nŒ∏ÃÇ = GNN(propagation, readout, deepset)\n\n# Apply the estimator to a single graph, a single graph containing sub-graphs,\n# and a vector of graphs:\nn‚ÇÅ, n‚ÇÇ = 11, 27                             # number of nodes\ne‚ÇÅ, e‚ÇÇ = 30, 50                             # number of edges\ng‚ÇÅ = rand_graph(n‚ÇÅ, e‚ÇÅ, ndata=rand(d, n‚ÇÅ))\ng‚ÇÇ = rand_graph(n‚ÇÇ, e‚ÇÇ, ndata=rand(d, n‚ÇÇ))\ng‚ÇÉ = batch([g‚ÇÅ, g‚ÇÇ])\nŒ∏ÃÇ(g‚ÇÅ)\nŒ∏ÃÇ(g‚ÇÉ)\nŒ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])\n\n@test size(Œ∏ÃÇ(g‚ÇÅ)) == (p, 1)\n@test size(Œ∏ÃÇ(g‚ÇÉ)) == (p, 1)\n@test size(Œ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])) == (p, 3)\n\n# Efficient storage approach when the nodes do not vary between replicates:\nn = 100                     # number of nodes in the graph\ne = 200                     # number of edges in the graph\nm = 30                      # number of replicates of the graph\ng = rand_graph(n, e)        # fixed structure for all graphs\nx = rand(d, m, n)\ng‚ÇÅ = Flux.batch([GNNGraph(g; ndata = x[:, i, :]) for i ‚àà 1:m]) # regular storage\ng‚ÇÇ = GNNGraph(g; ndata = x)                                    # efficient storage\nŒ∏‚ÇÅ = Œ∏ÃÇ(g‚ÇÅ)\nŒ∏‚ÇÇ = Œ∏ÃÇ(g‚ÇÇ)\n@test size(Œ∏‚ÇÅ) == (p, 1)\n@test size(Œ∏‚ÇÇ) == (p, 1)\n@test all(Œ∏‚ÇÅ .‚âà Œ∏‚ÇÇ)\n\nv‚ÇÅ = [g‚ÇÅ, g‚ÇÅ]\nv‚ÇÇ = [g‚ÇÇ, g‚ÇÇ]\nŒ∏‚ÇÅ = Œ∏ÃÇ(v‚ÇÅ)\nŒ∏‚ÇÇ = Œ∏ÃÇ(v‚ÇÇ)\n@test size(Œ∏‚ÇÅ) == (p, 2)\n@test size(Œ∏‚ÇÇ) == (p, 2)\n@test all(Œ∏‚ÇÅ .‚âà Œ∏‚ÇÇ)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.PropagateReadout","page":"Architectures and activations functions","title":"NeuralEstimators.PropagateReadout","text":"PropagateReadout(propagation, readout)\n\nA module intended to act as the inner network œà in a DeepSet or DeepSetExpert architecture, performing the propagation and readout (global pooling) transformations of a GNN.\n\nThe graphical data should be stored as a GNNGraph or AbstractVector{GNNGraph}, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.\n\nThis approach is less efficient than GNN but currently more flexible, as it allows us to exploit the DeepSetExpert architecture and set-level covariate methods for DeepSet. It may be possible to improve the efficiency of this approach by carefully defining specialised methods, or I could make GNN more flexible, again by carefully defining specialised methods.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\nusing Flux: batch\nusing GraphNeuralNetworks\nusing Statistics: mean\n\n# Create some graph data\nd = 1                                        # dimension of response variable\nn‚ÇÅ, n‚ÇÇ = 11, 27                              # number of nodes\ne‚ÇÅ, e‚ÇÇ = 30, 50                              # number of edges\ng‚ÇÅ = rand_graph(n‚ÇÅ, e‚ÇÅ, ndata = rand(d, n‚ÇÅ))\ng‚ÇÇ = rand_graph(n‚ÇÇ, e‚ÇÇ, ndata = rand(d, n‚ÇÇ))\ng‚ÇÉ = batch([g‚ÇÅ, g‚ÇÇ])\n\n# propagation module and readout modules\nw = 5; o = 7\npropagation = GNNChain(GraphConv(d => w), GraphConv(w => w), GraphConv(w => o))\nreadout = GlobalPool(mean)\n\n# DeepSet estimator with GNN for the inner network œà\nw = 32\np = 3\nœà = PropagateReadout(propagation, readout)\nœï = Chain(Dense(o, w, relu), Dense(w, p))\nŒ∏ÃÇ = DeepSet(œà, œï)\n\n# Apply the estimator to a single graph, a single graph containing sub-graphs,\n# and a vector of graphs:\nŒ∏ÃÇ(g‚ÇÅ)\nŒ∏ÃÇ(g‚ÇÉ)\nŒ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])\n\n# Repeat the above but with set-level information:\nq‚Çì = 2\nœï = Chain(Dense(o + q‚Çì, w, relu), Dense(w, p))\nŒ∏ÃÇ = DeepSet(œà, œï)\nx‚ÇÅ = rand(q‚Çì)\nx‚ÇÇ = [rand(q‚Çì) for _ ‚àà eachindex([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])]\nŒ∏ÃÇ((g‚ÇÅ, x‚ÇÅ))\nŒ∏ÃÇ((g‚ÇÉ, x‚ÇÅ))\nŒ∏ÃÇ(([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ], x‚ÇÇ))\n\n# Repeat the above but with expert statistics:\nS = samplesize\nq‚Çõ = 1\nœï = Chain(Dense(o + q‚Çì + q‚Çõ, w, relu), Dense(w, p))\nŒ∏ÃÇ = DeepSetExpert(œà, œï, S)\nŒ∏ÃÇ((g‚ÇÅ, x‚ÇÅ))\nŒ∏ÃÇ((g‚ÇÉ, x‚ÇÅ))\nŒ∏ÃÇ(([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ], x‚ÇÇ))\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#Activation-functions","page":"Architectures and activations functions","title":"Activation functions","text":"","category":"section"},{"location":"API/architectures/","page":"Architectures and activations functions","title":"Architectures and activations functions","text":"These layers can be used at the end of an architecture to ensure that the neural estimator provides valid parameters.","category":"page"},{"location":"API/architectures/","page":"Architectures and activations functions","title":"Architectures and activations functions","text":"Compress\n\nCholeskyCovariance\n\nCovarianceMatrix\n\nCorrelationMatrix\n\nSplitApply","category":"page"},{"location":"API/architectures/#NeuralEstimators.Compress","page":"Architectures and activations functions","title":"NeuralEstimators.Compress","text":"Compress(a, b, k = 1)\n\nLayer that compresses its input to be within the range a and b, where each element of a is less than the corresponding element of b.\n\nThe layer uses a logistic function,\n\nl(Œ∏) = a + fracb - a1 + e^-kŒ∏\n\nwhere the arguments a and b together combine to shift and scale the logistic function to the desired range, and the growth rate k controls the steepness of the curve.\n\nThe logistic function given here contains an additional parameter, Œ∏‚ÇÄ, which is the input value corresponding to the functions midpoint. In Compress, we fix Œ∏‚ÇÄ = 0, since the output of a randomly initialised neural network is typically around zero.\n\nExamples\n\nusing NeuralEstimators\nusing Flux\n\na = [25, 0.5, -pi/2]\nb = [500, 2.5, 0]\np = length(a)\nK = 100\nŒ∏ = randn(p, K)\nl = Compress(a, b)\nl(Œ∏)\n\nn = 20\nŒ∏ÃÇ = Chain(Dense(n, p), l)\nZ = randn(n, K)\nŒ∏ÃÇ(Z)\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CholeskyCovariance","page":"Architectures and activations functions","title":"NeuralEstimators.CholeskyCovariance","text":"CholeskyCovariance(d)\n\nLayer for constructing the parameters of the lower Cholesky factor associated with an unconstrained d√ód covariance matrix.\n\nThe layer transforms a Matrix with d(d+1)√∑2 rows into a Matrix of the same dimension, but with d rows constrained to be positive (corresponding to the diagonal elements of the Cholesky factor) and the remaining rows unconstrained.\n\nThe ordering of the transformed Matrix aligns with Julia's column-major ordering. For example, when modelling the Cholesky factor,\n\nbeginbmatrix\nL‚ÇÅ‚ÇÅ           \nL‚ÇÇ‚ÇÅ  L‚ÇÇ‚ÇÇ      \nL‚ÇÉ‚ÇÅ  L‚ÇÉ‚ÇÇ  L‚ÇÉ‚ÇÉ \nendbmatrix\n\nthe rows of the matrix returned by a CholeskyCovariance layer will be ordered as\n\nL‚ÇÅ‚ÇÅ L‚ÇÇ‚ÇÅ L‚ÇÉ‚ÇÅ L‚ÇÇ‚ÇÇ L‚ÇÉ‚ÇÇ L‚ÇÉ‚ÇÉ\n\nwhich means that the output can easily be transformed into the implied Cholesky factors using vectotril.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\np = d*(d+1)√∑2\nŒ∏ = randn(p, 50)\nl = CholeskyCovariance(d)\nŒ∏ = l(Œ∏)                              # returns matrix (used for Flux networks)\nL = [vectotril(y) for y ‚àà eachcol(Œ∏)] # convert matrix to Cholesky factors\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CovarianceMatrix","page":"Architectures and activations functions","title":"NeuralEstimators.CovarianceMatrix","text":"CovarianceMatrix(d)\n\nLayer for constructing the parameters of an unconstrained d√ód covariance matrix.\n\nThe layer transforms a Matrix with d(d+1)√∑2 rows into a Matrix of the same dimension.\n\nInternally, it uses a CholeskyCovariance layer to construct a valid Cholesky factor ùêã, and then extracts the lower triangle from the positive-definite covariance matrix ùö∫ = ùêãùêã'. The lower triangle is extracted and vectorised in line with Julia's column-major ordering. For example, when modelling the covariance matrix,\n\nbeginbmatrix\nŒ£‚ÇÅ‚ÇÅ  Œ£‚ÇÅ‚ÇÇ  Œ£‚ÇÅ‚ÇÉ \nŒ£‚ÇÇ‚ÇÅ  Œ£‚ÇÇ‚ÇÇ  Œ£‚ÇÇ‚ÇÉ \nŒ£‚ÇÉ‚ÇÅ  Œ£‚ÇÉ‚ÇÇ  Œ£‚ÇÉ‚ÇÉ \nendbmatrix\n\nthe rows of the matrix returned by a CovarianceMatrix layer will be ordered as\n\nŒ£‚ÇÅ‚ÇÅ Œ£‚ÇÇ‚ÇÅ Œ£‚ÇÉ‚ÇÅ Œ£‚ÇÇ‚ÇÇ Œ£‚ÇÉ‚ÇÇ Œ£‚ÇÉ‚ÇÉ\n\nwhich means that the output can easily be transformed into the implied covariance matrices using vectotril and Symmetric.\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra\n\nd = 4\np = d*(d+1)√∑2\nŒ∏ = randn(p, 50)\n\nl = CovarianceMatrix(d)\nŒ∏ = l(Œ∏)\nŒ£ = [Symmetric(cpu(vectotril(y)), :L) for y ‚àà eachcol(Œ∏)]\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.CorrelationMatrix","page":"Architectures and activations functions","title":"NeuralEstimators.CorrelationMatrix","text":"CorrelationMatrix(d)\n\nLayer for constructing the parameters of an unconstrained d√ód correlation matrix.\n\nThe layer transforms a Matrix with d(d-1)√∑2 rows into a Matrix with the same dimension.\n\nInternally, the layers uses the algorithm described here and here to construct a valid Cholesky factor ùêã, and then extracts the strict lower triangle from the positive-definite correlation matrix ùêë = ùêãùêã'. The strict lower triangle is extracted and vectorised in line with Julia's column-major ordering. For example, when modelling the correlation matrix,\n\nbeginbmatrix\n1    R‚ÇÅ‚ÇÇ   R‚ÇÅ‚ÇÉ \nR‚ÇÇ‚ÇÅ  1     R‚ÇÇ‚ÇÉ\nR‚ÇÉ‚ÇÅ  R‚ÇÉ‚ÇÇ  1\nendbmatrix\n\nthe rows of the matrix returned by a CorrelationMatrix layer will be ordered as\n\nR‚ÇÇ‚ÇÅ R‚ÇÉ‚ÇÅ R‚ÇÉ‚ÇÇ\n\nwhich means that the output can easily be transformed into the implied correlation matrices using the strict variant of vectotril and Symmetric.\n\nExamples\n\nusing NeuralEstimators\nusing LinearAlgebra\n\nd = 4\np = d*(d-1)√∑2\nl = CorrelationMatrix(d)\nŒ∏ = randn(p, 50)\n\n# returns a matrix of parameters\nŒ∏ = l(Œ∏)\n\n# convert matrix of parameters to implied correlation matrices\nR = map(eachcol(Œ∏)) do y\n\tR = Symmetric(cpu(vectotril(y, strict = true)), :L)\n\tR[diagind(R)] .= 1\n\tR\nend\n\n\n\n\n\n","category":"type"},{"location":"API/architectures/#NeuralEstimators.SplitApply","page":"Architectures and activations functions","title":"NeuralEstimators.SplitApply","text":"SplitApply(layers, indices)\n\nSplits an array into multiple sub-arrays by subsetting the rows using the collection of indices, and then applies each layer in layers to the corresponding sub-array.\n\nSpecifically, for each i = 1, ‚Ä¶, n, with n the number of layers, SplitApply(x) performs layers[i](x[indices[i], :]), and then vertically concatenates the resulting transformed arrays.\n\nExamples\n\nusing NeuralEstimators\n\nd = 4\nK = 50\np‚ÇÅ = 2          # number of non-covariance matrix parameters\np‚ÇÇ = d*(d+1)√∑2  # number of covariance matrix parameters\np = p‚ÇÅ + p‚ÇÇ\n\na = [0.1, 4]\nb = [0.9, 9]\nl‚ÇÅ = Compress(a, b)\nl‚ÇÇ = CovarianceMatrix(d)\nl = SplitApply([l‚ÇÅ, l‚ÇÇ], [1:p‚ÇÅ, p‚ÇÅ+1:p])\n\nŒ∏ = randn(p, K)\nl(Œ∏)\n\n\n\n\n\n","category":"type"},{"location":"#NeuralEstimators","page":"NeuralEstimators","title":"NeuralEstimators","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Neural estimators are neural networks that transform data into parameter point estimates, and they are a promising recent approach to inference. They are likelihood free, substantially faster than classical methods, and can be designed to be approximate Bayes estimators.  Uncertainty quantification with neural estimators is also straightforward through the bootstrap distribution, which is essentially available \"for free\" with a neural estimator.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"The package NeuralEstimators facilitates the development of neural estimators in a user-friendly manner. It caters for arbitrary models by having the user implicitly define their model via simulated data. This makes the development of neural estimators particularly straightforward for models with existing implementations (possibly in other programming languages, e.g., R or python). A convenient interface for R users is available here.","category":"page"},{"location":"#Getting-started","page":"NeuralEstimators","title":"Getting started","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Install NeuralEstimators from Julia's package manager using the following command inside Julia:","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"using Pkg; Pkg.add(\"NeuralEstimators\")","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"Once familiar with the details of the Theoretical framework, see the Examples.","category":"page"},{"location":"#Supporting-and-citing","page":"NeuralEstimators","title":"Supporting and citing","text":"","category":"section"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"This software was developed as part of academic research. If you would like to support it, please star the repository. If you use NeuralEstimators in your research or other activities, please use the following citation.","category":"page"},{"location":"","page":"NeuralEstimators","title":"NeuralEstimators","text":"@article{SZH_2022_neural_estimators,\n\tauthor = {Sainsbury-Dale, Matthew and Zammit-Mangion, Andrew and Huser, Rapha√´l},\n\ttitle = {Likelihood-Free Parameter Estimation with Neural {B}ayes Estimators},\n\tjournal={arXiv:2208.12942},\n\tyear={2022}\n}","category":"page"}]
}
