<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced usage · NeuralEstimators.jl</title><meta name="title" content="Advanced usage · NeuralEstimators.jl"/><meta property="og:title" content="Advanced usage · NeuralEstimators.jl"/><meta property="twitter:title" content="Advanced usage · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Advanced usage</a><ul class="internal"><li><a class="tocitem" href="#Saving-and-loading-neural-estimators"><span>Saving and loading neural estimators</span></a></li><li><a class="tocitem" href="#Storing-expensive-intermediate-objects-for-data-simulation"><span>Storing expensive intermediate objects for data simulation</span></a></li><li><a class="tocitem" href="#On-the-fly-and-just-in-time-simulation"><span>On-the-fly and just-in-time simulation</span></a></li><li><a class="tocitem" href="#Regularisation"><span>Regularisation</span></a></li><li><a class="tocitem" href="#Expert-summary-statistics"><span>Expert summary statistics</span></a></li><li><a class="tocitem" href="#Variable-sample-sizes"><span>Variable sample sizes</span></a></li></ul></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/core/">Core</a></li><li><a class="tocitem" href="../../API/architectures/">Architectures</a></li><li><a class="tocitem" href="../../API/loss/">Loss functions</a></li><li><a class="tocitem" href="../../API/simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../../API/utility/">Miscellaneous</a></li><li><a class="tocitem" href="../../API/">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Workflow</a></li><li class="is-active"><a href>Advanced usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced usage</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/workflow/advancedusage.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced-usage"><a class="docs-heading-anchor" href="#Advanced-usage">Advanced usage</a><a id="Advanced-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-usage" title="Permalink"></a></h1><h2 id="Saving-and-loading-neural-estimators"><a class="docs-heading-anchor" href="#Saving-and-loading-neural-estimators">Saving and loading neural estimators</a><a id="Saving-and-loading-neural-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Saving-and-loading-neural-estimators" title="Permalink"></a></h2><p>As training is by far the most computationally demanding part of the workflow, one often trains an estimator and then saves it for later use. As discussed in the <a href="https://fluxml.ai/Flux.jl/stable/saving/">Flux documentation</a>, there are a number of ways to do this. Perhaps the simplest approach is to save the parameters (i.e., weights and biases) of the neural network in a BSON file:</p><pre><code class="nohighlight hljs">using Flux
using BSON: @save, @load
model_state = Flux.state(θ̂)
@save &quot;estimator.bson&quot; model_state</code></pre><p>Then, in a later session, one may initialise a neural network with the same architecture used previously, and load the saved parameters:</p><pre><code class="nohighlight hljs">@load &quot;estimator.bson&quot; model_state
Flux.loadmodel!(θ̂, model_state)</code></pre><p>Note that the estimator <code>θ̂</code> must be already defined (i.e., only the network parameters are saved, not the architecture). </p><p>For convenience, the function <a href="../../API/core/#NeuralEstimators.train"><code>train()</code></a> allows for the automatic saving of the neural-network parameters during the training stage, via the argument <code>savepath</code>. Specifically, if <code>savepath</code> is specified, neural estimator&#39;s parameters will be saved in the folder <code>savepath</code> and, to load the optimal parameters post training, one may use the following code, or similar:</p><pre><code class="nohighlight hljs">using NeuralEstimators
Flux.loadparams!(θ̂, loadbestweights(savepath))</code></pre><p>Above, the function <code>loadparams!()</code> loads the parameters of the best (as determined by <a href="../../API/utility/#NeuralEstimators.loadbestweights"><code>loadbestweights()</code></a>) neural estimator saved in <code>savepath</code>.</p><h2 id="Storing-expensive-intermediate-objects-for-data-simulation"><a class="docs-heading-anchor" href="#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a><a id="Storing-expensive-intermediate-objects-for-data-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Storing-expensive-intermediate-objects-for-data-simulation" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution may be stored in two ways. Most simply, they can be stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. Alternatively, they can be stored in a user-defined struct subtyping <a href="../../API/core/#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the <span>$p \times K$</span> matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting &quot;on-the-fly&quot; simulation, which is discussed below.</p><h2 id="On-the-fly-and-just-in-time-simulation"><a class="docs-heading-anchor" href="#On-the-fly-and-just-in-time-simulation">On-the-fly and just-in-time simulation</a><a id="On-the-fly-and-just-in-time-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#On-the-fly-and-just-in-time-simulation" title="Permalink"></a></h2><p>When data simulation is (relatively) computationally inexpensive, the training data set, <span>$\mathcal{Z}_{\text{train}}$</span>, can be simulated continuously during training, a technique coined &quot;simulation-on-the-fly&quot;. Regularly refreshing <span>$\mathcal{Z}_{\text{train}}$</span> leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when <span>$\mathcal{Z}_{\text{train}}$</span> is fixed. Further, this technique allows for data be simulated &quot;just-in-time&quot;, in the sense that they can be simulated in small batches, used to train the neural estimator, and then removed from memory. This can substantially reduce pressure on memory resources, particularly when working with large data sets. </p><p>One may also regularly refresh the set <span>$\vartheta_{\text{train}}$</span> of parameter vectors used during training, and doing so leads to similar benefits. However, fixing <span>$\vartheta_{\text{train}}$</span> allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models. Hybrid approaches are also possible, whereby the parameters (and possibly the data) are held fixed for several epochs (i.e., several passes through the training set when performing stochastic gradient descent) before being refreshed. </p><p>The above strategies are facilitated with various methods of <a href="../../API/core/#NeuralEstimators.train"><code>train()</code></a>.</p><h2 id="Regularisation"><a class="docs-heading-anchor" href="#Regularisation">Regularisation</a><a id="Regularisation-1"></a><a class="docs-heading-anchor-permalink" href="#Regularisation" title="Permalink"></a></h2><p>The term <em>regularisation</em> refers to a variety of techniques aimed to reduce overfitting when training a neural network, primarily by discouraging complex models.</p><p>One common regularisation technique is known as dropout <a href="https://jmlr.org/papers/v15/srivastava14a.html">(Srivastava et al., 2014)</a>, implemented in Flux&#39;s <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dropout"><code>Dropout</code></a> layer. Dropout involves temporarily dropping (&quot;turning off&quot;) a randomly selected set of neurons (along with their connections) at each iteration of the training stage, and this results in a computationally-efficient form of model (neural-network) averaging.</p><p>Another class of regularisation techniques involve modifying the loss function. For instance, L₁ regularisation (sometimes called lasso regression) adds to the loss a penalty based on the absolute value of the neural-network parameters. Similarly, L₂ regularisation (sometimes called ridge regression) adds to the loss a penalty based on the square of the neural-network parameters. Note that these penalty terms are not functions of the data or of the statistical-model parameters that we are trying to infer, and therefore do not modify the Bayes risk or the associated Bayes estimator. These regularisation techniques can be implemented straightforwardly by providing a custom <code>optimiser</code> to <a href="../../API/core/#NeuralEstimators.train"><code>train</code></a> that includes a <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.SignDecay"><code>SignDecay</code></a> object for L₁ regularisation, or a <a href="https://fluxml.ai/Flux.jl/stable/training/optimisers/#Flux.Optimise.WeightDecay"><code>WeightDecay</code></a> object for L₂ regularisation. See the <a href="https://fluxml.ai/Flux.jl/stable/training/training/#Regularisation">Flux documentation</a> for further details.</p><p>For example, the following code constructs a neural Bayes estimator using dropout and L₁ regularisation with penalty coefficient <span>$\lambda = 10^{-4}$</span>:</p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

# Generate data from the model Z ~ N(θ, 1) and θ ~ N(0, 1)
p = 1       # number of unknown parameters in the statistical model
m = 5       # number of independent replicates
d = 1       # dimension of each independent replicate
K = 3000    # number of training samples
θ_train = randn(1, K)
θ_val   = randn(1, K)
Z_train = [μ .+ randn(1, m) for μ ∈ eachcol(θ_train)]
Z_val   = [μ .+ randn(1, m) for μ ∈ eachcol(θ_val)]

# Architecture with dropout layers
ψ = Chain(
	Dense(1, 32, relu),
	Dropout(0.1),
	Dense(32, 32, relu),
	Dropout(0.5)
	)     
ϕ = Chain(
	Dense(32, 32, relu),
	Dropout(0.5),
	Dense(32, 1)
	)           
θ̂ = DeepSet(ψ, ϕ)

# Optimiser with L₂ regularisation
optimiser = Flux.setup(OptimiserChain(SignDecay(1e-4), Adam()), θ̂)

# Train the estimator
train(θ̂, θ_train, θ_val, Z_train, Z_val; optimiser = optimiser)</code></pre><p>Note that when the training data and/or parameters are held fixed during training, L₂ regularisation with penalty coefficient <span>$\lambda = 10^{-4}$</span> is applied by default.</p><h2 id="Expert-summary-statistics"><a class="docs-heading-anchor" href="#Expert-summary-statistics">Expert summary statistics</a><a id="Expert-summary-statistics-1"></a><a class="docs-heading-anchor-permalink" href="#Expert-summary-statistics" title="Permalink"></a></h2><p>Implicitly, neural estimators involve the learning of summary statistics. However, some summary statistics are available in closed form, simple to compute, and highly informative (e.g., sample quantiles, the empirical variogram, etc.). Often, explicitly incorporating these expert summary statistics in a neural estimator can simplify the optimisation problem, and lead to a better estimator. </p><p>The fusion of learned and expert summary statistics is facilitated by our implementation of the <a href="../../API/architectures/#NeuralEstimators.DeepSet"><code>DeepSet</code></a> framework. Note that this implementation also allows the user to construct a neural estimator using only expert summary statistics, following, for example, <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/sta4.382">Gerber and Nychka (2021)</a> and <a href="https://onlinelibrary.wiley.com/doi/abs/10.1002/env.2845">Rai et al. (2024)</a>. Note also that the user may specify arbitrary expert summary statistics, however, for convenience several standard <a href="../../API/architectures/#User-defined-summary-statistics">User-defined summary statistics</a> are provided with the package, including a fast approximate version of the empirical variogram. </p><h2 id="Variable-sample-sizes"><a class="docs-heading-anchor" href="#Variable-sample-sizes">Variable sample sizes</a><a id="Variable-sample-sizes-1"></a><a class="docs-heading-anchor-permalink" href="#Variable-sample-sizes" title="Permalink"></a></h2><p>A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, <span>$m$</span>, and is not necessarily a Bayes estimator for <span>$m^* \ne m$</span>. Denote a data set comprising <span>$m$</span> replicates as <span>$\boldsymbol{Z}^{(m)} \equiv (\boldsymbol{Z}_1&#39;, \dots, \boldsymbol{Z}_m&#39;)&#39;$</span>. There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying <span>$m$</span> are envisaged, which we describe below.</p><h3 id="Piecewise-estimators"><a class="docs-heading-anchor" href="#Piecewise-estimators">Piecewise estimators</a><a id="Piecewise-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Piecewise-estimators" title="Permalink"></a></h3><p>If data sets with varying <span>$m$</span> are envisaged, one could train <span>$l$</span> neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints <span>$m_1$</span>, <span>$m_2$</span>, <span>$\dots$</span>, <span>$m_{l-1}$</span>, one could construct a piecewise neural Bayes estimator,</p><p class="math-container">\[\hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(m)}; \boldsymbol{\gamma}^*)
=
\begin{cases}
\hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(m)}; \boldsymbol{\gamma}^*_{\tilde{m}_1}) &amp; m \leq m_1,\\
\hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(m)}; \boldsymbol{\gamma}^*_{\tilde{m}_2}) &amp; m_1 &lt; m \leq m_2,\\
\quad \vdots \\
\hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(m)}; \boldsymbol{\gamma}^*_{\tilde{m}_l}) &amp; m &gt; m_{l-1},
\end{cases}\]</p><p>where, here, <span>$\boldsymbol{\gamma}^* \equiv (\boldsymbol{\gamma}^*_{\tilde{m}_1}, \dots, \boldsymbol{\gamma}^*_{\tilde{m}_{l-1}})$</span>, and where <span>$\boldsymbol{\gamma}^*_{\tilde{m}}$</span> are the neural-network parameters optimised for sample size <span>$\tilde{m}$</span> chosen so that <span>$\hat{\boldsymbol{\theta}}(\cdot; \boldsymbol{\gamma}^*_{\tilde{m}})$</span> is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.</p><p>Piecewise neural estimators are implemented with the struct, <a href="../../API/core/#NeuralEstimators.PiecewiseEstimator"><code>PiecewiseEstimator</code></a>, and their construction is facilitated with <a href="../../API/core/#NeuralEstimators.trainx"><code>trainx()</code></a>.  </p><h3 id="Training-with-variable-sample-sizes"><a class="docs-heading-anchor" href="#Training-with-variable-sample-sizes">Training with variable sample sizes</a><a id="Training-with-variable-sample-sizes-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-variable-sample-sizes" title="Permalink"></a></h3><p>Alternatively, one could treat the sample size as a random variable, <span>$M$</span>, with support over a set of positive integers, <span>$\mathcal{M}$</span>, in which case, for the neural Bayes estimator, the risk function becomes</p><p class="math-container">\[\sum_{m \in \mathcal{M}}
P(M=m)\left(
\int_\Theta \int_{\mathcal{Z}^m}  L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{z}^{(m)}))f(\boldsymbol{z}^{(m)} \mid \boldsymbol{\theta}) \rm{d} \boldsymbol{z}^{(m)} \rm{d} \Pi(\boldsymbol{\theta})
\right).\]</p><p>This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data during the training phase. </p><p>The following pseudocode illustrates how one may modify a general data simulator to train under a range of sample sizes, with the distribution of <span>$M$</span> defined by passing any object that can be sampled using <code>rand(m, K)</code> (e.g., an integer range like <code>1:30</code>, an integer-valued distribution from <a href="https://juliastats.org/Distributions.jl/stable/univariate/">Distributions.jl</a>, etc.):</p><pre><code class="nohighlight hljs">function simulate(parameters, m) 

	## Number of parameter vectors stored in parameters
	K = size(parameters, 2)

	## Generate K sample sizes from the prior distribution for M
	m̃ = rand(m, K)

	## Pseudocode for data simulation
	Z = [&lt;simulate m̃[k] realisations from the model&gt; for k ∈ 1:K]

	return Z
end

## Method that allows an integer to be passed for m
simulate(parameters, m::Integer) = simulate(parameters, range(m, m))</code></pre></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../../API/core/">Core »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 16 July 2024 07:22">Tuesday 16 July 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
