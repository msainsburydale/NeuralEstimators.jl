<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Advanced usage · NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../overview/">Overview</a></li><li><a class="tocitem" href="../examples/">Examples</a></li><li class="is-active"><a class="tocitem" href>Advanced usage</a><ul class="internal"><li><a class="tocitem" href="#Storing-expensive-intermediate-objects-for-data-simulation"><span>Storing expensive intermediate objects for data simulation</span></a></li><li><a class="tocitem" href="#On-the-fly-and-just-in-time-simulation"><span>On-the-fly and just-in-time simulation</span></a></li><li><a class="tocitem" href="#Variable-sample-sizes"><span>Variable sample sizes</span></a></li><li><a class="tocitem" href="#Piecewise-estimators"><span>Piecewise estimators</span></a></li><li><a class="tocitem" href="#Training-with-variable-sample-sizes"><span>Training with variable sample sizes</span></a></li><li><a class="tocitem" href="#Loading-previously-saved-neural-estimators"><span>Loading previously saved neural estimators</span></a></li></ul></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../../API/core/">Core functions</a></li><li><a class="tocitem" href="../../API/simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../../API/utility/">Utility functions</a></li><li><a class="tocitem" href="../../API/">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">Workflow</a></li><li class="is-active"><a href>Advanced usage</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Advanced usage</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/workflow/advancedusage.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Advanced-usage"><a class="docs-heading-anchor" href="#Advanced-usage">Advanced usage</a><a id="Advanced-usage-1"></a><a class="docs-heading-anchor-permalink" href="#Advanced-usage" title="Permalink"></a></h1><p>In this section, we discuss practical considerations on how to construct neural estimators most effectively.</p><h2 id="Storing-expensive-intermediate-objects-for-data-simulation"><a class="docs-heading-anchor" href="#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a><a id="Storing-expensive-intermediate-objects-for-data-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#Storing-expensive-intermediate-objects-for-data-simulation" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> may be stored in two ways. Most simply, they can be stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution; this is the approach taken in the example using univariate Gaussian data. Alternatively, they can be stored in a user-defined subtype of the abstract type <a href="../../API/core/#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the <span>$p \times K$</span> matrix of parameters. With this approach, one may store computationally expensive intermediate objects, such as Cholesky factors, for later use when conducting &quot;on-the-fly&quot; simulation, which is discussed below.</p><h2 id="On-the-fly-and-just-in-time-simulation"><a class="docs-heading-anchor" href="#On-the-fly-and-just-in-time-simulation">On-the-fly and just-in-time simulation</a><a id="On-the-fly-and-just-in-time-simulation-1"></a><a class="docs-heading-anchor-permalink" href="#On-the-fly-and-just-in-time-simulation" title="Permalink"></a></h2><p>When data simulation is (relatively) computationally inexpensive, <span>$\mathcal{Z}_{\text{train}}$</span> can be simulated periodically during training, a technique coined &quot;simulation-on-the-fly&quot;. Regularly refreshing <span>$\mathcal{Z}_{\text{train}}$</span> leads to lower out-of-sample error and to a reduction in overfitting. This strategy therefore facilitates the use of larger, more representationally-powerful networks that are prone to overfitting when <span>$\mathcal{Z}_{\text{train}}$</span> is fixed. Refreshing <span>$\mathcal{Z}_{\text{train}}$</span> also has an additional computational benefit; data can be simulated &quot;just-in-time&quot;, in the sense that they can be simulated from a small batch of <span>$\vartheta_{\text{train}}$</span>, used to train the neural estimator, and then removed from memory. This can reduce pressure on memory resources when <span>$|\vartheta_{\text{train}}|$</span> is very large.</p><p>One may also regularly refresh <span>$\vartheta_{\text{train}}$</span>, and doing so leads to similar benefits. However, fixing <span>$\vartheta_{\text{train}}$</span> allows computationally expensive terms, such as Cholesky factors when working with Gaussian process models, to be reused throughout training, which can substantially reduce the training time for some models.  </p><p>The above strategies are facilitated with the various methods of <a href="../../API/core/#NeuralEstimators.train"><code>train</code></a>.</p><h2 id="Variable-sample-sizes"><a class="docs-heading-anchor" href="#Variable-sample-sizes">Variable sample sizes</a><a id="Variable-sample-sizes-1"></a><a class="docs-heading-anchor-permalink" href="#Variable-sample-sizes" title="Permalink"></a></h2><p>A neural estimator in the Deep Set representation can be applied to data sets of arbitrary size. However, even when the neural Bayes estimator approximates the true Bayes estimator arbitrarily well, it is conditional on the number of replicates, <span>$m$</span>, and is not necessarily a Bayes estimator for <span>$m^* \ne m$</span>. Denote a data set comprising <span>$m$</span> replicates as <span>$\mathbf{Z}^{(m)} \equiv (\mathbf{Z}_1&#39;, \dots, \mathbf{Z}_m&#39;)&#39;$</span>. There are at least two (non-mutually exclusive) approaches one could adopt if data sets with varying <span>$m$</span> are envisaged, which we describe below.</p><h2 id="Piecewise-estimators"><a class="docs-heading-anchor" href="#Piecewise-estimators">Piecewise estimators</a><a id="Piecewise-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Piecewise-estimators" title="Permalink"></a></h2><p>If data sets with varying <span>$m$</span> are envisaged, one could train <span>$l$</span> neural Bayes estimators for different sample sizes, or groups thereof (e.g., a small-sample estimator and a large-sample estimator).  Specifically, for sample-size changepoints <span>$m_1$</span>, <span>$m_2$</span>, <span>$\dots$</span>, <span>$m_{l-1}$</span>, one could construct a piecewise neural Bayes estimator,</p><p class="math-container">\[\hat{\mathbf{\theta}}(\mathbf{Z}^{(m)}; \mathbf{\gamma}^*)
=
\begin{cases}
\hat{\mathbf{\theta}}(\mathbf{Z}^{(m)}; \mathbf{\gamma}^*_{\tilde{m}_1}) &amp; m \leq m_1,\\
\hat{\mathbf{\theta}}(\mathbf{Z}^{(m)}; \mathbf{\gamma}^*_{\tilde{m}_2}) &amp; m_1 &lt; m \leq m_2,\\
\quad \vdots \\
\hat{\mathbf{\theta}}(\mathbf{Z}^{(m)}; \mathbf{\gamma}^*_{\tilde{m}_l}) &amp; m &gt; m_{l-1},
\end{cases}\]</p><p>where, here, <span>$\mathbf{\gamma}^* \equiv (\mathbf{\gamma}^*_{\tilde{m}_1}, \dots, \mathbf{\gamma}^*_{\tilde{m}_{l-1}})$</span>, and where <span>$\mathbf{\gamma}^*_{\tilde{m}}$</span> are the neural-network parameters optimised for sample size <span>$\tilde{m}$</span> chosen so that <span>$\hat{\mathbf{\theta}}(\cdot; \mathbf{\gamma}^*_{\tilde{m}})$</span> is near-optimal over the range of sample sizes in which it is applied. This approach works well in practice, and it is less computationally burdensome than it first appears when used in conjunction with pre-training.</p><p>Piecewise neural estimators are implemented with the struct, <a href="../../API/core/#NeuralEstimators.PiecewiseEstimator"><code>PiecewiseEstimator</code></a>. The method <a href="../../API/core/#NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}"><code>train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T, M::Vector{I}) where {T, P &lt;: Union{AbstractMatrix, ParameterConfigurations}, I &lt;: Integer}</code></a> is particularly useful for training piecewise neural estimators. Below, we replicate the example of inferring <span>$\mu$</span> and <span>$\sigma$</span> from <span>$N(\mu, \sigma^2)$</span> data, but this time we train three neural estimators with sample sizes <span>$\tilde{m}_l$</span> equal to 1, 10, and 30, respectively.   </p><pre><code class="nohighlight hljs">using NeuralEstimators
import NeuralEstimators: simulate
using Flux
using Distributions

Ω = (μ = Normal(0, 1), σ = Uniform(0.1, 1))

function sample(Ω, K)
	μ = rand(Ω.μ, K)
	σ = rand(Ω.σ, K)
	θ = hcat(μ, σ)&#39;
	return θ
end

θ_train = sample(Ω, 10000)
θ_val   = sample(Ω, 2000)

function simulate(θ_set, m)
	Z = [rand(Normal(θ[1], θ[2]), 1, 1, m) for θ ∈ eachcol(θ_set)]
	Z = broadcast.(Float32, Z)
	return Z
end

M = [1, 10, 30]
Z_train = simulate(θ_train, maximum(M))
Z_val   = simulate(θ_val, maximum(M))

n = 1    # size of each replicate (univariate data)
w = 32   # number of neurons in each layer
p = 2    # number of parameters in the statistical model

ψ = Chain(Dense(n, w, relu), Dense(w, w, relu))
ϕ = Chain(Dense(w, w, relu), Dense(w, p), Flux.flatten)
θ̂ = DeepSet(ψ, ϕ)

estimators = train(θ̂ , θ_train, θ_val, Z_train, Z_val, M, epochs = 10)
mchange = [5, 20]

piecewise_estimator = PiecewiseEstimator(estimators, mchange)</code></pre><h2 id="Training-with-variable-sample-sizes"><a class="docs-heading-anchor" href="#Training-with-variable-sample-sizes">Training with variable sample sizes</a><a id="Training-with-variable-sample-sizes-1"></a><a class="docs-heading-anchor-permalink" href="#Training-with-variable-sample-sizes" title="Permalink"></a></h2><p>Alternatively, one could treat the sample size as a random variable, <span>$M$</span>, with support over a set of positive integers, <span>$\mathcal{M}$</span>, in which case, for the neural Bayes estimator, the risk function becomes</p><p class="math-container">\[R(\mathbf{\theta}, \hat{\mathbf{\theta}}(\cdot; \mathbf{\gamma}))
\equiv
\sum_{m \in \mathcal{M}}
P(M=m)\left(\int_{\mathcal{S}^m}  L(\mathbf{\theta}, \hat{\mathbf{\theta}}(\mathbf{Z}^{(m)}; \mathbf{\gamma}))p(\mathbf{Z}^{(m)} \mid \mathbf{\theta}) d \mathbf{Z}^{(m)}\right).\]</p><p>This approach does not materially alter the workflow, except that one must also sample the number of replicates before simulating the data.</p><p>Below we define data simulation for a range of sample sizes (i.e., a range of integers) under a discrete uniform prior for <span>$M$</span>, the random variable corresponding to sample size.</p><pre><code class="nohighlight hljs">function simulate(parameters, m::R) where {R &lt;: AbstractRange{I}} where I &lt;: Integer

	# Number of parameter vectors stored in parameters
	K = size(parameters, 2)

	# Generate K sample sizes from the prior distribution for M
	m̃ = rand(m, K)

	# Pseudocode for data simulation
	Z = [&lt;simulate m̃[k] iid realisations from the model&gt; for k ∈ 1:K]

	return Z
end</code></pre><p>Then, setting the argument <code>m</code> in <a href="../../API/core/#NeuralEstimators.train"><code>train</code></a> to be an integer range will train the neural estimator with the given variable sample sizes.</p><h2 id="Loading-previously-saved-neural-estimators"><a class="docs-heading-anchor" href="#Loading-previously-saved-neural-estimators">Loading previously saved neural estimators</a><a id="Loading-previously-saved-neural-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Loading-previously-saved-neural-estimators" title="Permalink"></a></h2><p>As training is by far the most computationally demanding part of the workflow, one typically trains an estimator and then saves it for later use. More specifically, one usually saves the <em>parameters</em> of the neural estimator (e.g., the weights and biases of the neural networks); then, to load the neural estimator at a later time, one initialises an estimator with the same architecture used during training, and then loads the saved parameters into this estimator.</p><p><a href="../../API/core/#NeuralEstimators.train"><code>train</code></a> automatically saves the neural estimator&#39;s parameters; to load them, one may use the following code, or similar:</p><pre><code class="nohighlight hljs">θ̂ = architecture()
Flux.loadparams!(θ̂, loadbestweights(path))</code></pre><p>Above, <code>architecture()</code> is a user-defined function that returns a neural estimator with the same architecture as the estimator that we wish to load, but with randomly initialised parameters, and the function <code>Flux.loadparams!</code> loads the parameters of the best (as determined by <a href="../../API/utility/#NeuralEstimators.loadbestweights"><code>loadbestweights</code></a>) neural estimator saved in <code>path</code>.</p></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../examples/">« Examples</a><a class="docs-footer-nextpage" href="../../API/core/">Core functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Sunday 5 February 2023 06:34">Sunday 5 February 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
