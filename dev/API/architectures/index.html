<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Architectures Â· NeuralEstimators.jl</title><meta name="title" content="Architectures Â· NeuralEstimators.jl"/><meta property="og:title" content="Architectures Â· NeuralEstimators.jl"/><meta property="twitter:title" content="Architectures Â· NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li class="is-active"><a class="tocitem" href>Architectures</a><ul class="internal"><li><a class="tocitem" href="#Modules"><span>Modules</span></a></li><li class="toplevel"><a class="tocitem" href="#User-defined-summary-statistics"><span>User-defined summary statistics</span></a></li><li><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li class="toplevel"><a class="tocitem" href="#Output-activation-functions"><span>Output activation functions</span></a></li></ul></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Architectures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Architectures</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands">ï‚›</span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/architectures.md" title="Edit source on GitHub"><span class="docs-icon fa-solid">ï„</span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h1><h2 id="Modules"><a class="docs-heading-anchor" href="#Modules">Modules</a><a id="Modules-1"></a><a class="docs-heading-anchor-permalink" href="#Modules" title="Permalink"></a></h2><p>The following high-level modules are often used when constructing a neural-network architecture. In particular, the <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> is the building block for most classes of <a href="../core/#Estimators">Estimators</a> in the package. </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(Ïˆ, Ï•, a = mean; S = nothing)</code></pre><p>The DeepSets representation <a href="https://arxiv.org/abs/1703.06114">(Zaheer et al., 2017)</a>,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•(ğ“(ğ™)),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>where ğ™ â‰¡ (ğ™â‚&#39;, â€¦, ğ™â‚˜&#39;)&#39; are independent replicates from the statistical model, <code>Ïˆ</code> and <code>Ï•</code> are neural networks, and <code>a</code> is a permutation-invariant aggregation function. Expert summary statistics can be incorporated as,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ’(ğ™)&#39;)&#39;),\]</p><p>where <code>S</code> is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are provided either as a <code>Function</code> that returns a <code>Vector</code>, or as a vector of functions. In the case that  <code>Ïˆ</code> is set to <code>nothing</code>, only expert summary statistics will be used. </p><p>The aggregation function <code>a</code> can be any function that acts on an array and has a keyword argument <code>dims</code> that allows aggregation over a specific dimension of the array (e.g., <code>sum</code>, <code>mean</code>, <code>maximum</code>, <code>minimum</code>, <code>logsumexp</code>).</p><p><code>DeepSet</code> objects act on data of type <code>Vector{A}</code>, where each element of the vector is associated with one data set (i.e., one set of independent replicates from the statistical model), and where the type <code>A</code> depends on the form of the data and the chosen architecture for <code>Ïˆ</code>. As a rule of thumb, when <code>A</code> is an array, the replicates are stored in the final dimension. For example, with gridded spatial data and <code>Ïˆ</code> a CNN, <code>A</code> should be a 4-dimensional array, with the replicates stored in the 4áµ—Ê° dimension. Note that in Flux, the final dimension is usually the &quot;batch&quot; dimension, but batching with <code>DeepSet</code> objects is done at the data set level (i.e., sets of replicates are batched together).</p><p>Data stored as <code>Vector{Arrays}</code> are first concatenated along the replicates dimension before being passed into the summary network <code>Ïˆ</code>. This means that <code>Ïˆ</code> is applied to a single large array rather than many small arrays, which can substantially improve computational efficiency.</p><p>Set-level information, <span>$ğ±$</span>, that is not a function of the data can be passed directly into the inference network <code>Ï•</code> in the following manner,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ±&#39;)&#39;),	â€‚	â€‚\]</p><p>or, in the case that expert summary statistics are also used,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ’(ğ™)&#39;, ğ±&#39;)&#39;).	â€‚\]</p><p>This is done by calling the <code>DeepSet</code> object on a <code>Tuple{Vector{A}, Vector{Vector}}</code>, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level information (i.e., one vector for each data set).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

# Two dummy data sets containing 3 and 4 replicates
p = 5  # number of parameters in the statistical model
n = 10 # dimension of each replicate
Z = [rand32(n, m) for m âˆˆ (3, 4)]

# Construct the deepset object
S = samplesize
qâ‚› = 1   # dimension of expert summary statistic
qâ‚œ = 16  # dimension of neural summary statistic
w = 32  # width of hidden layers
Ïˆ = Chain(Dense(n, w, relu), Dense(w, qâ‚œ, relu))
Ï• = Chain(Dense(qâ‚œ + qâ‚›, w, relu), Dense(w, p))
Î¸Ì‚ = DeepSet(Ïˆ, Ï•; S = S)

# Apply the deepset object
Î¸Ì‚(Z)

# Data with set-level information
qâ‚“ = 2 # dimension of set-level vector
Ï• = Chain(Dense(qâ‚œ + qâ‚› + qâ‚“, w, relu), Dense(w, p))
Î¸Ì‚ = DeepSet(Ïˆ, Ï•; S = S)
x = [rand32(qâ‚“) for _ âˆˆ eachindex(Z)]
Î¸Ì‚((Z, x))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L45-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.GNNSummary" href="#NeuralEstimators.GNNSummary"><code>NeuralEstimators.GNNSummary</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GNNSummary(propagation, readout; globalfeatures = nothing)</code></pre><p>A graph neural network (GNN) module designed to serve as the summary network <code>Ïˆ</code> in the <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> representation when the data are graphical (e.g., irregularly observed spatial data).</p><p>The <code>propagation</code> module transforms graphical input data into a set of hidden-feature graphs. The <code>readout</code> module aggregates these feature graphs into a single hidden feature vector of fixed length (i.e., a vector of summary statistics). The summary network is then defined as the composition of the propagation and readout modules.</p><p>Optionally, one may also include a module that extracts features directly  from the graph, through the keyword argument <code>globalfeatures</code>. This module,  when applied to a <code>GNNGraph</code>, should return a matrix of features,  where the columns of the matrix correspond to the independent replicates  (e.g., a 5x10 matrix is expected for 5 hidden features for each of 10  independent replicates stored in the graph).  </p><p>The data should be stored as a <code>GNNGraph</code> or <code>Vector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain subgraphs corresponding to independent replicates.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux, GraphNeuralNetworks
using Flux: batch
using Statistics: mean

# Propagation module
d = 1      # dimension of response variable
nâ‚• = 32    # dimension of node feature vectors
propagation = GNNChain(GraphConv(d =&gt; nâ‚•), GraphConv(nâ‚• =&gt; nâ‚•))

# Readout module
readout = GlobalPool(mean)
náµ£ = nâ‚•   # dimension of readout vector

# Summary network
Ïˆ = GNNSummary(propagation, readout)

# Inference network
p = 3     # number of parameters in the statistical model
w = 64    # width of hidden layer
Ï• = Chain(Dense(náµ£, w, relu), Dense(w, p))

# Construct the estimator
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator to a single graph, a single graph with subgraphs
# (corresponding to independent replicates), and a vector of graphs
# (corresponding to multiple data sets each with independent replicates)
gâ‚ = rand_graph(11, 30, ndata=rand(d, 11))
gâ‚‚ = rand_graph(13, 40, ndata=rand(d, 13))
gâ‚ƒ = batch([gâ‚, gâ‚‚])
Î¸Ì‚(gâ‚)
Î¸Ì‚(gâ‚ƒ)
Î¸Ì‚([gâ‚, gâ‚‚, gâ‚ƒ])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Graphs.jl#L486-L546">source</a></section></article><h1 id="User-defined-summary-statistics"><a class="docs-heading-anchor" href="#User-defined-summary-statistics">User-defined summary statistics</a><a id="User-defined-summary-statistics-1"></a><a class="docs-heading-anchor-permalink" href="#User-defined-summary-statistics" title="Permalink"></a></h1><ul></ul><p>The following functions correspond to summary statistics that are often useful as user-defined summary statistics in <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> objects.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.samplesize" href="#NeuralEstimators.samplesize"><code>NeuralEstimators.samplesize</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">samplesize(Z::AbstractArray)</code></pre><p>Computes the sample size of a set of independent realisations <code>Z</code>.</p><p>Note that this function is a wrapper around <a href="../utility/#NeuralEstimators.numberreplicates"><code>numberreplicates</code></a>, but this function returns the number of replicates as the eltype of <code>Z</code>, rather than as an integer.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/summarystatistics.jl#L3-L10">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.samplecorrelation" href="#NeuralEstimators.samplecorrelation"><code>NeuralEstimators.samplecorrelation</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">samplecorrelation(Z::AbstractArray)</code></pre><p>Computes the sample correlation matrix, RÌ‚, and returns the vectorised strict lower triangle of RÌ‚.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># 5 independent replicates of a 3-dimensional vector
z = rand(3, 5)
samplecorrelation(z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/summarystatistics.jl#L36-L48">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.samplecovariance" href="#NeuralEstimators.samplecovariance"><code>NeuralEstimators.samplecovariance</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">samplecovariance(Z::AbstractArray)</code></pre><p>Computes the <a href="https://en.wikipedia.org/wiki/Sample_mean_and_covariance#Definition_of_sample_covariance">sample covariance matrix</a>, Î£Ì‚, and returns the vectorised lower triangle of Î£Ì‚.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># 5 independent replicates of a 3-dimensional vector
z = rand(3, 5)
samplecovariance(z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/summarystatistics.jl#L13-L25">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.NeighbourhoodVariogram" href="#NeuralEstimators.NeighbourhoodVariogram"><code>NeuralEstimators.NeighbourhoodVariogram</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeighbourhoodVariogram(h_max, n_bins) 
(l::NeighbourhoodVariogram)(g::GNNGraph)</code></pre><p>Computes the empirical variogram, </p><p class="math-container">\[\hat{\gamma}(h \pm \delta) = \frac{1}{2|N(h \pm \delta)|} \sum_{(i,j) \in N(h \pm \delta)} (Z_i - Z_j)^2\]</p><p>where <span>$N(h \pm \delta) \equiv \left\{(i,j) : \|\boldsymbol{s}_i - \boldsymbol{s}_j\| \in (h-\delta, h+\delta)\right\}$</span>  is the set of pairs of locations separated by a distance within <span>$(h-\delta, h+\delta)$</span>, and <span>$|\cdot|$</span> denotes set cardinality. </p><p>The distance bins are constructed to have constant width <span>$2\delta$</span>, chosen based on the maximum distance  <code>h_max</code> to be considered, and the specified number of bins <code>n_bins</code>. </p><p>The input type is a <code>GNNGraph</code>, and the empirical variogram is computed based on the corresponding graph structure.  Specifically, only locations that are considered neighbours will be used when computing the empirical variogram. </p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Distances, LinearAlgebra
  
# Simulate Gaussian spatial data with exponential covariance function 
Î¸ = 0.1                                 # true range parameter 
n = 250                                 # number of spatial locations 
S = rand(n, 2)                          # spatial locations 
D = pairwise(Euclidean(), S, dims = 1)  # distance matrix 
Î£ = exp.(-D ./ Î¸)                       # covariance matrix 
L = cholesky(Symmetric(Î£)).L            # Cholesky factor 
m = 5                                   # number of independent replicates 
Z = L * randn(n, m)                     # simulated data 

# Construct the spatial graph 
r = 0.15                                # radius of neighbourhood set
g = spatialgraph(S, Z, r = r)

# Construct the variogram object wth 10 bins
nv = NeighbourhoodVariogram(r, 10) 

# Compute the empirical variogram 
nv(g)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/summarystatistics.jl#L113-L156">source</a></section></article><h2 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h2><p>In addition to the <a href="https://fluxml.ai/Flux.jl/stable/reference/models/layers/">built-in layers</a> provided by Flux, the following layers may be used when constructing a neural-network architecture. </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.DensePositive" href="#NeuralEstimators.DensePositive"><code>NeuralEstimators.DensePositive</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DensePositive(layer::Dense, g::Function)
DensePositive(layer::Dense; g::Function = Flux.relu)</code></pre><p>Wrapper around the standard <a href="https://fluxml.ai/Flux.jl/stable/models/layers/#Flux.Dense">Dense</a> layer that ensures positive weights (biases are left unconstrained).</p><p>This layer can be useful for constucting (partially) monotonic neural networks (see, e.g., <a href="../core/#NeuralEstimators.QuantileEstimatorContinuous"><code>QuantileEstimatorContinuous</code></a>).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

layer = DensePositive(Dense(5 =&gt; 2))
x = rand32(5, 64)
layer(x)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L657-L674">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PowerDifference" href="#NeuralEstimators.PowerDifference"><code>NeuralEstimators.PowerDifference</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PowerDifference(a, b)</code></pre><p>Function <span>$f(x, y) = |ax - (1-a)y|^b$</span> for trainable parameters a âˆˆ [0, 1] and b &gt; 0.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux 

# Generate some data 
d = 5
K = 10000
X = randn32(d, K)
Y = randn32(d, K)
XY = (X, Y)
a = 0.2f0
b = 1.3f0
Z = (abs.(a .* X - (1 .- a) .* Y)).^b

# Initialise layer
f = PowerDifference([0.5f0], [2.0f0])

# Optimise the layer 
loader = Flux.DataLoader((XY, Z), batchsize=32, shuffle=false)
optim = Flux.setup(Flux.Adam(0.01), f)  
for epoch in 1:100
    for (xy, z) in loader
        loss, grads = Flux.withgradient(f) do m
            Flux.mae(m(xy), z)
        end
        Flux.update!(optim, f, grads[1])
    end
end

# Estimates of a and b 
f.a 
f.b </code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L705-L742">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.SpatialGraphConv" href="#NeuralEstimators.SpatialGraphConv"><code>NeuralEstimators.SpatialGraphConv</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SpatialGraphConv(in =&gt; out, g=relu; args...)</code></pre><p>Implements a spatial graph convolution for isotropic processes, </p><p class="math-container">\[ \boldsymbol{h}^{(l)}_{j} =
 g\Big(
 \boldsymbol{\Gamma}_{\!1}^{(l)} \boldsymbol{h}^{(l-1)}_{j}
 +
 \boldsymbol{\Gamma}_{\!2}^{(l)} \bar{\boldsymbol{h}}^{(l)}_{j}
 +
 \boldsymbol{\gamma}^{(l)}
 \Big),
 \quad
 \bar{\boldsymbol{h}}^{(l)}_{j} = \sum_{j&#39; \in \mathcal{N}(j)}\boldsymbol{w}^{(l)}(\|\boldsymbol{s}_{j&#39;} - \boldsymbol{s}_j\|) \odot f^{(l)}(\boldsymbol{h}^{(l-1)}_{j}, \boldsymbol{h}^{(l-1)}_{j&#39;}),\]</p><p>where <span>$\boldsymbol{h}^{(l)}_{j}$</span> is the hidden feature vector at location <span>$\boldsymbol{s}_j$</span> at layer <span>$l$</span>, <span>$g(\cdot)$</span> is a non-linear activation function applied elementwise, <span>$\boldsymbol{\Gamma}_{\!1}^{(l)}$</span> and <span>$\boldsymbol{\Gamma}_{\!2}^{(l)}$</span> are trainable parameter matrices, <span>$\boldsymbol{\gamma}^{(l)}$</span> is a trainable bias vector, <span>$\mathcal{N}(j)$</span> denotes the indices of neighbours of <span>$\boldsymbol{s}_j$</span>, <span>$\boldsymbol{w}^{(l)}(\cdot)$</span> is a (learnable) spatial weighting function, <span>$\odot$</span> denotes elementwise multiplication,  and <span>$f^{(l)}(\cdot, \cdot)$</span> is a (learnable) function. </p><p>By default, the function <span>$f^{(l)}(\cdot, \cdot)$</span> is modelled using a <a href="#NeuralEstimators.PowerDifference"><code>PowerDifference</code></a> function.  One may alternatively employ a nonlearnable function, for example, <code>f = (háµ¢, hâ±¼) -&gt; (háµ¢ - hâ±¼).^2</code>,  specified through the keyword argument <code>f</code>.  </p><p>The spatial distances between locations must be stored as an edge feature, as facilitated by <a href="../utility/#NeuralEstimators.spatialgraph"><code>spatialgraph()</code></a>.  The input to <span>$\boldsymbol{w}(\cdot)$</span> is a <span>$1 \times n$</span> matrix (i.e., a row vector) of spatial distances.  The output of <span>$\boldsymbol{w}(\cdot)$</span> must be either a scalar; a vector of the same dimension as the feature vectors of the previous layer;  or, if the features vectors of the previous layer are scalars, a vector of arbitrary dimension.  To promote identifiability, the weights are normalised to sum to one (row-wise) within each neighbourhood set.  By default, <span>$\boldsymbol{w}(\cdot)$</span> is taken to be a multilayer perceptron with a single hidden layer,  although a custom choice for this function can be provided using the keyword argument <code>w</code>. </p><p><strong>Arguments</strong></p><ul><li><code>in</code>: The dimension of input features.</li><li><code>out</code>: The dimension of output features.</li><li><code>g = relu</code>: Activation function.</li><li><code>bias = true</code>: Add learnable bias?</li><li><code>init = glorot_uniform</code>: Initialiser for <span>$\boldsymbol{\Gamma}_{\!1}^{(l)}$</span>, <span>$\boldsymbol{\Gamma}_{\!2}^{(l)}$</span>, and <span>$\boldsymbol{\gamma}^{(l)}$</span>. </li><li><code>f = nothing</code></li><li><code>w = nothing</code> </li><li><code>w_width = 128</code>: (Only applicable if <code>w = nothing</code>) The width of the hidden layer in the MLP used to model <span>$\boldsymbol{w}(\cdot, \cdot)$</span>. </li><li><code>w_out = in</code>: (Only applicable if <code>w = nothing</code>) The output dimension of <span>$\boldsymbol{w}(\cdot, \cdot)$</span>.  </li><li><code>glob = false</code>: If <code>true</code>, global features will be computed directly from the entire spatial graph. These features are of the form: <span>$\boldsymbol{T} = \sum_{j=1}^n\sum_{j&#39; \in \mathcal{N}(j)}\boldsymbol{w}^{(l)}(\|\boldsymbol{s}_{j&#39;} - \boldsymbol{s}_j\|) \odot f^{(l)}(\boldsymbol{h}^{(l-1)}_{j}, \boldsymbol{h}^{(l-1)}_{j&#39;})$</span>. Note that these global features are no longer associated with a graph structure, and should therefore only be used in the final layer of a summary-statistics module. </li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux, GraphNeuralNetworks

# Toy spatial data
m = 5                  # number of replicates
d = 2                  # spatial dimension
n = 250                # number of spatial locations
S = rand(n, d)         # spatial locations
Z = rand(n, m)         # data
g = spatialgraph(S, Z) # construct the graph

# Construct and apply spatial graph convolution layer
l = SpatialGraphConv(1 =&gt; 10, relu)
l(g)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Graphs.jl#L237-L304">source</a></section></article><h1 id="Output-activation-functions"><a class="docs-heading-anchor" href="#Output-activation-functions">Output activation functions</a><a id="Output-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Output-activation-functions" title="Permalink"></a></h1><ul></ul><p>In addition to the <a href="https://fluxml.ai/Flux.jl/stable/models/activation/">standard activation functions</a> provided by Flux, the following structs can be used at the end of an architecture to act as output activation functions that ensure valid estimates for certain models. <strong>NB:</strong> Although we refer to the following objects as &quot;activation functions&quot;, they should be treated as layers that are included in the final stage of a Flux <code>Chain()</code>. </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.Compress" href="#NeuralEstimators.Compress"><code>NeuralEstimators.Compress</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Compress(a, b, k = 1)</code></pre><p>Layer that compresses its input to be within the range <code>a</code> and <code>b</code>, where each element of <code>a</code> is less than the corresponding element of <code>b</code>.</p><p>The layer uses a logistic function,</p><p class="math-container">\[l(Î¸) = a + \frac{b - a}{1 + e^{-kÎ¸}},\]</p><p>where the arguments <code>a</code> and <code>b</code> together combine to shift and scale the logistic function to the range (<code>a</code>, <code>b</code>), and the growth rate <code>k</code> controls the steepness of the curve.</p><p>The logistic function given <a href="https://en.wikipedia.org/wiki/Logistic_function">here</a> contains an additional parameter, Î¸â‚€, which is the input value corresponding to the functions midpoint. In <code>Compress</code>, we fix Î¸â‚€ = 0, since the output of a randomly initialised neural network is typically around zero.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

a = [25, 0.5, -pi/2]
b = [500, 2.5, 0]
p = length(a)
K = 100
Î¸ = randn(p, K)
l = Compress(a, b)
l(Î¸)

n = 20
Î¸Ì‚ = Chain(Dense(n, p), l)
Z = randn(n, K)
Î¸Ì‚(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L322-L359">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.CorrelationMatrix" href="#NeuralEstimators.CorrelationMatrix"><code>NeuralEstimators.CorrelationMatrix</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CorrelationMatrix(d)
(object::CorrelationMatrix)(x::Matrix, cholesky::Bool = false)</code></pre><p>Transforms a vector ğ¯ âˆˆ â„áµˆ to the parameters of an unconstrained <code>d</code>Ã—<code>d</code> correlation matrix or, if <code>cholesky = true</code>, the lower Cholesky factor of an unconstrained <code>d</code>Ã—<code>d</code> correlation matrix.</p><p>The expected input is a <code>Matrix</code> with T(<code>d</code>-1) = (<code>d</code>-1)<code>d</code>Ã·2 rows, where T(<code>d</code>-1) is the (<code>d</code>-1)th triangular number (the number of free parameters in an unconstrained <code>d</code>Ã—<code>d</code> correlation matrix), and the output is a <code>Matrix</code> of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different correlation matrices).</p><p>Internally, the layer constructs a valid Cholesky factor ğ‹ for a correlation matrix, and then extracts the strict lower triangle from the correlation matrix ğ‘ = ğ‹ğ‹&#39;. The lower triangle is extracted and vectorised in line with Julia&#39;s column-major ordering: for example, when modelling the correlation matrix</p><p class="math-container">\[\begin{bmatrix}
1   &amp; Râ‚â‚‚ &amp;  Râ‚â‚ƒ \\
Râ‚‚â‚ &amp; 1   &amp;  Râ‚‚â‚ƒ\\
Râ‚ƒâ‚ &amp; Râ‚ƒâ‚‚ &amp; 1\\
\end{bmatrix},\]</p><p>the rows of the matrix returned by a <code>CorrelationMatrix</code> layer are ordered as</p><p class="math-container">\[\begin{bmatrix}
Râ‚‚â‚ \\
Râ‚ƒâ‚ \\
Râ‚ƒâ‚‚ \\
\end{bmatrix},\]</p><p>which means that the output can easily be transformed into the implied correlation matrices using <a href="../utility/#NeuralEstimators.vectotril"><code>vectotril</code></a> and <code>Symmetric</code>.</p><p>See also <a href="#NeuralEstimators.CovarianceMatrix"><code>CovarianceMatrix</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using LinearAlgebra
using Flux

d  = 4
l  = CorrelationMatrix(d)
p  = (d-1)*dÃ·2
Î¸  = randn(p, 100)

# Returns a matrix of parameters, which can be converted to correlation matrices
R = l(Î¸)
R = map(eachcol(R)) do r
	R = Symmetric(cpu(vectotril(r, strict = true)), :L)
	R[diagind(R)] .= 1
	R
end

# Obtain the Cholesky factor directly
L = l(Î¸, true)
L = map(eachcol(L)) do x
	# Only the strict lower diagonal elements are returned
	L = LowerTriangular(cpu(vectotril(x, strict = true)))

	# Diagonal elements are determined under the constraint diag(L*L&#39;) = ğŸ
	L[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)
	L
end
L[1] * L[1]&#39;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L510-L582">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.CovarianceMatrix" href="#NeuralEstimators.CovarianceMatrix"><code>NeuralEstimators.CovarianceMatrix</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CovarianceMatrix(d)
(object::CovarianceMatrix)(x::Matrix, cholesky::Bool = false)</code></pre><p>Transforms a vector ğ¯ âˆˆ â„áµˆ to the parameters of an unconstrained <code>d</code>Ã—<code>d</code> covariance matrix or, if <code>cholesky = true</code>, the lower Cholesky factor of an unconstrained <code>d</code>Ã—<code>d</code> covariance matrix.</p><p>The expected input is a <code>Matrix</code> with T(<code>d</code>) = <code>d</code>(<code>d</code>+1)Ã·2 rows, where T(<code>d</code>) is the <code>d</code>th triangular number (the number of free parameters in an unconstrained <code>d</code>Ã—<code>d</code> covariance matrix), and the output is a <code>Matrix</code> of the same dimension. The columns of the input and output matrices correspond to independent parameter configurations (i.e., different covariance matrices).</p><p>Internally, the layer constructs a valid Cholesky factor ğ‹ and then extracts the lower triangle from the positive-definite covariance matrix ğšº = ğ‹ğ‹&#39;. The lower triangle is extracted and vectorised in line with Julia&#39;s column-major ordering: for example, when modelling the covariance matrix</p><p class="math-container">\[\begin{bmatrix}
Î£â‚â‚ &amp; Î£â‚â‚‚ &amp; Î£â‚â‚ƒ \\
Î£â‚‚â‚ &amp; Î£â‚‚â‚‚ &amp; Î£â‚‚â‚ƒ \\
Î£â‚ƒâ‚ &amp; Î£â‚ƒâ‚‚ &amp; Î£â‚ƒâ‚ƒ \\
\end{bmatrix},\]</p><p>the rows of the matrix returned by a <code>CovarianceMatrix</code> are ordered as</p><p class="math-container">\[\begin{bmatrix}
Î£â‚â‚ \\
Î£â‚‚â‚ \\
Î£â‚ƒâ‚ \\
Î£â‚‚â‚‚ \\
Î£â‚ƒâ‚‚ \\
Î£â‚ƒâ‚ƒ \\
\end{bmatrix},\]</p><p>which means that the output can easily be transformed into the implied covariance matrices using <a href="../utility/#NeuralEstimators.vectotril"><code>vectotril</code></a> and <code>Symmetric</code>.</p><p>See also <a href="#NeuralEstimators.CorrelationMatrix"><code>CorrelationMatrix</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using LinearAlgebra

d = 4
l = CovarianceMatrix(d)
p = d*(d+1)Ã·2
Î¸ = randn(p, 50)

# Returns a matrix of parameters, which can be converted to covariance matrices
Î£ = l(Î¸)
Î£ = [Symmetric(cpu(vectotril(x)), :L) for x âˆˆ eachcol(Î£)]

# Obtain the Cholesky factor directly
L = l(Î¸, true)
L = [LowerTriangular(cpu(vectotril(x))) for x âˆˆ eachcol(L)]
L[1] * L[1]&#39;</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/473f16b6903944f415ac4295a1d08ff919437f83/src/Architectures.jl#L399-L463">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../core/">Â« Core</a><a class="docs-footer-nextpage" href="../loss/">Loss functions Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.5.0 on <span class="colophon-date" title="Tuesday 16 July 2024 07:22">Tuesday 16 July 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
