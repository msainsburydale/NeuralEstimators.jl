<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Architectures and activations functions Â· NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li class="is-active"><a class="tocitem" href>Architectures and activations functions</a><ul class="internal"><li><a class="tocitem" href="#Index"><span>Index</span></a></li><li><a class="tocitem" href="#Architectures"><span>Architectures</span></a></li><li><a class="tocitem" href="#Layers"><span>Layers</span></a></li><li><a class="tocitem" href="#Output-activation-functions"><span>Output activation functions</span></a></li></ul></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Architectures and activations functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Architectures and activations functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/architectures.md" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Architectures-and-activations-functions"><a class="docs-heading-anchor" href="#Architectures-and-activations-functions">Architectures and activations functions</a><a id="Architectures-and-activations-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures-and-activations-functions" title="Permalink"></a></h1><h2 id="Index"><a class="docs-heading-anchor" href="#Index">Index</a><a id="Index-1"></a><a class="docs-heading-anchor-permalink" href="#Index" title="Permalink"></a></h2><ul><li><a href="#NeuralEstimators.CholeskyCovariance"><code>NeuralEstimators.CholeskyCovariance</code></a></li><li><a href="#NeuralEstimators.Compress"><code>NeuralEstimators.Compress</code></a></li><li><a href="#NeuralEstimators.CorrelationMatrix"><code>NeuralEstimators.CorrelationMatrix</code></a></li><li><a href="#NeuralEstimators.CovarianceMatrix"><code>NeuralEstimators.CovarianceMatrix</code></a></li><li><a href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a></li><li><a href="#NeuralEstimators.DeepSetExpert"><code>NeuralEstimators.DeepSetExpert</code></a></li><li><a href="#NeuralEstimators.GNN"><code>NeuralEstimators.GNN</code></a></li><li><a href="#NeuralEstimators.SplitApply"><code>NeuralEstimators.SplitApply</code></a></li><li><a href="#NeuralEstimators.UniversalPool"><code>NeuralEstimators.UniversalPool</code></a></li><li><a href="#NeuralEstimators.WeightedGraphConv"><code>NeuralEstimators.WeightedGraphConv</code></a></li></ul><h2 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h2><p>Although the user is free to construct their neural estimator however they see fit, <code>NeuralEstimators</code> provides several useful architectures described below.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(Ïˆ, Ï•, a)
DeepSet(Ïˆ, Ï•; a::String = &quot;mean&quot;)</code></pre><p>The Deep Set representation,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•(ğ“(ğ™)),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>where ğ™ â‰¡ (ğ™â‚&#39;, â€¦, ğ™â‚˜&#39;)&#39; are independent replicates from the model, <code>Ïˆ</code> and <code>Ï•</code> are neural networks, and <code>a</code> is a permutation-invariant aggregation function.</p><p>To make the architecture agnostic to the sample size <span>$m$</span>, the aggregation function <code>a</code> must aggregate over the replicates. It can be specified as a positional argument of type <code>Function</code>, or as a keyword argument with permissible values <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code>, and <code>&quot;logsumexp&quot;</code>.</p><p><code>DeepSet</code> objects act on data stored as <code>Vector{A}</code>, where each element of the vector is associated with one parameter vector (i.e., one set of independent replicates), and where <code>A</code> depends on the form of the data and the chosen architecture for <code>Ïˆ</code>. As a rule of thumb, when the data are stored as an array, the replicates are stored in the final dimension of the array. (This is usually the &#39;batch&#39; dimension, but batching with <code>DeepSets</code> is done at the set level, i.e., sets of replicates are batched together.) For example, with gridded spatial data and <code>Ïˆ</code> a CNN, <code>A</code> should be a 4-dimensional array, with the replicates stored in the 4áµ—Ê° dimension.</p><p>Note that, internally, data stored as <code>Vector{Arrays}</code> are first concatenated along the replicates dimension before being passed into the inner neural network <code>Ïˆ</code>; this means that <code>Ïˆ</code> is applied to a single large array rather than many small arrays, which can substantially improve computational efficiency, particularly on the GPU.</p><p>Set-level information, <span>$ğ±$</span>, that is not a function of the data can be passed directly into the outer network <code>Ï•</code> in the following manner,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ±&#39;)&#39;),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>This is done by providing a <code>Tuple{Vector{A}, Vector{B}}</code>, where the first element of the tuple contains the vector of data sets and the second element contains the vector of set-level information.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï• = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator
Zâ‚ = rand(n, 3);                  # single set of 3 realisations
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 3)]; # two sets each containing 3 realisations
Zâ‚ƒ = [rand(n, m) for m âˆˆ (3, 4)]; # two sets containing 3 and 4 realisations
Î¸Ì‚(Zâ‚)
Î¸Ì‚(Zâ‚‚)
Î¸Ì‚(Zâ‚ƒ)

# Repeat the above but with set-level information:
qâ‚“ = 2
Ï•  = Chain(Dense(w + qâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSet(Ïˆ, Ï•)
xâ‚ = rand(qâ‚“)
xâ‚‚ = [rand(qâ‚“) for _ âˆˆ eachindex(Zâ‚‚)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚‚, xâ‚‚))
Î¸Ì‚((Zâ‚ƒ, xâ‚‚))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L35-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSetExpert" href="#NeuralEstimators.DeepSetExpert"><code>NeuralEstimators.DeepSetExpert</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSetExpert(Ïˆ, Ï•, S, a)
DeepSetExpert(Ïˆ, Ï•, S; a::String = &quot;mean&quot;)
DeepSetExpert(deepset::DeepSet, Ï•, S)</code></pre><p>Identical to <code>DeepSet</code>, but with additional expert summary statistics,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ’(ğ™)&#39;)&#39;),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>where <code>S</code> is a function that returns a vector of expert summary statistics.</p><p>The constructor <code>DeepSetExpert(deepset::DeepSet, Ï•, S)</code> inherits <code>Ïˆ</code> and <code>a</code> from <code>deepset</code>.</p><p>Similarly to <code>DeepSet</code>, set-level information can be incorporated by passing a <code>Tuple</code>, in which case we have</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ’(ğ™)&#39;, ğ±&#39;)&#39;),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}).\]</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qâ‚› = 1
qâ‚œ = 32
w = 16
Ïˆ = Chain(Dense(n, w, relu), Dense(w, qâ‚œ, relu));
Ï• = Chain(Dense(qâ‚œ + qâ‚›, w), Dense(w, p));
Î¸Ì‚ = DeepSetExpert(Ïˆ, Ï•, S)

# Apply the estimator
Zâ‚ = rand(n, 3);                  # single set
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 4)]; # two sets
Î¸Ì‚(Zâ‚)
Î¸Ì‚(Zâ‚‚)

# Repeat the above but with set-level information:
qâ‚“ = 2
Ï•  = Chain(Dense(qâ‚œ + qâ‚› + qâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSetExpert(Ïˆ, Ï•, S)
xâ‚ = rand(qâ‚“)
xâ‚‚ = [rand(qâ‚“) for _ âˆˆ eachindex(Zâ‚‚)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚‚, xâ‚‚))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L213-L268">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.GNN" href="#NeuralEstimators.GNN"><code>NeuralEstimators.GNN</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GNN(propagation, readout, Ï•, a)
GNN(propagation, readout, Ï•, a::String = &quot;mean&quot;)</code></pre><p>A graph neural network (GNN) designed for parameter point estimation.</p><p>The <code>propagation</code> module transforms graphical input data into a set of hidden-feature graphs; the <code>readout</code> module aggregates these feature graphs into a single hidden feature vector of fixed length; the function <code>a</code>(â‹…) is a permutation-invariant aggregation function, and <code>Ï•</code> is a neural network.</p><p>The data should be stored as a <code>GNNGraph</code> or <code>AbstractVector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Propagation module
d = 1      # dimension of response variable
nh = 32    # dimension of node feature vectors
propagation = GNNChain(GraphConv(d =&gt; nh), GraphConv(nh =&gt; nh), GraphConv(nh =&gt; nh))

# Readout module (using &quot;universal pooling&quot;)
nt = 64   # dimension of the summary vector for each node
no = 128  # dimension of the final summary vector for each graph
readout = UniversalPool(Dense(nh, nt), Dense(nt, nt))

# Alternative readout module (using the elementwise average)
# readout = GlobalPool(mean); no = nh

# Mapping module
p = 3     # number of parameters in the statistical model
w = 64    # width of layers used for the mapping network Ï•
Ï• = Chain(Dense(no, w, relu), Dense(w, w, relu), Dense(w, p))

# Construct the estimator
Î¸Ì‚ = GNN(propagation, readout, Ï•)

# Apply the estimator to:
# 	1. a single graph,
# 	2. a single graph with sub-graphs (corresponding to independent replicates), and
# 	3. a vector of graphs (corresponding to multiple spatial data sets).
gâ‚ = rand_graph(11, 30, ndata=rand(d, 11))
gâ‚‚ = rand_graph(13, 40, ndata=rand(d, 13))
gâ‚ƒ = batch([gâ‚, gâ‚‚])
Î¸Ì‚(gâ‚)
Î¸Ì‚(gâ‚ƒ)
Î¸Ì‚([gâ‚, gâ‚‚, gâ‚ƒ])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Graphs.jl#L409-L464">source</a></section></article><h2 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.WeightedGraphConv" href="#NeuralEstimators.WeightedGraphConv"><code>NeuralEstimators.WeightedGraphConv</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightedGraphConv(in =&gt; out, Ïƒ=identity; aggr=mean, bias=true, init=glorot_uniform)</code></pre><p>Same as regular <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GraphConv"><code>GraphConv</code></a> layer, but where the neighbours of a node are weighted by their spatial distance to that node.</p><p><strong>Arguments</strong></p><ul><li><code>in</code>: The dimension of input features.</li><li><code>out</code>: The dimension of output features.</li><li><code>Ïƒ</code>: Activation function.</li><li><code>aggr</code>: Aggregation operator for the incoming messages (e.g. <code>+</code>, <code>*</code>, <code>max</code>, <code>min</code>, and <code>mean</code>).</li><li><code>bias</code>: Add learnable bias.</li><li><code>init</code>: Weights&#39; initializer.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using GraphNeuralNetworks

# Construct a spatially-weighted adjacency matrix based on k-nearest neighbours
# with k = 5, and convert to a graph with random (uncorrelated) dummy data:
n = 100
S = rand(n, 2)
d = 1 # dimension of each observation (univariate data here)
A = adjacencymatrix(S, 5)
Z = GNNGraph(A, ndata = rand(d, n))

# Construct the layer and apply it to the data to generate convolved features
layer = WeightedGraphConv(d =&gt; 16)
layer(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Graphs.jl#L85-L114">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.UniversalPool" href="#NeuralEstimators.UniversalPool"><code>NeuralEstimators.UniversalPool</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">UniversalPool(Ïˆ, Ï•)</code></pre><p>Pooling layer (i.e., readout layer) from the paper <a href="https://ieeexplore.ieee.org/document/8852103">&#39;Universal Readout for Graph Convolutional Neural Networks&#39;</a>. It takes the form,</p><p class="math-container">\[\mathbf{V} = Ï•(|G|â»Â¹ \sum_{s\in G} Ïˆ(\mathbf{h}_s)),\]</p><p>where <span>$\mathbf{V}$</span> denotes the summary vector for graph <span>$G$</span>, <span>$\mathbf{h}_s$</span> denotes the vector of hidden features for node <span>$s \in G$</span>, and <code>Ïˆ</code> and <code>Ï•</code> are dense neural networks.</p><p>See also the pooling layers available from <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/stable/api/pool/"><code>GraphNeuralNetworks.jl</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using NeuralEstimators
using Flux
using GraphNeuralNetworks
using Graphs: random_regular_graph

# Construct an input graph G
n_h     = 16  # dimension of each feature node
n_nodes = 10
n_edges = 4
G = GNNGraph(random_regular_graph(n_nodes, n_edges), ndata = rand(n_h, n_nodes))

# Construct the pooling layer
n_t = 32  # dimension of the summary vector for each node
n_v = 64  # dimension of the final summary vector V
Ïˆ = Dense(n_h, n_t)
Ï• = Dense(n_t, n_v)
pool = UniversalPool(Ïˆ, Ï•)

# Apply the pooling layer
pool(G)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Graphs.jl#L352-L388">source</a></section></article><h2 id="Output-activation-functions"><a class="docs-heading-anchor" href="#Output-activation-functions">Output activation functions</a><a id="Output-activation-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Output-activation-functions" title="Permalink"></a></h2><p>These layers can be used at the end of an architecture to ensure that the neural estimator provides valid parameters.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Compress" href="#NeuralEstimators.Compress"><code>NeuralEstimators.Compress</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Compress(a, b, k = 1)</code></pre><p>Layer that compresses its input to be within the range <code>a</code> and <code>b</code>, where each element of <code>a</code> is less than the corresponding element of <code>b</code>.</p><p>The layer uses a logistic function,</p><p class="math-container">\[l(Î¸) = a + \frac{b - a}{1 + e^{-kÎ¸}},\]</p><p>where the arguments <code>a</code> and <code>b</code> together combine to shift and scale the logistic function to the desired range, and the growth rate <code>k</code> controls the steepness of the curve.</p><p>The logistic function given <a href="https://en.wikipedia.org/wiki/Logistic_function">here</a> contains an additional parameter, Î¸â‚€, which is the input value corresponding to the functions midpoint. In <code>Compress</code>, we fix Î¸â‚€ = 0, since the output of a randomly initialised neural network is typically around zero.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

a = [25, 0.5, -pi/2]
b = [500, 2.5, 0]
p = length(a)
K = 100
Î¸ = randn(p, K)
l = Compress(a, b)
l(Î¸)

n = 20
Î¸Ì‚ = Chain(Dense(n, p), l)
Z = randn(n, K)
Î¸Ì‚(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L377-L415">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.CholeskyCovariance" href="#NeuralEstimators.CholeskyCovariance"><code>NeuralEstimators.CholeskyCovariance</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CholeskyCovariance(d)</code></pre><p>Layer for constructing the parameters of the lower Cholesky factor associated with an unconstrained <code>d</code>Ã—<code>d</code> covariance matrix.</p><p>The layer transforms a <code>Matrix</code> with <code>d</code>(<code>d</code>+1)Ã·2 rows into a <code>Matrix</code> of the same dimension, but with <code>d</code> rows constrained to be positive (corresponding to the diagonal elements of the Cholesky factor) and the remaining rows unconstrained.</p><p>The ordering of the transformed <code>Matrix</code> aligns with Julia&#39;s column-major ordering. For example, when modelling the Cholesky factor,</p><p class="math-container">\[\begin{bmatrix}
Lâ‚â‚ &amp;     &amp;     \\
Lâ‚‚â‚ &amp; Lâ‚‚â‚‚ &amp;     \\
Lâ‚ƒâ‚ &amp; Lâ‚ƒâ‚‚ &amp; Lâ‚ƒâ‚ƒ \\
\end{bmatrix},\]</p><p>the rows of the matrix returned by a <code>CholeskyCovariance</code> layer will be ordered as</p><p class="math-container">\[Lâ‚â‚, Lâ‚‚â‚, Lâ‚ƒâ‚, Lâ‚‚â‚‚, Lâ‚ƒâ‚‚, Lâ‚ƒâ‚ƒ,\]</p><p>which means that the output can easily be transformed into the implied Cholesky factors using <a href="../utility/#NeuralEstimators.vectotril"><code>vectotril</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

d = 4
p = d*(d+1)Ã·2
Î¸ = randn(p, 50)
l = CholeskyCovariance(d)
Î¸ = l(Î¸)                              # returns matrix (used for Flux networks)
L = [vectotril(y) for y âˆˆ eachcol(Î¸)] # convert matrix to Cholesky factors</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L570-L612">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.CovarianceMatrix" href="#NeuralEstimators.CovarianceMatrix"><code>NeuralEstimators.CovarianceMatrix</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CovarianceMatrix(d)</code></pre><p>Layer for constructing the parameters of an unconstrained <code>d</code>Ã—<code>d</code> covariance matrix.</p><p>The layer transforms a <code>Matrix</code> with <code>d</code>(<code>d</code>+1)Ã·2 rows into a <code>Matrix</code> of the same dimension.</p><p>Internally, it uses a <code>CholeskyCovariance</code> layer to construct a valid Cholesky factor ğ‹, and then extracts the lower triangle from the positive-definite covariance matrix ğšº = ğ‹ğ‹&#39;. The lower triangle is extracted and vectorised in line with Julia&#39;s column-major ordering. For example, when modelling the covariance matrix,</p><p class="math-container">\[\begin{bmatrix}
Î£â‚â‚ &amp; Î£â‚â‚‚ &amp; Î£â‚â‚ƒ \\
Î£â‚‚â‚ &amp; Î£â‚‚â‚‚ &amp; Î£â‚‚â‚ƒ \\
Î£â‚ƒâ‚ &amp; Î£â‚ƒâ‚‚ &amp; Î£â‚ƒâ‚ƒ \\
\end{bmatrix},\]</p><p>the rows of the matrix returned by a <code>CovarianceMatrix</code> layer will be ordered as</p><p class="math-container">\[Î£â‚â‚, Î£â‚‚â‚, Î£â‚ƒâ‚, Î£â‚‚â‚‚, Î£â‚ƒâ‚‚, Î£â‚ƒâ‚ƒ,\]</p><p>which means that the output can easily be transformed into the implied covariance matrices using <a href="../utility/#NeuralEstimators.vectotril"><code>vectotril</code></a> and <code>Symmetric</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using LinearAlgebra

d = 4
p = d*(d+1)Ã·2
Î¸ = randn(p, 50)

l = CovarianceMatrix(d)
Î¸ = l(Î¸)
Î£ = [Symmetric(cpu(vectotril(y)), :L) for y âˆˆ eachcol(Î¸)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L630-L674">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.CorrelationMatrix" href="#NeuralEstimators.CorrelationMatrix"><code>NeuralEstimators.CorrelationMatrix</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">CorrelationMatrix(d)</code></pre><p>Layer for constructing the parameters of an unconstrained <code>d</code>Ã—<code>d</code> correlation matrix.</p><p>The layer transforms a <code>Matrix</code> with <code>d</code>(<code>d</code>-1)Ã·2 rows into a <code>Matrix</code> with the same dimension.</p><p>Internally, the layers uses the algorithm described <a href="https://mc-stan.org/docs/reference-manual/cholesky-factors-of-correlation-matrices-1.html#cholesky-factor-of-correlation-matrix-inverse-transform">here</a> and <a href="https://mc-stan.org/docs/reference-manual/correlation-matrix-transform.html#correlation-matrix-transform.section">here</a> to construct a valid Cholesky factor ğ‹, and then extracts the strict lower triangle from the positive-definite correlation matrix ğ‘ = ğ‹ğ‹&#39;. The strict lower triangle is extracted and vectorised in line with Julia&#39;s column-major ordering. For example, when modelling the correlation matrix,</p><p class="math-container">\[\begin{bmatrix}
1   &amp; Râ‚â‚‚ &amp;  Râ‚â‚ƒ \\
Râ‚‚â‚ &amp; 1   &amp;  Râ‚‚â‚ƒ\\
Râ‚ƒâ‚ &amp; Râ‚ƒâ‚‚ &amp; 1\\
\end{bmatrix},\]</p><p>the rows of the matrix returned by a <code>CorrelationMatrix</code> layer will be ordered as</p><p class="math-container">\[Râ‚‚â‚, Râ‚ƒâ‚, Râ‚ƒâ‚‚,\]</p><p>which means that the output can easily be transformed into the implied correlation matrices using the strict variant of <a href="../utility/#NeuralEstimators.vectotril"><code>vectotril</code></a> and <code>Symmetric</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using LinearAlgebra

d = 4
p = d*(d-1)Ã·2
l = CorrelationMatrix(d)
Î¸ = randn(p, 50)

# returns a matrix of parameters
Î¸ = l(Î¸)

# convert matrix of parameters to implied correlation matrices
R = map(eachcol(Î¸)) do y
	R = Symmetric(cpu(vectotril(y, strict = true)), :L)
	R[diagind(R)] .= 1
	R
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L473-L526">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.SplitApply" href="#NeuralEstimators.SplitApply"><code>NeuralEstimators.SplitApply</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">SplitApply(layers, indices)</code></pre><p>Splits an array into multiple sub-arrays by subsetting the rows using the collection of <code>indices</code>, and then applies each layer in <code>layers</code> to the corresponding sub-array.</p><p>Specifically, for each <code>i</code> = 1, â€¦, <span>$n$</span>, with <span>$n$</span> the number of <code>layers</code>, <code>SplitApply(x)</code> performs <code>layers[i](x[indices[i], :])</code>, and then vertically concatenates the resulting transformed arrays.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

d = 4
K = 50
pâ‚ = 2          # number of non-covariance matrix parameters
pâ‚‚ = d*(d+1)Ã·2  # number of covariance matrix parameters
p = pâ‚ + pâ‚‚

a = [0.1, 4]
b = [0.9, 9]
lâ‚ = Compress(a, b)
lâ‚‚ = CovarianceMatrix(d)
l = SplitApply([lâ‚, lâ‚‚], [1:pâ‚, pâ‚+1:p])

Î¸ = randn(p, K)
l(Î¸)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/1a4044aed10a8efb8909110293aae8ab8cfce77e/src/Architectures.jl#L430-L459">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../core/">Â« Core</a><a class="docs-footer-nextpage" href="../loss/">Loss functions Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.25 on <span class="colophon-date" title="Wednesday 30 August 2023 03:45">Wednesday 30 August 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
