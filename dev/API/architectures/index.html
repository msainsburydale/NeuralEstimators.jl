<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Architectures · NeuralEstimators.jl</title><meta name="title" content="Architectures · NeuralEstimators.jl"/><meta property="og:title" content="Architectures · NeuralEstimators.jl"/><meta property="twitter:title" content="Architectures · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li class="is-active"><a class="tocitem" href>Architectures</a><ul class="internal"><li><a class="tocitem" href="#Layers"><span>Layers</span></a></li></ul></li><li><a class="tocitem" href="../summarystatistics/">User-defined summary statistics</a></li><li><a class="tocitem" href="../activationfunctions/">Output activation functions</a></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Architectures</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Architectures</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/architectures.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h1><ul><li><a href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a></li><li><a href="#NeuralEstimators.GNN"><code>NeuralEstimators.GNN</code></a></li><li><a href="#NeuralEstimators.UniversalPool"><code>NeuralEstimators.UniversalPool</code></a></li><li><a href="#NeuralEstimators.WeightedGraphConv"><code>NeuralEstimators.WeightedGraphConv</code></a></li></ul><p>Although the user is free to construct their neural estimator however they see fit (i.e., using arbitrary <code>Flux</code> code), <code>NeuralEstimators</code> provides several useful architectures described below that are specifically relevant to neural estimation. See also the convenience constructor <a href="../utility/#NeuralEstimators.initialise_estimator"><code>initialise_estimator</code></a>.  </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(ψ, ϕ, a; S = nothing)
DeepSet(ψ, ϕ; a::String = &quot;mean&quot;, S = nothing)</code></pre><p>The DeepSets representation,</p><p class="math-container">\[θ̂(𝐙) = ϕ(𝐓(𝐙)),	 	 𝐓(𝐙) = 𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\}),\]</p><p>where 𝐙 ≡ (𝐙₁&#39;, …, 𝐙ₘ&#39;)&#39; are independent replicates from the statistical model, <code>ψ</code> and <code>ϕ</code> are neural networks, and <code>a</code> is a permutation-invariant function. Expert summary statistics can be incorporated as,</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐒(𝐙)&#39;)&#39;),\]</p><p>where <code>S</code> is a function that returns a vector of user-defined summary statistics. These user-defined summary statistics are typically provided either as a <code>Function</code> that returns a <code>Vector</code>, or a vector of such functions.</p><p>To ensure that the architecture is agnostic to the sample size <span>$m$</span>, the aggregation function <code>a</code> must aggregate over the replicates. It can be specified as a positional argument of type <code>Function</code>, or as a keyword argument with permissible values <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code>, and <code>&quot;logsumexp&quot;</code>.</p><p><code>DeepSet</code> objects act on data of type <code>Vector{A}</code>, where each element of the vector is associated with one data set (i.e., one set of independent replicates from the statistical model), and where the type <code>A</code> depends on the form of the data and the chosen architecture for <code>ψ</code>. As a rule of thumb, when <code>A</code> is an array, the replicates are stored in the final dimension. The final dimension is usually the &#39;batch&#39; dimension, but batching with <code>DeepSet</code> objects is done at the data set level (i.e., sets of replicates are batched together). For example, with gridded spatial data and <code>ψ</code> a CNN, <code>A</code> should be a 4-dimensional array, with the replicates stored in the 4ᵗʰ dimension.</p><p>Set-level information, <span>$𝐱$</span>, that is not a function of the data can be passed directly into the outer network <code>ϕ</code> in the following manner,</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐱&#39;)&#39;),	 	 \]</p><p>or, in the case that expert summary statistics are also used,</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐒(𝐙)&#39;, 𝐱&#39;)&#39;).	 \]</p><p>This is done by providing a <code>Tuple{Vector{A}, Vector{Vector}}</code>, where the first element of the tuple contains a vector of data sets and the second element contains a vector of set-level information (i.e., one vector for each data set).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # dimension of each replicate
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qₛ = 1  # dimension of expert summary statistic
qₜ = 16 # dimension of neural summary statistic
w = 16 # width of each hidden layer
ψ = Chain(Dense(n, w, relu), Dense(w, qₜ, relu));
ϕ = Chain(Dense(qₜ + qₛ, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ, S = S)

# Toy data
Z = [rand(Float32, n, m) for m ∈ (3, 4)]; # two data sets containing 3 and 4 replicates

# Apply the data
θ̂(Z)

# Inference with set-level information
qₓ = 2 # dimension of set-level vector
ϕ  = Chain(Dense(qₜ + qₛ + qₓ, w, relu), Dense(w, p));
θ̂  = DeepSet(ψ, ϕ; S = S)
x  = [rand(Float32, qₓ) for _ ∈ eachindex(Z)]
θ̂((Z, x))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/2acc9133073b3706f010d796ce3f616981660199/src/Architectures.jl#L48-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.GNN" href="#NeuralEstimators.GNN"><code>NeuralEstimators.GNN</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GNN(propagation, readout, ϕ, a; S = nothing)
GNN(propagation, readout, ϕ; a::String = &quot;mean&quot;, S = nothing)</code></pre><p>A graph neural network (GNN) designed for parameter point estimation.</p><p>The <code>propagation</code> module transforms graphical input data into a set of hidden-feature graphs; the <code>readout</code> module aggregates these feature graphs into a single hidden feature vector of fixed length; the function <code>a</code>(⋅) is a permutation-invariant aggregation function, and <code>ϕ</code> is a neural network. Expert, user-defined summary statistics <code>S</code> can also be utilised, as described in <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a>.</p><p>The data should be stored as a <code>GNNGraph</code> or <code>Vector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Propagation module
d = 1      # dimension of response variable
nh = 32    # dimension of node feature vectors
propagation = GNNChain(GraphConv(d =&gt; nh), GraphConv(nh =&gt; nh), GraphConv(nh =&gt; nh))

# Readout module (using &quot;universal pooling&quot;)
nt = 64   # dimension of the summary vector for each node
no = 128  # dimension of the final summary vector for each graph
readout = UniversalPool(Dense(nh, nt), Dense(nt, nt))

# Alternative readout module (using the elementwise average)
# readout = GlobalPool(mean); no = nh

# Mapping module
p = 3     # number of parameters in the statistical model
w = 64    # width of layers used for the mapping network ϕ
ϕ = Chain(Dense(no, w, relu), Dense(w, w, relu), Dense(w, p))

# Construct the estimator
θ̂ = GNN(propagation, readout, ϕ)

# Apply the estimator to:
# 	1. a single graph,
# 	2. a single graph with sub-graphs (corresponding to independent replicates), and
# 	3. a vector of graphs (corresponding to multiple spatial data sets).
g₁ = rand_graph(11, 30, ndata=rand(d, 11))
g₂ = rand_graph(13, 40, ndata=rand(d, 13))
g₃ = batch([g₁, g₂])
θ̂(g₁)
θ̂(g₃)
θ̂([g₁, g₂, g₃])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/2acc9133073b3706f010d796ce3f616981660199/src/Graphs.jl#L605-L661">source</a></section></article><h2 id="Layers"><a class="docs-heading-anchor" href="#Layers">Layers</a><a id="Layers-1"></a><a class="docs-heading-anchor-permalink" href="#Layers" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.WeightedGraphConv" href="#NeuralEstimators.WeightedGraphConv"><code>NeuralEstimators.WeightedGraphConv</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">WeightedGraphConv(in =&gt; out, σ=identity; aggr=mean, bias=true, init=glorot_uniform)</code></pre><p>Same as regular <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/stable/api/conv/#GraphNeuralNetworks.GraphConv"><code>GraphConv</code></a> layer, but where the neighbours of a node are weighted by their spatial distance to that node.</p><p><strong>Arguments</strong></p><ul><li><code>in</code>: The dimension of input features.</li><li><code>out</code>: The dimension of output features.</li><li><code>σ</code>: Activation function.</li><li><code>aggr</code>: Aggregation operator for the incoming messages (e.g. <code>+</code>, <code>*</code>, <code>max</code>, <code>min</code>, and <code>mean</code>).</li><li><code>bias</code>: Add learnable bias.</li><li><code>init</code>: Weights&#39; initializer.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using GraphNeuralNetworks

# Construct a spatially-weighted adjacency matrix based on k-nearest neighbours
# with k = 5, and convert to a graph with random (uncorrelated) dummy data:
n = 100
S = rand(n, 2)
d = 1 # dimension of each observation (univariate data here)
A = adjacencymatrix(S, 5)
Z = GNNGraph(A, ndata = rand(d, n))

# Construct the layer and apply it to the data to generate convolved features
layer = WeightedGraphConv(d =&gt; 16)
layer(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/2acc9133073b3706f010d796ce3f616981660199/src/Graphs.jl#L83-L112">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.UniversalPool" href="#NeuralEstimators.UniversalPool"><code>NeuralEstimators.UniversalPool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">UniversalPool(ψ, ϕ)</code></pre><p>Pooling layer (i.e., readout layer) from the paper <a href="https://ieeexplore.ieee.org/document/8852103">&#39;Universal Readout for Graph Convolutional Neural Networks&#39;</a>. It takes the form,</p><p class="math-container">\[\mathbf{V} = ϕ(|G|⁻¹ \sum_{s\in G} ψ(\mathbf{h}_s)),\]</p><p>where <span>$\mathbf{V}$</span> denotes the summary vector for graph <span>$G$</span>, <span>$\mathbf{h}_s$</span> denotes the vector of hidden features for node <span>$s \in G$</span>, and <code>ψ</code> and <code>ϕ</code> are dense neural networks.</p><p>See also the pooling layers available from <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/stable/api/pool/"><code>GraphNeuralNetworks.jl</code></a>.</p><p><strong>Examples</strong></p><pre><code class="language-julia hljs">using NeuralEstimators
using Flux
using GraphNeuralNetworks
using Graphs: random_regular_graph

# Construct an input graph G
n_h     = 16  # dimension of each feature node
n_nodes = 10
n_edges = 4
G = GNNGraph(random_regular_graph(n_nodes, n_edges), ndata = rand(n_h, n_nodes))

# Construct the pooling layer
n_t = 32  # dimension of the summary vector for each node
n_v = 64  # dimension of the final summary vector V
ψ = Dense(n_h, n_t)
ϕ = Dense(n_t, n_v)
pool = UniversalPool(ψ, ϕ)

# Apply the pooling layer
pool(G)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/2acc9133073b3706f010d796ce3f616981660199/src/Graphs.jl#L548-L584">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../core/">« Core</a><a class="docs-footer-nextpage" href="../summarystatistics/">User-defined summary statistics »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Sunday 17 March 2024 00:51">Sunday 17 March 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
