<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core functions Â· NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core functions</a><ul class="internal"><li><a class="tocitem" href="#Sampling-parameters"><span>Sampling parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Neural-estimator-representations"><span>Neural-estimator representations</span></a></li><li><a class="tocitem" href="#Loss-functions"><span>Loss functions</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessing-a-neural-estimator"><span>Assessing a neural estimator</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Utility functions</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/core.md" title="Edit on GitHub"><span class="docs-icon fab">ï‚›</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Core-functions"><a class="docs-heading-anchor" href="#Core-functions">Core functions</a><a id="Core-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Core-functions" title="Permalink"></a></h1><p>This page documents the functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.</p><h2 id="Sampling-parameters"><a class="docs-heading-anchor" href="#Sampling-parameters">Sampling parameters</a><a id="Sampling-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> are stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution.</p><p>It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>Î¸</code> that stores the matrix of parameters. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with <a href="#NeuralEstimators.simulate"><code>simulate</code></a>.</p><p>The user-defined type must have a field <code>Î¸</code> that stores the <span>$p$</span> Ã— <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p>See <a href="../utility/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a> for the generic function for subsetting these objects.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">struct P &lt;: ParameterConfigurations
	Î¸
	# ...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Parameters.jl#L1-L22">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model (by overloading the generic function <code>simulate</code>).</p><p>The data should be stored as a <code>Vector{A}</code>, where each element of the vector is associated with one parameter configuration, and where <code>A</code> depends on the representation of the neural estimator. For example, if the neural estimator is a <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> object, the data should be stored as a <code>Vector{Array}</code>, where each array may store independent replicates in its final dimension. Similarly, if the neural estimator is a <a href="#NeuralEstimators.GNNEstimator"><code>GNNEstimator</code></a>, the data should be stored as a <code>Vector{GNNGraph}</code>, where each graph may store independent replicates in sub-graphs.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate" href="#NeuralEstimators.simulate"><code>NeuralEstimators.simulate</code></a> â€” <span class="docstring-category">Function</span></header><section><div><p>Generic function that may be overloaded to implicitly define a statistical model. Specifically, the user should provide a method <code>simulate(parameters, m)</code> that returns <code>m</code> simulated replicates for each element in the given set of <code>parameters</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/simulate.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate-Tuple{Any, Any, Integer}" href="#NeuralEstimators.simulate-Tuple{Any, Any, Integer}"><code>NeuralEstimators.simulate</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simulate(parameters, m, J::Integer)</code></pre><p>Simulates <code>J</code> sets of <code>m</code> independent replicates for each parameter vector in <code>parameters</code> by calling <code>simulate(parameters, m)</code> a total of <code>J</code> times.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/simulate.jl#L9-L14">source</a></section></article><h2 id="Neural-estimator-representations"><a class="docs-heading-anchor" href="#Neural-estimator-representations">Neural-estimator representations</a><a id="Neural-estimator-representations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-estimator-representations" title="Permalink"></a></h2><p>Although the user is free to construct their neural estimator however they see fit, <code>NeuralEstimators</code> provides several useful representations described below.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(Ïˆ, Ï•, a)
DeepSet(Ïˆ, Ï•; a::String = &quot;mean&quot;)</code></pre><p>A neural estimator in the <code>DeepSet</code> representation,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•(ğ“(ğ™)),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>where ğ™ â‰¡ (ğ™â‚&#39;, â€¦, ğ™â‚˜&#39;)&#39; are independent replicates from the model, <code>Ïˆ</code> and <code>Ï•</code> are neural networks, and <code>ğš</code> is a permutation-invariant aggregation function.</p><p>The function <code>ğš</code> must aggregate over the last dimension of an array (i.e., the replicates dimension). It can be specified as a positional argument of type <code>Function</code>, or as a keyword argument of type <code>String</code> with permissible values <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code>, and <code>&quot;logsumexp&quot;</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï• = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Apply the estimator to a single set of 3 realisations:
Zâ‚ = rand(n, 3);
Î¸Ì‚(Zâ‚)

# Apply the estimator to two sets each containing 3 realisations:
Zâ‚‚ = [rand(n, m) for m âˆˆ (3, 3)];
Î¸Ì‚(Zâ‚‚)

# Apply the estimator to two sets containing 3 and 4 realisations, respectively:
Zâ‚ƒ = [rand(n, m) for m âˆˆ (3, 4)];
Î¸Ì‚(Zâ‚ƒ)

# Repeat the above but with some covariates:
dâ‚“ = 2
Ï•â‚“ = Chain(Dense(w + dâ‚“, w, relu), Dense(w, p));
Î¸Ì‚  = DeepSet(Ïˆ, Ï•â‚“)
xâ‚ = rand(dâ‚“)
xâ‚‚ = [rand(dâ‚“), rand(dâ‚“)]
Î¸Ì‚((Zâ‚, xâ‚))
Î¸Ì‚((Zâ‚ƒ, xâ‚‚))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Architectures.jl#L28-L81">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSetExpert" href="#NeuralEstimators.DeepSetExpert"><code>NeuralEstimators.DeepSetExpert</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSetExpert(Ïˆ, Ï•, S, a)
DeepSetExpert(Ïˆ, Ï•, S; a::String)
DeepSetExpert(deepset::DeepSet, Ï•, S)</code></pre><p>A neural estimator in the <code>DeepSet</code> representation with additional expert summary statistics,</p><p class="math-container">\[Î¸Ì‚(ğ™) = Ï•((ğ“(ğ™)&#39;, ğ’(ğ™)&#39;)&#39;),	â€‚	â€‚ğ“(ğ™) = ğš(\{Ïˆ(ğ™áµ¢) : i = 1, â€¦, m\}),\]</p><p>where ğ™ â‰¡ (ğ™â‚&#39;, â€¦, ğ™â‚˜&#39;)&#39; are independent replicates from the model, <code>Ïˆ</code> and <code>Ï•</code> are neural networks, <code>S</code> is a function that returns a vector of expert summary statistics, and <code>ğš</code> is a permutation-invariant aggregation function.</p><p>The dimension of the domain of <code>Ï•</code> must be qâ‚œ + qâ‚›, where qâ‚œ and qâ‚› are the dimensions of the ranges of <code>Ïˆ</code> and <code>S</code>, respectively.</p><p>The constructor <code>DeepSetExpert(deepset::DeepSet, Ï•, S)</code> inherits <code>Ïˆ</code> and <code>a</code> from <code>deepset</code>.</p><p>See <code>?DeepSet</code> for discussion on the aggregation function <code>ğš</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qâ‚› = 1
qâ‚œ = 32
w = 16
Ïˆ = Chain(Dense(n, w, relu), Dense(w, qâ‚œ, relu));
Ï• = Chain(Dense(qâ‚œ + qâ‚›, w), Dense(w, p));
Î¸Ì‚ = DeepSetExpert(Ïˆ, Ï•, S)

# Apply the estimator to a single set of 3 realisations:
Z = rand(n, 3);
Î¸Ì‚(Z)

# Apply the estimator to two sets each containing 3 realisations:
Z = [rand(n, m) for m âˆˆ (3, 3)];
Î¸Ì‚(Z)

# Apply the estimator to two sets containing 3 and 4 realisations, respectively:
Z = [rand(n, m) for m âˆˆ (3, 4)];
Î¸Ì‚(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Architectures.jl#L214-L269">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PiecewiseEstimator(estimators, breaks)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of sample-size changepoints, <code>breaks</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Suppose that we&#39;ve trained two neural estimators. The first, Î¸Ì‚â‚, is trained
# for small sample sizes (e.g., m â‰¤ 30), and the second, `Î¸Ì‚â‚‚`, is trained for
# moderate-to-large sample sizes (e.g., m &gt; 30). Then we construct a piecewise
# estimator with a sample-size changepoint of 30, which dispatches Î¸Ì‚â‚ if m â‰¤ 30
# and Î¸Ì‚â‚‚ if m &gt; 30.

using NeuralEstimators
using Flux

n = 2  # bivariate data
p = 3  # number of parameters in the model
w = 8  # width of each layer

Ïˆâ‚ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï•â‚ = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚â‚ = DeepSet(Ïˆâ‚, Ï•â‚)

Ïˆâ‚‚ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï•â‚‚ = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚â‚‚ = DeepSet(Ïˆâ‚‚, Ï•â‚‚)

Î¸Ì‚ = PiecewiseEstimator([Î¸Ì‚â‚, Î¸Ì‚â‚‚], [30])
Z = [rand(n, 1, m) for m âˆˆ (10, 50)]
Î¸Ì‚(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Estimators.jl#L67-L100">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.GNNEstimator" href="#NeuralEstimators.GNNEstimator"><code>NeuralEstimators.GNNEstimator</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GNNEstimator(propagation, globalpool, deepset)</code></pre><p>A neural estimator based on a graph neural network (GNN). The <code>propagation</code> module transforms graphical input data into a set of hidden feature graphs; the <code>globalpool</code> module aggregates the feature graphs (graph-wise) into a single hidden feature vector; and the <code>deepset</code> module maps the hidden feature vectors onto the parameter space.</p><p>The data should be a <code>GNNGraph</code> or <code>AbstractVector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Create some graphs
d = 1             # dimension of the response variable
nâ‚, nâ‚‚ = 11, 27   # number of nodes
eâ‚, eâ‚‚ = 30, 50   # number of edges
gâ‚ = rand_graph(nâ‚, eâ‚, ndata=rand(d, nâ‚))
gâ‚‚ = rand_graph(nâ‚‚, eâ‚‚, ndata=rand(d, nâ‚‚))
g  = batch([gâ‚, gâ‚‚])

# propagation module
w = 5; o = 7
propagation = GNNChain(GraphConv(d =&gt; w), GraphConv(w =&gt; w), GraphConv(w =&gt; o))

# global pooling module
meanpool = GlobalPool(mean)

# Deep Set module
w = 32
p = 3
Ïˆâ‚‚ = Chain(Dense(o, w, relu), Dense(w, w, relu), Dense(w, w, relu))
Ï•â‚‚ = Chain(Dense(w, w, relu), Dense(w, p))
deepset = DeepSet(Ïˆâ‚‚, Ï•â‚‚)

# GNN estimator
est = GNNEstimator(propagation, meanpool, deepset)

# Apply the estimator to a single graph, a single graph containing sub-graphs,
# and a vector of graphs:
Î¸Ì‚ = est(gâ‚)
Î¸Ì‚ = est(g)
Î¸Ì‚ = est([gâ‚, gâ‚‚, g])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Architectures.jl#L375-L427">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Compress" href="#NeuralEstimators.Compress"><code>NeuralEstimators.Compress</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Compress(a, b)</code></pre><p>Uses the scaled logistic function to compress the output of a neural network to be between <code>a</code> and <code>b</code>.</p><p>The elements of <code>a</code> should be less than the corresponding element of <code>b</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

p = 3
a = [0.1, 4, 2]
b = [0.9, 9, 3]
l = Compress(a, b)
K = 10
Î¸ = rand(p, K)
l(Î¸)

n = 20
Z = rand(n, K)
Î¸Ì‚ = Chain(Dense(n, 15), Dense(15, p), l)
Î¸Ì‚(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/Architectures.jl#L642-L668">source</a></section></article><h2 id="Loss-functions"><a class="docs-heading-anchor" href="#Loss-functions">Loss functions</a><a id="Loss-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions" title="Permalink"></a></h2><p>In addition to the standard loss functions provided by <code>Flux</code> (e.g., <code>mae</code>, <code>mse</code>, etc.), <code>NeuralEstimators</code> provides the following loss functions.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.kpowerloss" href="#NeuralEstimators.kpowerloss"><code>NeuralEstimators.kpowerloss</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">kpowerloss(Î¸Ì‚, y, k; agg = mean, safeorigin = true, Ïµ = 0.1)</code></pre><p>For <code>k</code> âˆˆ (0, âˆ), the <code>k</code>-th power absolute-distance loss,</p><p class="math-container">\[L(Î¸Ì‚, Î¸) = |Î¸Ì‚ - Î¸|áµ,\]</p><p>contains the squared-error, absolute-error, and 0-1 loss functions as special cases (the latter obtained in the limit as <code>k</code> â†’ 0).</p><p>It is Lipschitz continuous iff <code>k</code> = 1, convex iff <code>k</code> â‰¥ 1, and strictly convex iff <code>k</code> &gt; 1. It is quasiconvex for all <code>k</code> &gt; 0.</p><p>If <code>safeorigin = true</code>, the loss function is modified to avoid pathologies around the origin, so that the resulting loss function behaves similarly to the absolute-error loss in the <code>Ïµ</code>-interval surrounding the origin.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/loss.jl#L16-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.quantileloss" href="#NeuralEstimators.quantileloss"><code>NeuralEstimators.quantileloss</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quantileloss(Î¸Ì‚, Î¸, q; agg = mean)
quantileloss(Î¸Ì‚, Î¸, q::V; agg = mean) where {T, V &lt;: AbstractVector{T}}</code></pre><p>The asymmetric loss function whose minimiser is the <code>q</code>th posterior quantile; namely,</p><p class="math-container">\[L(Î¸Ì‚, Î¸, q) = (Î¸Ì‚ - Î¸)(ğ•€(Î¸Ì‚ - Î¸ &gt; 0) - q),\]</p><p>where <code>q</code> âˆˆ (0, 1) and ğ•€(â‹…) is the indicator function.</p><p>The method that takes <code>q</code> as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in <code>Î¸Ì‚</code> is assumed to be pr, where p is the number of parameters: then, <code>q</code> should be an r-vector.</p><p>For further discussion on this loss function, see Equation (7) of Cressie, N. (2022), &quot;Decisions, decisions, decisions in an uncertain environment&quot;, arXiv:2209.13157.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">p = 1
K = 10
Î¸ = rand(p, K)
Î¸Ì‚ = rand(p, K)
quantileloss(Î¸Ì‚, Î¸, 0.1)

Î¸Ì‚ = rand(3p, K)
quantileloss(Î¸Ì‚, Î¸, [0.1, 0.5, 0.9])

p = 2
Î¸ = rand(p, K)
Î¸Ì‚ = rand(p, K)
quantileloss(Î¸Ì‚, Î¸, 0.1)

Î¸Ì‚ = rand(3p, K)
quantileloss(Î¸Ì‚, Î¸, [0.1, 0.5, 0.9])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/loss.jl#L58-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.intervalscore" href="#NeuralEstimators.intervalscore"><code>NeuralEstimators.intervalscore</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">intervalscore(l, u, Î¸, Î±; agg = mean)</code></pre><p>Given a 100Ã—(1-<code>Î±</code>)% confidence interval [<code>l</code>, <code>u</code>] with true value <code>Î¸</code>, the interval score is defined by</p><p class="math-container">\[S(l, u, Î¸; Î±) = (u - l) + 2Î±â»Â¹(l - Î¸)ğ•€(Î¸ &lt; l) + 2Î±â»Â¹(Î¸ - u)ğ•€(Î¸ &gt; u),\]</p><p>where <code>Î±</code> âˆˆ (0, 1) and ğ•€(â‹…) is the indicator function.</p><p>For further discussion, see Section 6 of Gneiting, T. and Raftery, A. E. (2007), &quot;Strictly proper scoring rules, prediction, and estimation&quot;, Journal of the American statistical Association, 102, 359â€“378.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/loss.jl#L141-L154">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> â€” <span class="docstring-category">Function</span></header><section><div><p>Generic function for training a neural estimator.</p><p>The methods are designed to cater for different forms of &quot;on-the-fly simulation&quot; (see the online documentation). In all methods, the validation data are held fixed to reduce noise when evaluating the validation risk function (which is used to monitor the performance of the estimator during training).</p><p><strong>Keyword arguments</strong></p><p>Arguments common to all methods:</p><ul><li><code>loss = mae</code>: the loss function, which should return the average loss when applied to multiple replicates.</li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM(1e-4)</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the trained <code>Î¸Ì‚</code> and other information; if savepath is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 5</code>: cease training if the risk doesn&#39;t improve in <code>stopping_epochs</code> epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p>Arguments common to <code>train(Î¸Ì‚, P, simulator)</code> and <code>train(Î¸Ì‚, Î¸_train, Î¸_val, simulator)</code>:</p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>).</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: should we simulate the data &quot;just-in-time&quot;? If <code>true</code>, the user must overload the generic function <code>simulate</code> with a method <code>simulate(parameters, m)</code>.</li></ul><p>Arguments unique to <code>train(Î¸Ì‚, P)</code>:</p><ul><li><code>K::Integer = 10000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K Ã· 5</code>.</li><li><code>Î¾ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices); if <code>Î¾</code> is provided, the constructor <code>P</code> is called as <code>P(K, Î¾)</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/train.jl#L1-L29">source</a></section></article><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>train(Î¸Ì‚, P)</code>. Check Documenter&#39;s build log for details.</p></div></div><div class="admonition is-warning"><header class="admonition-header">Missing docstring.</header><div class="admonition-body"><p>Missing docstring for <code>train(Î¸Ì‚, Î¸_train::P, Î¸_val::P) where {P &lt;: Union{AbstractMatrix, ParameterConfigurations}}</code>. Check Documenter&#39;s build log for details.</p></div></div><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}" href="#NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}"><code>NeuralEstimators.train</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(Î¸Ì‚, Î¸_train::P, Î¸_val::P, Z_train::T, Z_val::T; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>Î¸Ì‚</code> by providing the training and validation parameter sets, <code>Î¸_train</code> and <code>Î¸_val</code>, and the training and validation data sets, <code>Z_train</code> and <code>Z_val</code>, all of which are held fixed during training.</p><p>If the elements of <code>Z_train</code> and <code>Z_val</code> are equally replicated, and the number of replicates for each element of <code>Z_train</code> is a multiple of the number of replicates for each element of <code>Z_val</code>, then the training data will then be recycled throughout training to imitate on-the-fly simulation. For example, if each element of <code>Z_train</code> consists of 50 replicates, and each element of <code>Z_val</code> consists of 10 replicates, the first epoch uses the first 10 replicates in <code>Z_train</code>, the second epoch uses the next 10 replicates, and so on, until epoch 6 again uses the first 10 replicates. Note that this requires the data to be subsetted throughout training with the function <code>subsetdata</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/train.jl#L263-L279">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}" href="#NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}"><code>NeuralEstimators.train</code></a> â€” <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(Î¸Ì‚, Î¸_train::P, Î¸_val::P, Z_train::T, Z_val::T, M; &lt;keyword args&gt;)
train(Î¸Ì‚, Î¸_train::P, Î¸_val::P, Z_train::V, Z_val::V; &lt;keyword args&gt;) where {V &lt;: Vector{Vector{T}}}</code></pre><p>Train several neural estimators with the architecture <code>Î¸Ì‚</code> under different sample sizes.</p><p>The first method accepts sample sizes as a vector of integers, <code>M</code>. Each estimator is pre-trained with the estimator trained for the previous sample size. For example, if <code>M = [mâ‚, mâ‚‚]</code>, with <code>mâ‚‚</code> &gt; <code>mâ‚</code>, the estimator for sample size <code>mâ‚‚</code> is pre-trained with the estimator for sample size <code>mâ‚</code>.</p><p>The second method requires <code>Z_train</code> and <code>Z_val</code> to be a <code>Vector{Vector{T}}</code>, where <code>T</code> is arbitrary. In this method, a separate estimator is trained for each element in <code>Z_train</code> and <code>Z_val</code> and, importantly, it does not invoke <code>subsetdata()</code>, which can be slow for graphical data. Again, pre-training is used.</p><p>These methods wrap <code>train(Î¸Ì‚, Î¸_train, Î¸_val, Z_train, Z_val)</code> and, hence, they inherit its keyword arguments. Further, certain keyword arguments can be given as vectors. For instance, if we are training two neural estimators, we can use a different number of epochs by providing <code>epochs = [eâ‚, eâ‚‚]</code>. Other arguments that allow vectors are <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/train.jl#L398-L420">source</a></section></article><h2 id="Assessing-a-neural-estimator"><a class="docs-heading-anchor" href="#Assessing-a-neural-estimator">Assessing a neural estimator</a><a id="Assessing-a-neural-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-a-neural-estimator" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">assess(estimators, parameters, Z; &lt;keyword args&gt;)
assess(estimators, parameters; &lt;keyword args&gt;)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data <code>Z</code> simulated from a set of <code>parameters</code>.</p><p>The data <code>Z</code> should be an iterable collection, where each element contains testing data for a single sample size. If there are more simulated data sets than unique parameter vectors, the data should be stored in an &#39;outer&#39; fashion, so that the parameter vectors run faster than the replicated data.</p><p><strong>Keyword arguments</strong></p><ul><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible defaults provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible defaults provided). If <code>Î¾</code> is provided with a field <code>parameter_names</code>, those names will be used.</li><li><code>Î¾ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices).</li><li><code>use_Î¾ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses <code>Î¾</code>: if it does, the estimator will be applied as <code>estimator(Z, Î¾)</code>. This argument is useful when multiple <code>estimators</code> are provided, only some of which need <code>Î¾</code>; hence, if only one estimator is provided and <code>Î¾</code> is not <code>nothing</code>, <code>use_Î¾</code> is automatically set to <code>true</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
Ïˆ = Chain(Dense(n, w, relu), Dense(w, w, relu));
Ï• = Chain(Dense(w, w, relu), Dense(w, p));
Î¸Ì‚ = DeepSet(Ïˆ, Ï•)

# Generate fake parameters and corresponding data for a range of sample sizes:
K = 100
Î¸ = rand(p, K)
Z = [[rand(n, m) for _ âˆˆ 1:K] for m âˆˆ (1, 10, 20)]

assessment = assess([Î¸Ì‚], Î¸, Z)
risk(assessment)
risk(assessment, average_over_parameters = false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/assess.jl#L64-L107">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> â€” <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Assessment(Î¸andÎ¸Ì‚::DataFrame, runtime::DataFrame)</code></pre><p>A type for storing the output of <code>assess()</code>. It contains two fields. The field <code>runtime</code> contains the total <code>time</code> taken for each <code>estimator</code> for each sample size <code>m</code>. The field <code>Î¸andÎ¸Ì‚</code> is a long-form <code>DataFrame</code> containing the true parameters and corresponding estimates. Specifically, its columns are:</p><ul><li><code>estimator</code>: the name of the estimator</li><li><code>parameter</code>: the name of the parameter</li><li><code>truth</code>:     the true value of the parameter</li><li><code>estimate</code>:  the estimated value of the parameter</li><li><code>m</code>:         the sample size</li><li><code>k</code>:         the index of the parameter vector in the test set</li><li><code>j</code>: the index of the data set</li></ul><p>Multiple <code>Assessment</code> objects can be combined with the function <code>merge</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/assess.jl#L3-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.risk" href="#NeuralEstimators.risk"><code>NeuralEstimators.risk</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk(assessment::Assessment; loss = (x, y) -&gt; abs(x - y), average_over_parameters = true)</code></pre><p>Estimates the Bayes risk with respect to the <code>loss</code> function for each estimator, parameter, and sample size considered in <code>assessment</code>.</p><p>The argument <code>loss</code> should be a binary operator (default absolute-error loss).</p><p>If <code>average_over_parameters = true</code> (default), the risk is averaged over all parameters; otherwise, the risk is evaluated over each parameter separately.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/assess.jl#L37-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.coverage" href="#NeuralEstimators.coverage"><code>NeuralEstimators.coverage</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">coverage(ci::V, Î¸) where  {V &lt;: AbstractArray{M}} where M &lt;: AbstractMatrix</code></pre><p>Given a pÃ—K matrix of true parameters <code>Î¸</code>, determine the empirical coverage of the confidence intervals <code>ci</code> (a K-vector of px2 matrices).</p><p>The overall empirical coverage is obtained by averaging the resulting 0-1 matrix over all data sets.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/bootstrap.jl#L208-L216">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotrisk" href="#NeuralEstimators.plotrisk"><code>NeuralEstimators.plotrisk</code></a> â€” <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/plotting.jl#L9-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotdistribution" href="#NeuralEstimators.plotdistribution"><code>NeuralEstimators.plotdistribution</code></a> â€” <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/plotting.jl#L3-L5">source</a></section></article><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.bootstrap" href="#NeuralEstimators.bootstrap"><code>NeuralEstimators.bootstrap</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bootstrap(Î¸Ì‚, parameters::P, ZÌƒ) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(Î¸Ì‚, parameters::P, m::Integer; B = 400) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(Î¸Ì‚, Z; B = 400, blocks = nothing)</code></pre><p>Generates <code>B</code> bootstrap estimates from an estimator <code>Î¸Ì‚</code>.</p><p>Parametric bootstrapping is facilitated by passing a single parameter configuration, <code>parameters</code>, and corresponding simulated data, <code>ZÌƒ</code>, whose length implicitly defines <code>B</code>. Alternatively, if the user has defined a method <code>simulate(parameters, m)</code>, one may simply pass the desired sample size <code>m</code> for the simulated data sets.</p><p>Non-parametric bootstrapping is facilitated by passing a single data set, <code>Z</code>. The argument <code>blocks</code> caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>[1, 1, 2, 2, 2]</code>. The resampling algorithm aims to produce resampled data sets that are of a similar size to <code>Z</code>, but this can only be achieved exactly if all blocks are equal in length.</p><p>The keyword argument <code>use_gpu</code> is a flag determining whether to use the GPU, if it is available (default <code>true</code>).</p><p>The return type is a p Ã— <code>B</code> matrix, where p is the number of parameters in the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/bootstrap.jl#L99-L124">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.confidenceinterval" href="#NeuralEstimators.confidenceinterval"><code>NeuralEstimators.confidenceinterval</code></a> â€” <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">confidenceinterval(Î¸Ìƒ, Î¸Ì‚ = nothing; type, probs = [0.05, 0.95], parameter_names)</code></pre><p>Compute a confidence interval using the quantiles of the p Ã— B matrix of bootstrap samples, <code>Î¸Ìƒ</code>, where p is the number of parameters in the model.</p><p>The quantile levels are controlled with the argument <code>probs</code>. The rows can be named with a vector of strings <code>parameter_names</code> (sensible defaults provided).</p><p>The return type is a p Ã— 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the confidence interval.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
p = 3
B = 50
Î¸Ìƒ = rand(p, B)
Î¸Ì‚ = rand(p)
confidenceinterval(Î¸Ìƒ)
confidenceinterval(Î¸Ìƒ, Î¸Ì‚, type = &quot;basic&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/08980a4ca34042635fcc49f06a0e1ec5231d60e8/src/bootstrap.jl#L2-L24">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">Â« Advanced usage</a><a class="docs-footer-nextpage" href="../simulation/">Model-specific functions Â»</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Saturday 25 March 2023 03:51">Saturday 25 March 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
