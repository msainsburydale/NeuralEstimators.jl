<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core · NeuralEstimators.jl</title><meta name="title" content="Core · NeuralEstimators.jl"/><meta property="og:title" content="Core · NeuralEstimators.jl"/><meta property="twitter:title" content="Core · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core</a><ul class="internal"><li><a class="tocitem" href="#Sampling-parameters"><span>Sampling parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Estimators"><span>Estimators</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessment/calibration"><span>Assessment/calibration</span></a></li><li><a class="tocitem" href="#Inference-with-observed-data"><span>Inference with observed data</span></a></li></ul></li><li><a class="tocitem" href="../architectures/">Architectures</a></li><li><a class="tocitem" href="../summarystatistics/">User-defined summary statistics</a></li><li><a class="tocitem" href="../activationfunctions/">Output activation functions</a></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/core.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Core"><a class="docs-heading-anchor" href="#Core">Core</a><a id="Core-1"></a><a class="docs-heading-anchor-permalink" href="#Core" title="Permalink"></a></h1><p>This page documents the classes and functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these classes and functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to using a neural Bayes estimator to make inference with observed data sets.</p><h2 id="Sampling-parameters"><a class="docs-heading-anchor" href="#Sampling-parameters">Sampling parameters</a><a id="Sampling-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution are stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the statistical model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution.</p><p>It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the matrix of parameters. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.</p><p>The user-defined type must have a field <code>θ</code> that stores the <span>$p$</span> × <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p>See <a href="../utility/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a> for the generic function for subsetting these objects.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">struct P &lt;: ParameterConfigurations
	θ
	# other expensive intermediate objects...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Parameters.jl#L1-L22">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define their model via simulated data, either as fixed instances or via a function that simulates data from the statistical model.</p><p>The data are always stored as a <code>Vector{A}</code>, where each element of the vector corresponds to a data set of <span>$m$</span> independent replicates associated with one parameter vector (note that <span>$m$</span> is arbitrary), and where the type <code>A</code> depends on the multivariate structure of the data:</p><ul><li>For univariate and unstructured multivariate data, <code>A</code> is a <span>$d \times m$</span> matrix where <span>$d$</span> is the dimension each replicate (e.g., <span>$d=1$</span> for univariate data).</li><li>For data collected over a regular grid, <code>A</code> is a (<span>$D + 2$</span>)-dimensional array, where <span>$D$</span> is the dimension of the grid (e.g., <span>$D = 1$</span> for time series, <span>$D = 2$</span> for two-dimensional spatial grids, etc.). The first <span>$D$</span> dimensions of the array correspond to the dimensions of the grid; the penultimate dimension stores the so-called &quot;channels&quot; (this dimension is singleton for univariate processes, two for bivariate processes, and so on); and the final dimension stores the independent replicates. For example, to store 50 independent replicates of a bivariate spatial process measured over a 10x15 grid, one would construct an array of dimension 10x15x2x50.</li><li>For spatial data collected over irregular spatial locations, <code>A</code> is a <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/dev/api/gnngraph/#GraphNeuralNetworks.GNNGraphs.GNNGraph"><code>GNNGraph</code></a> with independent replicates (possibly with differing spatial locations) stored as subgraphs using the function <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/dev/api/gnngraph/#MLUtils.batch-Tuple{AbstractVector{%3C:GNNGraph}}"><code>batch</code></a>.</li></ul><h2 id="Estimators"><a class="docs-heading-anchor" href="#Estimators">Estimators</a><a id="Estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Estimators" title="Permalink"></a></h2><p>Several classes of neural estimators are available in the package.</p><p>The simplest class is <a href="#NeuralEstimators.PointEstimator"><code>PointEstimator</code></a>, used for constructing arbitrary mappings from the sample space to the parameter space. When constructing a generic point estimator, the user defines the loss function and therefore the Bayes estimator that will be targeted.</p><p>Several classes cater for the estimation of marginal posterior quantiles, based on the quantile loss function (see <a href="../loss/#NeuralEstimators.quantileloss"><code>quantileloss()</code></a>); in particular, see <a href="#NeuralEstimators.IntervalEstimator"><code>IntervalEstimator</code></a> and <a href="#NeuralEstimators.QuantileEstimatorDiscrete"><code>QuantileEstimatorDiscrete</code></a> for estimating marginal posterior quantiles for a fixed set of probability levels, and <a href="#NeuralEstimators.QuantileEstimatorContinuous"><code>QuantileEstimatorContinuous</code></a> for estimating marginal posterior quantiles with the probability level as an input to the neural network.</p><p>In addition to point estimation, the package also provides the class <a href="#NeuralEstimators.RatioEstimator"><code>RatioEstimator</code></a> for approximating the so-called likelihood-to-evidence ratio. The binary classification problem at the heart of this approach proceeds based on the binary cross-entropy loss.</p><p>Users are free to choose the neural-network architecture of these estimators as they see fit (subject to some class-specific requirements), but the package also provides the convenience constructor <a href="../utility/#NeuralEstimators.initialise_estimator"><code>initialise_estimator()</code></a>.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.NeuralEstimator" href="#NeuralEstimators.NeuralEstimator"><code>NeuralEstimators.NeuralEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralEstimator</code></pre><p>An abstract supertype for neural estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L1-L5">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PointEstimator" href="#NeuralEstimators.PointEstimator"><code>NeuralEstimators.PointEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PointEstimator(deepset::DeepSet)</code></pre><p>A neural point estimator, a mapping from the sample space to the parameter space.</p><p>The estimator leverages the <a href="../architectures/#NeuralEstimators.DeepSet"><code>DeepSet</code></a> architecture. The only requirement is that number of output neurons in the final layer of the inference network (i.e., the outer network) is equal to the number of parameters in the statistical model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L12-L20">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.IntervalEstimator" href="#NeuralEstimators.IntervalEstimator"><code>NeuralEstimators.IntervalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IntervalEstimator(u, v = u; probs = [0.025, 0.975], g::Function = exp)
IntervalEstimator(u, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)
IntervalEstimator(u, v, c::Union{Function,Compress}; probs = [0.025, 0.975], g::Function = exp)</code></pre><p>A neural interval estimator which, given data <span>$Z$</span>, jointly estimates marginal posterior credible intervals based on the probability levels <code>probs</code>.</p><p>The estimator employs a representation that prevents quantile crossing, namely, it constructs marginal posterior credible intervals for each parameter <span>$\theta_i$</span>, <span>$i = 1, \dots, p,$</span>  of the form,</p><p class="math-container">\[[c_i(u_i(\mathbf{Z})), \;\; c_i(u_i(\mathbf{Z})) + g(v_i(\mathbf{Z})))],\]</p><p>where  <span>$\mathbf{u}(⋅) \equiv (u_1(\cdot), \dots, u_p(\cdot))&#39;$</span> and <span>$\mathbf{v}(⋅) \equiv (v_1(\cdot), \dots, v_p(\cdot))&#39;$</span> are neural networks that transform data into <span>$p$</span>-dimensional vectors; <span>$g(\cdot)$</span> is a monotonically increasing function (e.g., exponential or softplus); and each <span>$c_i(⋅)$</span> is a monotonically increasing function that maps its input to the prior support of <span>$\theta_i$</span>.</p><p>The functions <span>$c_i(⋅)$</span> may be defined by a <span>$p$</span>-dimensional object of type <a href="../activationfunctions/#NeuralEstimators.Compress"><code>Compress</code></a>. If these functions are unspecified, they will be set to the identity function so that the range of the intervals will be unrestricted.</p><p>If only a single neural-network architecture is provided, it will be used for both <span>$\mathbf{u}(⋅)$</span> and <span>$\mathbf{v}(⋅)$</span>.</p><p>The return value  when applied to data is a matrix with <span>$2p$</span> rows, where the first and second <span>$p$</span> rows correspond to the lower and upper bounds, respectively.</p><p>See also <a href="#NeuralEstimators.QuantileEstimatorDiscrete"><code>QuantileEstimatorDiscrete</code></a> and <a href="#NeuralEstimators.QuantileEstimatorContinuous"><code>QuantileEstimatorContinuous</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

# Generate some toy data
n = 2   # bivariate data
m = 100 # number of independent replicates
Z = rand(n, m)

# prior
p = 3  # number of parameters in the statistical model
min_supp = [25, 0.5, -pi/2]
max_supp = [500, 2.5, 0]
g = Compress(min_supp, max_supp)

# Create an architecture
w = 8  # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
u = DeepSet(ψ, ϕ)

# Initialise the interval estimator
estimator = IntervalEstimator(u, g)

# Apply the (untrained) interval estimator
estimator(Z)
interval(estimator, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L33-L95">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.QuantileEstimatorDiscrete" href="#NeuralEstimators.QuantileEstimatorDiscrete"><code>NeuralEstimators.QuantileEstimatorDiscrete</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileEstimatorDiscrete(v; probs = [0.05, 0.25, 0.5, 0.75, 0.95], g = Flux.softplus)</code></pre><p>A neural estimator that jointly estimates a fixed set of marginal posterior quantiles with probability levels <span>$\{\tau_1, \dots, \tau_T\}$</span>, controlled by the keyword argument <code>probs</code>.</p><p>The estimator employs a representation that prevents quantile crossing, namely,</p><p class="math-container">\[\begin{aligned}
\hat{\mathbf{q}}^{(\tau_1)}(\mathbf{Z}) &amp;= \mathbf{v}^{(\tau_1)}(\mathbf{Z}),\\
\hat{\mathbf{q}}^{(\tau_t)}(\mathbf{Z}) &amp;= \mathbf{v}^{(\tau_1)}(\mathbf{Z}) + \sum_{j=2}^t g(\mathbf{v}^{(\tau_j)}(\mathbf{Z})), \quad t = 2, \dots, T,
\end{aligned}\]</p><p>where <span>$\mathbf{v}^{(\tau_t)}(\cdot)$</span>, <span>$t = 1, \dots, T$</span>, are unconstrained neural networks that transform data into <span>$p$</span>-dimensional vectors, and <span>$g(\cdot)$</span> is a non-negative function (e.g., exponential or softplus) applied elementwise to its arguments. In this implementation, the same neural-network architecture <code>v</code> is used for each <span>$\mathbf{v}^{(\tau_t)}(\cdot)$</span>, <span>$t = 1, \dots, T$</span>.</p><p>Note that one may use a simple <a href="#NeuralEstimators.PointEstimator"><code>PointEstimator</code></a> and the <a href="../loss/#NeuralEstimators.quantileloss"><code>quantileloss</code></a> to target a specific quantile.</p><p>The return value  when applied to data is a matrix with <span>$pT$</span> rows, where the first set of <span>$T$</span> rows corresponds to the estimated quantiles for the first parameter, the second set of <span>$T$</span> rows corresponds to the estimated quantiles for the second parameter, and so on.</p><p>See also <a href="#NeuralEstimators.IntervalEstimator"><code>IntervalEstimator</code></a> and <a href="#NeuralEstimators.QuantileEstimatorContinuous"><code>QuantileEstimatorContinuous</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux, Distributions

# Simple model Z|θ ~ N(θ, 1) with prior θ ~ N(0, 1)
d = 1   # dimension of each independent replicate
p = 1   # number of unknown parameters in the statistical model
m = 30  # number of independent replicates in each data set
prior(K) = randn32(p, K)
simulate(θ, m) = [μ .+ randn32(d, m) for μ ∈ eachcol(θ)]

# Architecture
ψ = Chain(Dense(d, 32, relu), Dense(32, 32, relu))
ϕ = Chain(Dense(32, 32, relu), Dense(32, p))
v = DeepSet(ψ, ϕ)

# Initialise the estimator
τ = [0.05, 0.25, 0.5, 0.75, 0.95]
q̂ = QuantileEstimatorDiscrete(v; probs = τ)

# Train the estimator
q̂ = train(q̂, prior, simulate, m = m)

# Closed-form posterior for comparison
function posterior(Z; μ₀ = 0, σ₀ = 1, σ² = 1)

	# Parameters of psoterior distribution
	μ̃ = (1/σ₀^2 + length(Z)/σ²)^-1 * (μ₀/σ₀^2 + sum(Z)/σ²)
	σ̃ = sqrt((1/σ₀^2 + length(Z)/σ²)^-1)

	# Posterior
	Normal(μ̃, σ̃)
end

# Estimate posterior quantiles for 1000 test data sets
θ = prior(1000)
Z = simulate(θ, m)
q̂(Z)                                             # neural quantiles
reduce(hcat, quantile.(posterior.(Z), Ref(τ)))   # true quantiles</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L118-L190">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.QuantileEstimatorContinuous" href="#NeuralEstimators.QuantileEstimatorContinuous"><code>NeuralEstimators.QuantileEstimatorContinuous</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileEstimatorContinuous(deepset::DeepSet)</code></pre><p>A neural estimator that estimates marginal posterior <span>$\tau$</span>-quantiles given as input the data <span>$\mathbf{Z}$</span> and the desired probability level <span>$\tau ∈ (0, 1)$</span>, therefore taking the form</p><p class="math-container">\[\hat{\mathbf{q}}(\mathbf{Z}, \tau), \quad \tau ∈ (0, 1).\]</p><p>The estimator leverages the <a href="../architectures/#NeuralEstimators.DeepSet"><code>DeepSet</code></a> architecture. The first requirement is that number of input neurons in the first layer of the inference network (i.e., the outer network) is one greater than the number of output neurons in the final layer of the summary network. The second requirement is that the number of output neurons in the final layer of the inference network is equal to the number <span>$p$</span> of parameters in the statistical model.</p><p>Although not a requirement, one may employ a (partially) monotonic neural network to prevent quantile crossing (i.e., to ensure that the <span>$\tau_1$</span>-quantile does not exceed the <span>$\tau_2$</span>-quantile for any <span>$\tau_2 &gt; \tau_1$</span>). There are several ways to construct such a neural network: one simple yet effective approach is to ensure that all weights associated with <span>$\tau$</span> are strictly positive (see, e.g., <a href="https://link.springer.com/article/10.1007/s00477-018-1573-6">Cannon, 2018</a>), and this can be done using the <a href="../architectures/#NeuralEstimators.DensePositive"><code>DensePositive</code></a> layer as illustrated in the examples below.</p><p>The return value when applied to data is a matrix with <span>$p$</span> rows, corresponding to the estimated marginal posterior quantiles for each parameter in the statistical model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux, Distributions, Statistics

# Simple model Z|θ ~ N(θ, 1) with prior θ ~ N(0, 1)
d = 1         # dimension of each independent replicate
p = 1         # number of unknown parameters in the statistical model
m = 30        # number of independent replicates in each data set
prior(K) = randn32(p, K)
simulateZ(θ, m) = [μ .+ randn32(d, m) for μ ∈ eachcol(θ)]
simulateτ(K)    = [rand32(1) for k in 1:K]
simulate(θ, m)  = simulateZ(θ, m), simulateτ(size(θ, 2))

# Architecture: partially monotonic network to preclude quantile crossing
w = 64  # width of each hidden layer
q = 2p  # output dimension of summary network
ψ = Chain(
	Dense(d, w, relu),
	Dense(w, w, relu),
	Dense(w, q, relu)
	)
ϕ = Chain(
	DensePositive(Dense(q + 1, w, relu); last_only = true),
	DensePositive(Dense(w, w, relu)),
	DensePositive(Dense(w, p))
	)
deepset = DeepSet(ψ, ϕ)

# Initialise the estimator
q̂ = QuantileEstimatorContinuous(deepset)

# Train the estimator
q̂ = train(q̂, prior, simulate, m = m)

# Closed-form posterior for comparison
function posterior(Z; μ₀ = 0, σ₀ = 1, σ² = 1)

	# Parameters of psoterior distribution
	μ̃ = (1/σ₀^2 + length(Z)/σ²)^-1 * (μ₀/σ₀^2 + sum(Z)/σ²)
	σ̃ = sqrt((1/σ₀^2 + length(Z)/σ²)^-1)

	# Posterior
	Normal(μ̃, σ̃)
end

# Estimate the posterior 0.1-quantile for 1000 test data sets
θ = prior(1000)
Z = simulateZ(θ, m)
τ = 0.1f0
q̂(Z, τ)                        # neural quantiles
quantile.(posterior.(Z), τ)&#39;   # true quantiles

# Estimate several quantiles for a single data set
z = Z[1]
τ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])
reduce(vcat, q̂.(Ref(z), τ))    # neural quantiles
quantile.(posterior(z), τ)     # true quantiles</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L217-L305">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.RatioEstimator" href="#NeuralEstimators.RatioEstimator"><code>NeuralEstimators.RatioEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">RatioEstimator(deepset::DeepSet)</code></pre><p>A neural estimator that estimates the likelihood-to-evidence ratio,</p><p class="math-container">\[r(\mathbf{Z}, \mathbf{\theta}) \equiv p(\mathbf{Z} \mid \mathbf{\theta})/p(\mathbf{Z}),\]</p><p>where <span>$p(\mathbf{Z} \mid \mathbf{\theta})$</span> is the likelihood and <span>$p(\mathbf{Z})$</span> is the marginal likelihood, also known as the model evidence.</p><p>The estimator leverages the <a href="../architectures/#NeuralEstimators.DeepSet"><code>DeepSet</code></a> architecture, subject to two requirements. First, the number of input neurons in the first layer of the inference network (i.e., the outer network) must equal the number of output neurons in the final layer of the summary network plus the number of parameters in the statistical model. Second, the number of output neurons in the final layer of the inference network must be equal to one.</p><p>The ratio estimator is trained by solving a relatively straightforward binary classification problem. Specifically, consider the problem of distinguishing dependent parameter–data pairs <span>${(\mathbf{\theta}&#39;, \mathbf{Z}&#39;)&#39; \sim p(\mathbf{Z}, \mathbf{\theta})}$</span> with class labels <span>$Y=1$</span> from independent parameter–data pairs <span>${(\tilde{\mathbf{\theta}}&#39;, \tilde{\mathbf{Z}}&#39;)&#39; \sim p(\mathbf{\theta})p(\mathbf{Z})}$</span> with class labels <span>$Y=0$</span>, and where the classes are balanced. Then the Bayes classifier under binary cross-entropy loss is given by</p><p class="math-container">\[c(\mathbf{Z}, \mathbf{\theta}) = \frac{p(\mathbf{Z}, \mathbf{\theta})}{p(\mathbf{Z}, \mathbf{\theta}) + p(\mathbf{\theta})p(\mathbf{Z})},\]</p><p>and hence,</p><p class="math-container">\[r(\mathbf{Z}, \mathbf{\theta}) = \frac{c(\mathbf{Z}, \mathbf{\theta})}{1 - c(\mathbf{Z}, \mathbf{\theta})}.\]</p><p>For numerical stability, training is done on the log-scale using <span>$\log r(\mathbf{Z}, \mathbf{\theta}) = \text{logit}(c(\mathbf{Z}, \mathbf{\theta}))$</span>.</p><p>When applying the estimator to data, by default the likelihood-to-evidence ratio <span>$r(\mathbf{Z}, \mathbf{\theta})$</span> is returned (setting the keyword argument <code>classifier = true</code> will yield class probability estimates). The estimated ratio can then be used in various downstream Bayesian (e.g., <a href="https://proceedings.mlr.press/v119/hermans20a.html">Hermans et al., 2020</a>) or Frequentist (e.g., <a href="https://arxiv.org/abs/2305.04634">Walchessen et al., 2023</a>) inferential algorithms.</p><p>See also <a href="#NeuralEstimators.mlestimate"><code>mlestimate</code></a> and <a href="#NeuralEstimators.mapestimate"><code>mapestimate</code></a> for obtaining approximate maximum-likelihood and maximum-a-posteriori estimates, and <a href="#NeuralEstimators.sampleposterior"><code>sampleposterior</code></a> for obtaining approximate posterior samples.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux, Statistics

# Generate data from Z|μ,σ ~ N(μ, σ²) with μ, σ ~ U(0, 1)
p = 2        # number of unknown parameters in the statistical model
d = 1        # dimension of each independent replicate
m = 100      # number of independent replicates
K = 10000    # number of training samples

prior(K) = rand32(p, K)
simulate(θ, m) = θ[1] .+ θ[2] .* randn32(d, m)
simulate(θ::AbstractMatrix, m) = simulate.(eachcol(θ), m)

θ_train = prior(K)
θ_val   = prior(K)
Z_train = simulate(θ_train, m)
Z_val   = simulate(θ_val, m)

# Architecture
w = 64 # width of each hidden layer
q = 2p # number of learned summary statistics
ψ = Chain(
	Dense(d, w, relu),
	Dropout(0.3),
	Dense(w, w, relu),
	Dropout(0.3),
	Dense(w, q, relu),
	Dropout(0.3)
	)
ϕ = Chain(
	Dense(q + p, w, relu),
	Dropout(0.3),
	Dense(w, w, relu),
	Dropout(0.3),
	Dense(w, 1)
	)
deepset = DeepSet(ψ, ϕ)

# Initialise the estimator
r̂ = RatioEstimator(deepset)

# Train the estimator
r̂ = train(r̂, θ_train, θ_val, Z_train, Z_val)

# Inference with &quot;observed&quot; data set
θ = prior(1)
z = simulate(θ, m)[1]
θ₀ = [0.5, 0.5]                           # initial estimate
mlestimate(r̂, z;  θ₀ = θ₀)                # maximum-likelihood estimate
mapestimate(r̂, z; θ₀ = θ₀)                # maximum-a-posteriori estimate
θ_grid = expandgrid(0:0.01:1, 0:0.01:1)&#39;  # fine gridding of the parameter space
r̂(z, θ_grid)                              # likelihood-to-evidence ratios over grid
sampleposterior(r̂, z; θ_grid = θ_grid)    # posterior samples</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L321-L424">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PiecewiseEstimator(estimators, breaks)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of changepoints, <code>breaks</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Suppose that we&#39;ve trained two neural estimators. The first, θ̂₁, is trained
# for small sample sizes (e.g., m ≤ 30), and the second, `θ̂₂`, is trained for
# moderate-to-large sample sizes (e.g., m &gt; 30). We construct a piecewise
# estimator with a sample-size changepoint of 30, which dispatches θ̂₁ if m ≤ 30
# and θ̂₂ if m &gt; 30.

using NeuralEstimators, Flux

n = 2  # bivariate data
p = 3  # number of parameters in the statistical model
w = 8  # width of each hidden layer

# Small-sample estimator
ψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₁ = PointEstimator(DeepSet(ψ₁, ϕ₁))

# Large-sample estimator
ψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₂ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₂ = PointEstimator(DeepSet(ψ₂, ϕ₂))

# Piecewise estimator
θ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], [30])

# Apply the estimator
Z = [rand(n, 1, m) for m ∈ (10, 50)]
θ̂(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/Estimators.jl#L458-L495">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>The function <a href="#NeuralEstimators.train"><code>train</code></a> is used to train a single neural estimator, while the wrapper function <a href="#NeuralEstimators.trainx"><code>trainx</code></a> is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, sampler::Function, simulator::Function; ...)
train(θ̂, θ_train::P, θ_val::P, simulator::Function; ...) where {P &lt;: Union{AbstractMatrix, ParameterConfigurations}}
train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; ...) where {T, P &lt;: Union{AbstractMatrix, ParameterConfigurations}}</code></pre><p>Train a neural estimator <code>θ̂</code>.</p><p>The methods cater for different variants of &quot;on-the-fly&quot; simulation. Specifically, a <code>sampler</code> can be provided to continuously sample new parameter vectors from the prior, and a <code>simulator</code> can be provided to continuously simulate new data conditional on the parameters. If provided with specific sets of parameters (<code>θ_train</code> and <code>θ_val</code>) and/or data (<code>Z_train</code> and <code>Z_val</code>), they will be held fixed during training.</p><p>In all methods, the validation parameters and data are held fixed to reduce noise when evaluating the validation risk.</p><p><strong>Keyword arguments common to all methods:</strong></p><ul><li><code>loss = mae</code></li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM()</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the neural-network weights during training (as <code>bson</code> files) and other information, such as the risk vs epoch (the risk function evaluated over the training and validation sets are saved in the first and second columns of <code>loss_per_epoch.csv</code>). If <code>savepath</code> is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 5</code>: cease training if the risk doesn&#39;t improve in this number of epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p><strong>Keyword arguments common to <code>train(θ̂, sampler, simulator)</code> and <code>train(θ̂, θ_train, θ_val, simulator)</code>:</strong></p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>). The <code>simulator</code> is called as <code>simulator(θ, m)</code>.</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: flag indicating whether we should simulate just-in-time, in the sense that only a <code>batchsize</code> number of parameter vectors and corresponding data are in memory at a given time.</li></ul><p><strong>Keyword arguments unique to <code>train(θ̂, sampler, simulator)</code>:</strong></p><ul><li><code>K::Integer = 10000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K ÷ 5</code>.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices). If provided, the parameter sampler is called as <code>sampler(K, ξ)</code>; otherwise, the parameter sampler will be called as <code>sampler(K)</code>. Can also be provided as <code>xi</code>.</li><li><code>epochs_per_θ_refresh::Integer = 1</code>: how often to refresh the training parameters. Must be a multiple of <code>epochs_per_Z_refresh</code>. Can also be provided as <code>epochs_per_theta_refresh</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

# parameter sampler
function sampler(K)
	μ = randn(K) # Gaussian prior
	σ = rand(K)  # Uniform prior
	θ = hcat(μ, σ)&#39;
	return θ
end

# data simulator
simulator(θ_matrix, m) = [θ[1] .+ θ[2] * randn32(1, m) for θ ∈ eachcol(θ_matrix)]

# architecture
d = 1   # dimension of each replicate
p = 2   # number of parameters in the statistical model
ψ = Chain(Dense(1, 32, relu), Dense(32, 32, relu))
ϕ = Chain(Dense(32, 32, relu), Dense(32, p))
θ̂ = DeepSet(ψ, ϕ)

# number of independent replicates to use during training
m = 15

# training: full simulation on-the-fly
θ̂ = train(θ̂, sampler, simulator, m = m, epochs = 5)

# training: simulation on-the-fly with fixed parameters
K = 10000
θ_train = sampler(K)
θ_val   = sampler(K ÷ 5)
θ̂ 		 = train(θ̂, θ_train, θ_val, simulator, m = m, epochs = 5)

# training: fixed parameters and fixed data
Z_train = simulator(θ_train, m)
Z_val   = simulator(θ_val, m)
θ̂ 		 = train(θ̂, θ_train, θ_val, Z_train, Z_val, epochs = 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/train.jl#L2-L77">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.trainx" href="#NeuralEstimators.trainx"><code>NeuralEstimators.trainx</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainx(θ̂, sampler::Function, simulator::Function, m::Vector{Integer}; ...)
trainx(θ̂, θ_train, θ_val, simulator::Function, m::Vector{Integer}; ...)
trainx(θ̂, θ_train, θ_val, Z_train, Z_val, m::Vector{Integer}; ...)
trainx(θ̂, θ_train, θ_val, Z_train::V, Z_val::V; ...) where {V &lt;: AbstractVector{AbstractVector{Any}}}</code></pre><p>A wrapper around <code>train()</code> to construct neural estimators for different sample sizes.</p><p>The positional argument <code>m</code> specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if <code>m = [m₁, m₂]</code>, the estimator for sample size <code>m₂</code> is pre-trained with the estimator for sample size <code>m₁</code>.</p><p>The method for <code>Z_train</code> and <code>Z_val</code> subsets the data using <code>subsetdata(Z, 1:mᵢ)</code> for each <code>mᵢ ∈ m</code>. The method for <code>Z_train::V</code> and <code>Z_val::V</code> trains an estimator for each element of <code>Z_train::V</code> and <code>Z_val::V</code> and, hence, it does not need to invoke <code>subsetdata()</code>, which can be slow or difficult to define in some cases (e.g., for graphical data). Note that, in this case, <code>m</code> is inferred from the data.</p><p>The keyword arguments inherit from <code>train()</code>. The keyword arguments <code>epochs</code>, <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code> can each be given as vectors. For example, if we are training two estimators, we can use a different number of epochs for each estimator by providing <code>epochs = [epoch₁, epoch₂]</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/train.jl#L556-L580">source</a></section></article><h2 id="Assessment/calibration"><a class="docs-heading-anchor" href="#Assessment/calibration">Assessment/calibration</a><a id="Assessment/calibration-1"></a><a class="docs-heading-anchor-permalink" href="#Assessment/calibration" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">assess(estimators, θ, Z)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data <code>Z</code> simulated based on true parameter vectors stored in <code>θ</code>.</p><p>The data <code>Z</code> should be a <code>Vector</code>, with each element corresponding to a single simulated data set. If <code>Z</code> contains more data sets than parameter vectors, the parameter matrix <code>θ</code> will be recycled by horizontal concatenation via the call <code>θ = repeat(θ, outer = (1, J))</code> where <code>J = length(Z) ÷ K</code> is the number of simulated data sets and <code>K = size(θ, 2)</code> is the number of parameter vectors.</p><p>The output is of type <code>Assessment</code>; see <code>?Assessment</code> for details.</p><p><strong>Keyword arguments</strong></p><ul><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible defaults provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible defaults provided). If <code>ξ</code> is provided with a field <code>parameter_names</code>, those names will be used.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices). Can also be provided as <code>xi</code>.</li><li><code>use_ξ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses <code>ξ</code>: if it does, the estimator will be applied as <code>estimator(Z, ξ)</code>. This argument is useful when multiple <code>estimators</code> are provided, only some of which need <code>ξ</code>; hence, if only one estimator is provided and <code>ξ</code> is not <code>nothing</code>, <code>use_ξ</code> is automatically set to <code>true</code>. Can also be provided as <code>use_xi</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Generate testing parameters
K = 100
θ = rand32(p, K)

# Data for a single sample size
m = 30
Z = [rand32(n, m) for _ ∈ 1:K];
assessment = assess(θ̂, θ, Z);
risk(assessment)

# Multiple data sets for each parameter vector
J = 5
Z = repeat(Z, J);
assessment = assess(θ̂, θ, Z);
risk(assessment)

# With set-level information
qₓ = 2
ϕ  = Chain(Dense(w + qₓ, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)
x = [rand(qₓ) for _ ∈ eachindex(Z)]
assessment = assess(θ̂, θ, (Z, x));
risk(assessment)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L298-L357">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Assessment(df::DataFrame, runtime::DataFrame)</code></pre><p>A type for storing the output of <code>assess()</code>. The field <code>runtime</code> contains the total time taken for each estimator. The field <code>df</code> is a long-form <code>DataFrame</code> with columns:</p><ul><li><code>estimator</code>: the name of the estimator</li><li><code>parameter</code>: the name of the parameter</li><li><code>truth</code>:     the true value of the parameter</li><li><code>estimate</code>:  the estimated value of the parameter</li><li><code>m</code>:         the sample size (number of iid replicates) for the given data set</li><li><code>k</code>:         the index of the parameter vector</li><li><code>j</code>:         the index of the data set (in the case that multiple data sets are associated with each parameter vector)</li></ul><p>Note that if <code>estimator</code> is an <code>IntervalEstimator</code>, the column <code>estimate</code> will be replaced by the columns <code>lower</code> and <code>upper</code>, containing the lower and upper bounds of the interval, respectively.</p><p>Multiple <code>Assessment</code> objects can be combined with <code>merge()</code> (used for combining assessments from multiple point estimators) or <code>join()</code> (used for combining assessments from a point estimator and an interval estimator).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L1-L21">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.diagnostics" href="#NeuralEstimators.diagnostics"><code>NeuralEstimators.diagnostics</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">diagnostics(assessment::Assessment; args...)</code></pre><p>Computes all applicable diagnostics.</p><p>For a <a href="#NeuralEstimators.PointEstimator"><code>PointEstimator</code></a>, the relevant diagnostics are the estimator&#39;s <a href="#NeuralEstimators.bias"><code>bias</code></a>, <a href="#NeuralEstimators.rmse"><code>rmse</code></a>, and <a href="#NeuralEstimators.risk"><code>risk</code></a>, while for an <a href="#NeuralEstimators.IntervalEstimator"><code>IntervalEstimator</code></a> the relevant diagnostics are the <a href="#NeuralEstimators.coverage"><code>coverage</code></a> and <a href="../loss/#NeuralEstimators.intervalscore"><code>intervalscore</code></a>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L263-L271">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.risk" href="#NeuralEstimators.risk"><code>NeuralEstimators.risk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk(assessment::Assessment; ...)</code></pre><p>Computes a Monte Carlo approximation of an estimator&#39;s Bayes risk,</p><p class="math-container">\[r(\hat{\boldsymbol{\theta}}(\cdot))
\approx
\frac{1}{K} \sum_{k=1}^K L(\boldsymbol{\theta}^{(k)}, \hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(k)})),\]</p><p>where <span>$\{\boldsymbol{\theta}^{(k)} : k = 1, \dots, K\}$</span> denotes a set of <span>$K$</span> parameter vectors sampled from the prior and, for each <span>$k$</span>, data <span>$\boldsymbol{Z}^{(k)}$</span> are simulated from the statistical model conditional on <span>$\boldsymbol{\theta}^{(k)}$</span>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>loss = (x, y) -&gt; abs(x - y)</code>: a binary operator defining the loss function (default absolute-error loss).</li><li><code>average_over_parameters::Bool = false</code>: if true, the loss is averaged over all parameters; otherwise (default), the loss is averaged over each parameter separately.</li><li><code>average_over_sample_sizes::Bool = true</code>: if true (default), the loss is averaged over all sample sizes <span>$m$</span>; otherwise, the loss is averaged over each sample size separately.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L108-L126">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.bias" href="#NeuralEstimators.bias"><code>NeuralEstimators.bias</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bias(assessment::Assessment; ...)</code></pre><p>Computes a Monte Carlo approximation of an estimator&#39;s bias,</p><p class="math-container">\[{\rm{bias}}(\hat{\boldsymbol{\theta}}(\cdot))
\approx
\frac{1}{K} \sum_{k=1}^K \hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(k)}) - \boldsymbol{\theta}^{(k)},\]</p><p>where <span>$\{\boldsymbol{\theta}^{(k)} : k = 1, \dots, K\}$</span> denotes a set of <span>$K$</span> parameter vectors sampled from the prior and, for each <span>$k$</span>, data <span>$\boldsymbol{Z}^{(k)}$</span> are simulated from the statistical model conditional on <span>$\boldsymbol{\theta}^{(k)}$</span>.</p><p>This function inherits the keyword arguments of <a href="#NeuralEstimators.risk"><code>risk</code></a> (excluding the argument <code>loss</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L146-L161">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.rmse" href="#NeuralEstimators.rmse"><code>NeuralEstimators.rmse</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rmse(assessment::Assessment; ...)</code></pre><p>Computes a Monte Carlo approximation of an estimator&#39;s root-mean-squared error,</p><p class="math-container">\[{\rm{rmse}}(\hat{\boldsymbol{\theta}}(\cdot))
\approx
\sqrt{\frac{1}{K} \sum_{k=1}^K (\hat{\boldsymbol{\theta}}(\boldsymbol{Z}^{(k)}) - \boldsymbol{\theta}^{(k)})^2},\]</p><p>where <span>$\{\boldsymbol{\theta}^{(k)} : k = 1, \dots, K\}$</span> denotes a set of <span>$K$</span> parameter vectors sampled from the prior and, for each <span>$k$</span>, data <span>$\boldsymbol{Z}^{(k)}$</span> are simulated from the statistical model conditional on <span>$\boldsymbol{\theta}^{(k)}$</span>.</p><p>This function inherits the keyword arguments of <a href="#NeuralEstimators.risk"><code>risk</code></a> (excluding the argument <code>loss</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L173-L188">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.coverage" href="#NeuralEstimators.coverage"><code>NeuralEstimators.coverage</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">coverage(assessment::Assessment; ...)</code></pre><p>Computes a Monte Carlo approximation of an interval estimator&#39;s expected coverage.</p><p><strong>Keyword arguments</strong></p><ul><li><code>average_over_parameters::Bool = false</code>: if true, the coverage is averaged over all parameters; otherwise (default), it is computed over each parameter separately.</li><li><code>average_over_sample_sizes::Bool = true</code>: if true (default), the coverage is averaged over all sample sizes <span>$m$</span>; otherwise, it is computed over each sample size separately.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/assess.jl#L203-L211">source</a></section></article><h2 id="Inference-with-observed-data"><a class="docs-heading-anchor" href="#Inference-with-observed-data">Inference with observed data</a><a id="Inference-with-observed-data-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-with-observed-data" title="Permalink"></a></h2><h3 id="Inference-using-point-estimators"><a class="docs-heading-anchor" href="#Inference-using-point-estimators">Inference using point estimators</a><a id="Inference-using-point-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-using-point-estimators" title="Permalink"></a></h3><p>Inference with a neural Bayes (point) estimator proceeds simply by applying the estimator <code>θ̂</code> to the observed data <code>Z</code> (possibly containing multiple data sets) in a call of the form <code>θ̂(Z)</code>. To leverage a GPU, simply move the estimator and the data to the GPU using <a href="https://fluxml.ai/Flux.jl/stable/models/functors/#Flux.gpu-Tuple{Any}"><code>gpu()</code></a>; see also <a href="../utility/#NeuralEstimators.estimateinbatches"><code>estimateinbatches()</code></a> to apply the estimator over batches of data, which can alleviate memory issues when working with a large number of data sets.</p><p>Uncertainty quantification often proceeds through the bootstrap distribution, which is essentially available &quot;for free&quot; when bootstrap data sets can be quickly generated; this is facilitated by <a href="#NeuralEstimators.bootstrap"><code>bootstrap()</code></a> and <a href="#NeuralEstimators.interval"><code>interval()</code></a>. Alternatively, one may approximate a set of low and high marginal posterior quantiles using a specially constructed neural Bayes estimator, which can then be used to construct credible intervals: see <a href="#NeuralEstimators.IntervalEstimator"><code>IntervalEstimator</code></a>, <a href="#NeuralEstimators.QuantileEstimatorDiscrete"><code>QuantileEstimatorDiscrete</code></a>, and <a href="#NeuralEstimators.QuantileEstimatorContinuous"><code>QuantileEstimatorContinuous</code></a>.  </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.bootstrap" href="#NeuralEstimators.bootstrap"><code>NeuralEstimators.bootstrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bootstrap(θ̂, parameters::P, Z) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, parameters::P, simulator, m::Integer; B = 400) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, Z; B = 400, blocks = nothing)</code></pre><p>Generates <code>B</code> bootstrap estimates from an estimator <code>θ̂</code>.</p><p>Parametric bootstrapping is facilitated by passing a single parameter configuration, <code>parameters</code>, and corresponding simulated data, <code>Z</code>, whose length implicitly defines <code>B</code>. Alternatively, one may provide a <code>simulator</code> and the desired sample size, in which case the data will be simulated using <code>simulator(parameters, m)</code>.</p><p>Non-parametric bootstrapping is facilitated by passing a single data set, <code>Z</code>. The argument <code>blocks</code> caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>[1, 1, 2, 2, 2]</code>. The resampling algorithm aims to produce resampled data sets that are of a similar size to <code>Z</code>, but this can only be achieved exactly if all blocks are equal in length.</p><p>The keyword argument <code>use_gpu</code> is a flag determining whether to use the GPU, if it is available (default <code>true</code>).</p><p>The return type is a p × <code>B</code> matrix, where p is the number of parameters in the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/inference.jl#L246-L271">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.interval" href="#NeuralEstimators.interval"><code>NeuralEstimators.interval</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">interval(θ::Matrix; probs = [0.05, 0.95], parameter_names = nothing)
interval(estimator::IntervalEstimator, Z; parameter_names = nothing, use_gpu = true)</code></pre><p>Compute a confidence interval based either on a <span>$p$</span> × <span>$B$</span> matrix <code>θ</code> of parameters (typically containing bootstrap estimates or posterior draws) with <span>$p$</span> the number of parameters in the model, or from an <code>IntervalEstimator</code> and data <code>Z</code>.</p><p>When given <code>θ</code>, the intervals are constructed by compute quantiles with probability levels controlled by the keyword argument <code>probs</code>.</p><p>The return type is a <span>$p$</span> × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval. The rows of this matrix can be named by passing a vector of strings to the keyword argument <code>parameter_names</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
p = 3
B = 50
θ = rand(p, B)
interval(θ)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/inference.jl#L158-L182">source</a></section></article><h3 id="Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators"><a class="docs-heading-anchor" href="#Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators">Inference using likelihood and likelihood-to-evidence-ratio estimators</a><a id="Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Inference-using-likelihood-and-likelihood-to-evidence-ratio-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.mlestimate" href="#NeuralEstimators.mlestimate"><code>NeuralEstimators.mlestimate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mlestimate(estimator::RatioEstimator, Z; θ₀ = nothing, θ_grid = nothing, penalty::Function = θ -&gt; 1, use_gpu = true)</code></pre><p>Computes the (approximate) maximum likelihood estimate given data <span>$\mathbf{Z}$</span>,</p><p class="math-container">\[\argmax_{\mathbf{\theta}} \ell(\mathbf{\theta} ; \mathbf{Z})\]</p><p>where <span>$\ell(\cdot ; \cdot)$</span> denotes the approximate log-likelihood function derived from <code>estimator</code>.</p><p>If a vector <code>θ₀</code> of initial parameter estimates is given, the approximate likelihood is maximised by gradient descent. Otherwise, if a matrix of parameters <code>θ_grid</code> is given, the approximate likelihood is maximised by grid search.</p><p>A maximum penalised likelihood estimate,</p><p class="math-container">\[\argmax_{\mathbf{\theta}} \ell(\mathbf{\theta} ; \mathbf{Z}) + \log p(\mathbf{\theta}),\]</p><p>can be obtained by specifying the keyword argument <code>penalty</code> that defines the penalty term <span>$p(\mathbf{\theta})$</span>.</p><p>See also <a href="#NeuralEstimators.mapestimate"><code>mapestimate()</code></a> for computing (approximate) maximum a posteriori estimates.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/inference.jl#L59-L81">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.mapestimate" href="#NeuralEstimators.mapestimate"><code>NeuralEstimators.mapestimate</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">mapestimate(estimator::RatioEstimator, Z; θ₀ = nothing, θ_grid = nothing, prior::Function = θ -&gt; 1, use_gpu = true)</code></pre><p>Computes the (approximate) maximum a posteriori estimate given data <span>$\mathbf{Z}$</span>,</p><p class="math-container">\[\argmax_{\mathbf{\theta}} \ell(\mathbf{\theta} ; \mathbf{Z}) + \log p(\mathbf{\theta})\]</p><p>where <span>$\ell(\cdot ; \cdot)$</span> denotes the approximate log-likelihood function derived from <code>estimator</code>, and <span>$p(\mathbf{\theta})$</span> denotes the prior density function controlled through the keyword argument <code>prior</code> (by default, a uniform prior is used).</p><p>If a vector <code>θ₀</code> of initial parameter estimates is given, the approximate posterior density is maximised by gradient descent. Otherwise, if a matrix of parameters <code>θ_grid</code> is given, the approximate posterior density is maximised by grid search.</p><p>See also <a href="#NeuralEstimators.mlestimate"><code>mlestimate()</code></a> for computing (approximate) maximum likelihood estimates.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/inference.jl#L85-L101">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.sampleposterior" href="#NeuralEstimators.sampleposterior"><code>NeuralEstimators.sampleposterior</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">sampleposterior(estimator::RatioEstimator, Z, N::Integer = 1000; θ_grid, prior::Function = θ -&gt; 1f0)</code></pre><p>Samples from the approximate posterior distribution <span>$p(\mathbf{\theta} \mid \mathbf{Z})$</span> implied by <code>estimator</code>.</p><p>The positional argument <code>N</code> controls the size of the posterior sample.</p><p>The keyword agument <code>θ_grid</code> requires a (fine) gridding of the parameter space, given as a matrix with <span>$p$</span> rows, with <span>$p$</span> the number of parameters in the statistical model.</p><p>The prior distribution <span>$p(\mathbf{\theta})$</span> is controlled through the keyword argument <code>prior</code> (by default, a uniform prior is used).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/487c387fcb47f761b151de79b185f0eda3e56c57/src/inference.jl#L8-L21">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">« Advanced usage</a><a class="docs-footer-nextpage" href="../architectures/">Architectures »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Friday 5 April 2024 07:26">Friday 5 April 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
