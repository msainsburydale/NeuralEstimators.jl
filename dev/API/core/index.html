<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core functions · NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core functions</a><ul class="internal"><li><a class="tocitem" href="#Sampling-parameters"><span>Sampling parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Types-of-estimators"><span>Types of estimators</span></a></li><li><a class="tocitem" href="#Architectures"><span>Architectures</span></a></li><li><a class="tocitem" href="#Loss-functions"><span>Loss functions</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessing-a-neural-estimator"><span>Assessing a neural estimator</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/core.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Core-functions"><a class="docs-heading-anchor" href="#Core-functions">Core functions</a><a id="Core-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Core-functions" title="Permalink"></a></h1><p>This page documents the functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.</p><h2 id="Sampling-parameters"><a class="docs-heading-anchor" href="#Sampling-parameters">Sampling parameters</a><a id="Sampling-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> are stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution.</p><p>It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the matrix of parameters. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with <a href="#NeuralEstimators.simulate-Tuple{Any, Any, Integer}"><code>simulate</code></a>.</p><p>The user-defined type must have a field <code>θ</code> that stores the <span>$p$</span> × <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p>See <a href="../utility/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a> for the generic function for subsetting these objects.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">struct P &lt;: ParameterConfigurations
	θ
	# other expensive intermediate objects...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Parameters.jl#L1-L22">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model.</p><p>The data should be stored as a <code>Vector{A}</code>, where each element of the vector is associated with one parameter configuration, and where <code>A</code> depends on the architecture of the neural estimator.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate-Tuple{Any, Any, Integer}" href="#NeuralEstimators.simulate-Tuple{Any, Any, Integer}"><code>NeuralEstimators.simulate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simulate(parameters, m, J::Integer)</code></pre><p>Simulates <code>J</code> sets of <code>m</code> independent replicates for each parameter vector in <code>parameters</code> by calling <code>simulate(parameters, m)</code> a total of <code>J</code> times, where the method <code>simulate(parameters, m)</code> is provided by the user via function overloading.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">import NeuralEstimators: simulate

p = 2
K = 10
m = 15
parameters = rand(p, K)

# Univariate Gaussian model with unknown mean and standard deviation
simulate(parameters, m) = [θ[1] .+ θ[2] .* randn(1, m) for θ ∈ eachcol(parameters)]
simulate(parameters, m)
simulate(parameters, m, 2)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/simulate.jl#L9-L31">source</a></section></article><h2 id="Types-of-estimators"><a class="docs-heading-anchor" href="#Types-of-estimators">Types of estimators</a><a id="Types-of-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Types-of-estimators" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.NeuralEstimator" href="#NeuralEstimators.NeuralEstimator"><code>NeuralEstimators.NeuralEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralEstimator</code></pre><p>An abstract supertype for neural estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Estimators.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.PointEstimator" href="#NeuralEstimators.PointEstimator"><code>NeuralEstimators.PointEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PointEstimator(arch)</code></pre><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Estimators.jl#L12-L16">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.IntervalEstimator" href="#NeuralEstimators.IntervalEstimator"><code>NeuralEstimators.IntervalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IntervalEstimator(arch_lower, arch_upper)
IntervalEstimator(arch)</code></pre><p>A neural estimator that produces credible intervals constructed as,</p><p class="math-container">\[[l(Z), l(Z) + \mathrm{exp}(u(Z))],\]</p><p>where <span>$l(⋅)$</span> and <span>$u(⋅)$</span> are the neural networks <code>arch_lower</code> and <code>arch_upper</code>, both of which should transform data into <span>$p$</span>-dimensional vectors, where <span>$p$</span> is the number of parameters in the model. If only a single neural network architecture <code>arch</code> is provided, it will be used for both <code>arch_lower</code> and <code>arch_upper</code>.</p><p>Internally, the output from <code>arch_lower</code> and <code>arch_upper</code> are concatenated, so that <code>IntervalEstimator</code> objects transform data into <span>$2p$</span>-dimensional vectors.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

# Generate some toy data
n = 2  # bivariate data
m = 10 # number of independent replicates
Z = rand(n, m)

# Create an architecture
p = 3  # parameters in the model
w = 8  # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
architecture = DeepSet(ψ, ϕ)

# Initialise the interval estimator
estimator = IntervalEstimator(architecture)

# Apply the interval estimator
estimator(Z)
interval(estimator, Z, parameter_names = [&quot;ρ&quot;, &quot;σ&quot;, &quot;τ&quot;])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Estimators.jl#L27-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.QuantileEstimator" href="#NeuralEstimators.QuantileEstimator"><code>NeuralEstimators.QuantileEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileEstimator()</code></pre><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Estimators.jl#L87-L91">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PiecewiseEstimator(estimators, breaks)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of sample-size changepoints, <code>breaks</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p>Any estimator can be included in <code>estimators</code>, including any of the subtypes of <code>NeuralEstimator</code> exported with the package (e.g., <code>PointEstimator</code>, <code>IntervalEstimator</code>, etc.).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Suppose that we&#39;ve trained two neural estimators. The first, θ̂₁, is trained
# for small sample sizes (e.g., m ≤ 30), and the second, `θ̂₂`, is trained for
# moderate-to-large sample sizes (e.g., m &gt; 30). Then we construct a piecewise
# estimator with a sample-size changepoint of 30, which dispatches θ̂₁ if m ≤ 30
# and θ̂₂ if m &gt; 30.

using NeuralEstimators
using Flux

n = 2  # bivariate data
p = 3  # number of parameters in the model
w = 8  # width of each layer

ψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₁ = DeepSet(ψ₁, ϕ₁)

ψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₂ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₂ = DeepSet(ψ₂, ϕ₂)

θ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], [30])
Z = [rand(n, 1, m) for m ∈ (10, 50)]
θ̂(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Estimators.jl#L102-L139">source</a></section></article><h2 id="Architectures"><a class="docs-heading-anchor" href="#Architectures">Architectures</a><a id="Architectures-1"></a><a class="docs-heading-anchor-permalink" href="#Architectures" title="Permalink"></a></h2><p>Although the user is free to construct their neural estimator however they see fit, <code>NeuralEstimators</code> provides several useful architectures described below. See also <a href="../utility/#Architecture-layers">Architecture layers</a> for some useful layers that can be used to ensure that the neural estimator provides valid parameters. </p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(ψ, ϕ, a)
DeepSet(ψ, ϕ; a::String = &quot;mean&quot;)</code></pre><p>The Deep Set representation,</p><p class="math-container">\[θ̂(𝐙) = ϕ(𝐓(𝐙)),	 	 𝐓(𝐙) = 𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\}),\]</p><p>where 𝐙 ≡ (𝐙₁&#39;, …, 𝐙ₘ&#39;)&#39; are independent replicates from the model, <code>ψ</code> and <code>ϕ</code> are neural networks, and <code>a</code> is a permutation-invariant aggregation function.</p><p>To make the architecture agnostic to the sample size <span>$m$</span>, the aggregation function <code>a</code> must aggregate over the replicates. It can be specified as a positional argument of type <code>Function</code>, or as a keyword argument with permissible values <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code>, and <code>&quot;logsumexp&quot;</code>.</p><p><code>DeepSet</code> objects act on data stored as <code>Vector{A}</code>, where each element of the vector is associated with one parameter vector (i.e., one set of independent replicates), and where <code>A</code> depends on the form of the data and the chosen architecture for <code>ψ</code>. As a rule of thumb, when the data are stored as an array, the replicates are stored in the final dimension of the array. (This is usually the &#39;batch&#39; dimension, but batching with <code>DeepSets</code> is done at the set level, i.e., sets of replicates are batched together.) For example, with gridded spatial data and <code>ψ</code> a CNN, <code>A</code> should be a 4-dimensional array, with the replicates stored in the 4ᵗʰ dimension.</p><p>Note that, internally, data stored as <code>Vector{Arrays}</code> are first concatenated along the replicates dimension before being passed into the inner neural network <code>ψ</code>; this means that <code>ψ</code> is applied to a single large array rather than many small arrays, which can substantially improve computational efficiency, particularly on the GPU.</p><p>Set-level information, <span>$𝐱$</span>, that is not a function of the data can be passed directly into the outer network <code>ϕ</code> in the following manner,</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐱&#39;)&#39;),	 	 𝐓(𝐙) = 𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\}),\]</p><p>This is done by providing a <code>Tuple{Vector{A}, Vector{B}}</code>, where the first element of the tuple contains the vector of data sets and the second element contains the vector of set-level information.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Apply the estimator
Z₁ = rand(n, 3);                  # single set of 3 realisations
Z₂ = [rand(n, m) for m ∈ (3, 3)]; # two sets each containing 3 realisations
Z₃ = [rand(n, m) for m ∈ (3, 4)]; # two sets containing 3 and 4 realisations
θ̂(Z₁)
θ̂(Z₂)
θ̂(Z₃)

# Repeat the above but with set-level information:
qₓ = 2
ϕ  = Chain(Dense(w + qₓ, w, relu), Dense(w, p));
θ̂  = DeepSet(ψ, ϕ)
x₁ = rand(qₓ)
x₂ = [rand(qₓ) for _ ∈ eachindex(Z₂)]
θ̂((Z₁, x₁))
θ̂((Z₂, x₂))
θ̂((Z₃, x₂))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Architectures.jl#L35-L112">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSetExpert" href="#NeuralEstimators.DeepSetExpert"><code>NeuralEstimators.DeepSetExpert</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSetExpert(ψ, ϕ, S, a)
DeepSetExpert(ψ, ϕ, S; a::String = &quot;mean&quot;)
DeepSetExpert(deepset::DeepSet, ϕ, S)</code></pre><p>Identical to <code>DeepSet</code>, but with additional expert summary statistics,</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐒(𝐙)&#39;)&#39;),	 	 𝐓(𝐙) = 𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\}),\]</p><p>where <code>S</code> is a function that returns a vector of expert summary statistics.</p><p>The constructor <code>DeepSetExpert(deepset::DeepSet, ϕ, S)</code> inherits <code>ψ</code> and <code>a</code> from <code>deepset</code>.</p><p>Similarly to <code>DeepSet</code>, set-level information can be incorporated by passing a <code>Tuple</code>, in which case we have</p><p class="math-container">\[θ̂(𝐙) = ϕ((𝐓(𝐙)&#39;, 𝐒(𝐙)&#39;, 𝐱&#39;)&#39;),	 	 𝐓(𝐙) = 𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\}).\]</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
S = samplesize
qₛ = 1
qₜ = 32
w = 16
ψ = Chain(Dense(n, w, relu), Dense(w, qₜ, relu));
ϕ = Chain(Dense(qₜ + qₛ, w), Dense(w, p));
θ̂ = DeepSetExpert(ψ, ϕ, S)

# Apply the estimator
Z₁ = rand(n, 3);                  # single set
Z₂ = [rand(n, m) for m ∈ (3, 4)]; # two sets
θ̂(Z₁)
θ̂(Z₂)

# Repeat the above but with set-level information:
qₓ = 2
ϕ  = Chain(Dense(qₜ + qₛ + qₓ, w, relu), Dense(w, p));
θ̂  = DeepSetExpert(ψ, ϕ, S)
x₁ = rand(qₓ)
x₂ = [rand(qₓ) for _ ∈ eachindex(Z₂)]
θ̂((Z₁, x₁))
θ̂((Z₂, x₂))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Architectures.jl#L217-L272">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.GraphPropagatePool" href="#NeuralEstimators.GraphPropagatePool"><code>NeuralEstimators.GraphPropagatePool</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GraphPropagatePool(propagation, globalpool)</code></pre><p>A graph neural network (GNN) module designed to act as the inner network <code>ψ</code> in the <code>DeepSet</code>/<code>DeepSetExpert</code> architecture.</p><p>The <code>propagation</code> module transforms graphical input data into a set of hidden feature graphs; the <code>globalpool</code> module aggregates the feature graphs (graph-wise) into a single hidden-feature vector. Critically, this hidden-feature vector is of fixed length irrespective of the size and shape of the graph.</p><p>The data should be a <code>GNNGraph</code> or <code>AbstractVector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Create some graphs
d = 1             # dimension of the response variable
n₁, n₂ = 11, 27   # number of nodes
e₁, e₂ = 30, 50   # number of edges
g₁ = rand_graph(n₁, e₁, ndata = rand(d, n₁))
g₂ = rand_graph(n₂, e₂, ndata = rand(d, n₂))
g  = batch([g₁, g₂])

# propagation module and global pooling module
w = 5
o = 7
propagation = GNNChain(GraphConv(d =&gt; w), GraphConv(w =&gt; w), GraphConv(w =&gt; o))
meanpool = GlobalPool(mean)

# DeepSet-based estimator with GNN for the inner network ψ
w = 32
p = 3
ψ = GraphPropagatePool(propagation, meanpool)
ϕ = Chain(Dense(o, w, relu), Dense(w, p))
θ̂ = DeepSet(ψ, ϕ)

# Apply the estimator
θ̂(g₁)           # single graph with a single replicate
θ̂(g)            # single graph with sub-graphs (i.e., with replicates)
θ̂([g₁, g₂, g])  # vector of graphs (each element is a different data set)

# Repeat the above but with set-level information:
qₓ = 2
ϕ = Chain(Dense(o + qₓ, w, relu), Dense(w, p))
θ̂ = DeepSet(ψ, ϕ)
x₁ = rand(qₓ)
x₂ = [rand(qₓ) for _ ∈ eachindex([g₁, g₂, g])]
θ̂((g₁, x₁))
θ̂((g, x₁))
θ̂(([g₁, g₂, g], x₂))

# Repeat the above but with set-level information and expert statistics:
S = samplesize
qₛ = 1
ϕ = Chain(Dense(o + qₓ + qₛ, w, relu), Dense(w, p))
θ̂ = DeepSetExpert(ψ, ϕ, S)
θ̂((g₁, x₁))
θ̂((g, x₁))
θ̂(([g₁, g₂, g], x₂))</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/Architectures.jl#L386-L455">source</a></section></article><h2 id="Loss-functions"><a class="docs-heading-anchor" href="#Loss-functions">Loss functions</a><a id="Loss-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Loss-functions" title="Permalink"></a></h2><p>In addition to the standard loss functions provided by <code>Flux</code> (e.g., <code>mae</code>, <code>mse</code>, etc.), <code>NeuralEstimators</code> provides the following loss functions.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.kpowerloss" href="#NeuralEstimators.kpowerloss"><code>NeuralEstimators.kpowerloss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">kpowerloss(θ̂, y, k; agg = mean, safeorigin = true, ϵ = 0.1)</code></pre><p>For <code>k</code> ∈ (0, ∞), the <code>k</code>-th power absolute-distance loss,</p><p class="math-container">\[L(θ̂, θ) = |θ̂ - θ|ᵏ,\]</p><p>contains the squared-error, absolute-error, and 0-1 loss functions as special cases (the latter obtained in the limit as <code>k</code> → 0).</p><p>It is Lipschitz continuous iff <code>k</code> = 1, convex iff <code>k</code> ≥ 1, and strictly convex iff <code>k</code> &gt; 1. It is quasiconvex for all <code>k</code> &gt; 0.</p><p>If <code>safeorigin = true</code>, the loss function is modified to avoid pathologies around the origin, so that the resulting loss function behaves similarly to the absolute-error loss in the <code>ϵ</code>-interval surrounding the origin.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/loss.jl#L16-L34">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.quantileloss" href="#NeuralEstimators.quantileloss"><code>NeuralEstimators.quantileloss</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">quantileloss(θ̂, θ, q; agg = mean)
quantileloss(θ̂, θ, q::V; agg = mean) where {T, V &lt;: AbstractVector{T}}</code></pre><p>The asymmetric loss function whose minimiser is the <code>q</code>th posterior quantile; namely,</p><p class="math-container">\[L(θ̂, θ, q) = (θ̂ - θ)(𝕀(θ̂ - θ &gt; 0) - q),\]</p><p>where <code>q</code> ∈ (0, 1) and 𝕀(⋅) is the indicator function.</p><p>The method that takes <code>q</code> as a vector is useful for jointly approximating several quantiles of the posterior distribution. In this case, the number of rows in <code>θ̂</code> is assumed to be pr, where p is the number of parameters: then, <code>q</code> should be an r-vector.</p><p>For further discussion on this loss function, see Equation (7) of Cressie, N. (2022), &quot;Decisions, decisions, decisions in an uncertain environment&quot;, arXiv:2209.13157.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">p = 1
K = 10
θ = rand(p, K)
θ̂ = rand(p, K)
quantileloss(θ̂, θ, 0.1)

θ̂ = rand(3p, K)
quantileloss(θ̂, θ, [0.1, 0.5, 0.9])

p = 2
θ = rand(p, K)
θ̂ = rand(p, K)
quantileloss(θ̂, θ, 0.1)

θ̂ = rand(3p, K)
quantileloss(θ̂, θ, [0.1, 0.5, 0.9])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/loss.jl#L58-L96">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.intervalscore" href="#NeuralEstimators.intervalscore"><code>NeuralEstimators.intervalscore</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">intervalscore(l, u, θ, α; agg = mean)</code></pre><p>Given a 100×(1-<code>α</code>)% confidence interval [<code>l</code>, <code>u</code>] with true value <code>θ</code>, the interval score is defined by</p><p class="math-container">\[S(l, u, θ; α) = (u - l) + 2α⁻¹(l - θ)𝕀(θ &lt; l) + 2α⁻¹(θ - u)𝕀(θ &gt; u),\]</p><p>where <code>α</code> ∈ (0, 1) and 𝕀(⋅) is the indicator function.</p><p>For further discussion, see Section 6 of Gneiting, T. and Raftery, A. E. (2007), &quot;Strictly proper scoring rules, prediction, and estimation&quot;, Journal of the American statistical Association, 102, 359–378.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/loss.jl#L141-L154">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>The function <code>train</code> is used to train a single neural estimator, while the wrapper function <code>trainx</code> is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, sampler, simulator; )
train(θ̂, θ_train::P, θ_val::P, simulator; ) where {P &lt;: Union{AbstractMatrix, ParameterConfigurations}}
train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; ) where {T, P &lt;: Union{AbstractMatrix, ParameterConfigurations}}</code></pre><p>Train a neural estimator with architecture <code>θ̂</code>.</p><p>The methods cater for different forms of on-the-fly simulation. The method that takes functions <code>sampler</code> and <code>simulator</code>for sampling parameters and simulating data, respectively, allows for both the parameters and the data to be simulated on-the-fly. Note that <code>simulator</code> is called as <code>simulator(θ, m)</code>, where <code>θ</code> is a set of parameters and <code>m</code> is the sample size (see keyword arguments below). If provided with specific instances of parameters (<code>θ_train</code> and <code>θ_val</code>) or data (<code>Z_train</code> and <code>Z_val</code>), they will be held fixed during training.</p><p>In all methods, the validation set is held fixed to reduce noise when evaluating the validation risk function, which is used to monitor the performance of the estimator during training.</p><p>If the number of replicates in <code>Z_train</code> is a multiple of the number of replicates for each element of <code>Z_val</code>, the training data will be recycled throughout training. For example, if each element of <code>Z_train</code> consists of 50 replicates, and each element of <code>Z_val</code> consists of 10 replicates, the first epoch uses the first 10 replicates in <code>Z_train</code>, the second epoch uses the next 10 replicates, and so on, until the sixth epoch again uses the first 10 replicates. Note that this requires the data to be subsettable with the function <code>subsetdata</code>.</p><p><strong>Keyword arguments</strong></p><p>Arguments common to all methods:</p><ul><li><code>loss = mae</code>: the loss function, which should return the average loss when applied to multiple replicates.</li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM(1e-4)</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the trained estimator and other information; if savepath is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 5</code>: cease training if the risk doesn&#39;t improve in this number of epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p>Arguments common to <code>train(θ̂, P, simulator)</code> and <code>train(θ̂, θ_train, θ_val, simulator)</code>:</p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>).</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: flag indicating whether we should simulate just-in-time, in the sense that only a <code>batchsize</code> number of parameter vectors and corresponding data are in memory at a given time.</li></ul><p>Arguments unique to <code>train(θ̂, P, simulator)</code>:</p><ul><li><code>K::Integer = 10000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K ÷ 5</code>.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices); if <code>ξ</code> is provided, the parameter sampler is called as <code>sampler(K, ξ)</code>.</li><li><code>epochs_per_θ_refresh::Integer = 1</code>: how often to refresh the training parameters. Must be a multiple of <code>epochs_per_Z_refresh</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Distributions
import NeuralEstimators: simulate

# data simulator
m = 15
function simulate(θ_set, m)
	[Float32.(rand(Normal(θ[1], θ[2]), 1, m)) for θ ∈ eachcol(θ_set)]
end

# parameter sampler
K = 10000
Ω = (μ = Normal(0, 1), σ = Uniform(0.1, 1)) # prior
struct sampler
	μ
	σ
end
function (s::sampler)(K)
	μ = rand(s.μ, K)
	σ = rand(s.σ, K)
	θ = hcat(μ, σ)&#39;
	return θ
end
smplr = sampler(Ω.μ, Ω.σ)

# architecture
p = length(Ω)   # number of parameters in the statistical model
w = 32          # width of each layer
ψ = Chain(Dense(1, w, relu), Dense(w, w, relu))
ϕ = Chain(Dense(w, w, relu), Dense(w, p))
θ̂ = DeepSet(ψ, ϕ)

# training: full simulation on-the-fly
θ̂ = train(θ̂, smplr, simulate, m = m, K = K, epochs = 5)
θ̂ = train(θ̂, smplr,  simulate, m = m, K = K, epochs = 5)
θ̂ = train(θ̂, smplr,  simulate, m = m, K = K, epochs = 10, epochs_per_θ_refresh = 4, epochs_per_Z_refresh = 2)

# training: fixed parameters
θ_train = smplr(K)
θ_val   = smplr(K ÷ 5)
θ̂ = train(θ̂, θ_train, θ_val, simulate, m = m, epochs = 5)
θ̂ = train(θ̂, θ_train, θ_val, simulate, m = m, epochs = 5, epochs_per_Z_refresh = 2)

# training: fixed parameters and fixed data
Z_train = simulate(θ_train, m)
Z_val   = simulate(θ_val, m)
θ̂ = train(θ̂, θ_train, θ_val, Z_train, Z_val, epochs = 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/train.jl#L1-L103">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.trainx" href="#NeuralEstimators.trainx"><code>NeuralEstimators.trainx</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainx(θ̂, P, simulator, M; )
trainx(θ̂, θ_train, θ_val, simulator, M; )
trainx(θ̂, θ_train, θ_val, Z_train::T, Z_val::T, M; )
trainx(θ̂, θ_train, θ_val, Z_train::V, Z_val::V; ) where {V &lt;: AbstractVector{AbstractVector{Any}}}</code></pre><p>A wrapper around <code>train</code> to construct neural estimators for different sample sizes.</p><p>The collection <code>M</code> specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if <code>M = [m₁, m₂]</code>, the estimator for sample size <code>m₂</code> is pre-trained with the estimator for sample size <code>m₁</code>. The method for <code>Z_train::T</code> and <code>Z_val::T</code> subsets the data using <code>subsetdata(Z, 1:mᵢ)</code> for each <code>mᵢ ∈ M</code>.</p><p>The method for <code>Z_train::V</code> and <code>Z_val::V</code> trains an estimator for each element of <code>Z_train</code> and <code>Z_val</code>; hence, it does not need to invoke <code>subsetdata</code>, which can be slow or difficult to define in some cases (e.g., for graphical data).</p><p>The keyword arguments inherit from <code>train</code>, and certain keyword arguments can be given as vectors. For example, if we are training two estimators, we can use a different number of epochs by providing <code>epochs = [e₁, e₂]</code>. Other arguments that allow vectors are <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/train.jl#L460-L482">source</a></section></article><h2 id="Assessing-a-neural-estimator"><a class="docs-heading-anchor" href="#Assessing-a-neural-estimator">Assessing a neural estimator</a><a id="Assessing-a-neural-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-a-neural-estimator" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">assess(estimators, θ, Z; &lt;keyword args&gt;)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data <code>Z</code> simulated based on true parameter vectors stored in <code>θ</code>.</p><p>If <code>Z</code> contains more data sets than parameter vectors, the parameter matrix will be recycled by horizontal concatenation.</p><p>The output is of type <code>Assessment</code>; see <code>?Assessment</code> for details.</p><p><strong>Keyword arguments</strong></p><ul><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible defaults provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible defaults provided). If <code>ξ</code> is provided with a field <code>parameter_names</code>, those names will be used.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices).</li><li><code>use_ξ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses <code>ξ</code>: if it does, the estimator will be applied as <code>estimator(Z, ξ)</code>. This argument is useful when multiple <code>estimators</code> are provided, only some of which need <code>ξ</code>; hence, if only one estimator is provided and <code>ξ</code> is not <code>nothing</code>, <code>use_ξ</code> is automatically set to <code>true</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Generate testing parameters
K = 100
θ = rand(p, K)

# Data for a single sample size
m = 30
Z = [rand(n, m) for _ ∈ 1:K];
assessment = assess([θ̂], θ, Z);
risk(assessment)

# Multiple data sets for each parameter vector
J = 5
Z = repeat(Z, J);
assessment = assess([θ̂], θ, Z);
risk(assessment)

# With set-level information
qₓ = 2
ϕ  = Chain(Dense(w + qₓ, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)
x = [rand(qₓ) for _ ∈ eachindex(Z)]
assessment = assess([θ̂], θ, (Z, x));
risk(assessment)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/assess.jl#L75-L132">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Assessment(df::DataFrame, runtime::DataFrame)</code></pre><p>A type for storing the output of <code>assess()</code>. The field <code>runtime</code> contains the total time taken for each estimator. The field <code>df</code> is a long-form <code>DataFrame</code> with columns:</p><ul><li><code>estimator</code>: the name of the estimator</li><li><code>parameter</code>: the name of the parameter</li><li><code>truth</code>:     the true value of the parameter</li><li><code>estimate</code>:  the estimated value of the parameter</li><li><code>m</code>:         the sample size (number of iid replicates)</li><li><code>k</code>:         the index of the parameter vector in the test set</li><li><code>j</code>:         the index of the data set</li></ul><p>Multiple <code>Assessment</code> objects can be combined with <code>merge()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/assess.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.risk" href="#NeuralEstimators.risk"><code>NeuralEstimators.risk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk(assessment::Assessment; loss = (x, y) -&gt; abs(x - y), average_over_parameters = true)</code></pre><p>Computes a Monte Carlo approximation of the Bayes risk,</p><p class="math-container">\[r_{\Omega}(\hat{\boldsymbol{\theta}}(\cdot))
\approx
\frac{1}{K} \sum_{\boldsymbol{\theta} \in \vartheta} \frac{1}{J} \sum_{\boldsymbol{Z} \in \mathcal{Z}_{\boldsymbol{\theta}}} L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{Z})).\]</p><p>where <span>$\vartheta$</span> denotes a set of <span>$K$</span> parameter vectors sampled from the prior <span>$\Omega(\cdot)$</span> and, for each <span>$\boldsymbol{\theta} \in \vartheta$</span>, we have <span>$J$</span> sets of <span>$m$</span> mutually independent realisations from the model collected in <span>$\mathcal{Z}_{\boldsymbol{\theta}}$</span>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>loss = (x, y) -&gt; abs(x - y)</code>: a binary operator (default absolute-error loss).</li><li><code>average_over_parameters::Bool = true</code>: if true (default), the loss is averaged over all parameters; otherwise, the loss is averaged over each parameter separately.</li><li><code>average_over_sample_sizes::Bool = true</code>: if true (default), the loss is averaged over all sample sizes <span>$m$</span>; otherwise, the loss is averaged over each sample size separately.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/assess.jl#L35-L55">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.coverage" href="#NeuralEstimators.coverage"><code>NeuralEstimators.coverage</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">coverage(ci::V, θ) where  {V &lt;: AbstractArray{M}} where M &lt;: AbstractMatrix</code></pre><p>Given a p×K matrix of true parameters <code>θ</code>, determine the empirical coverage of the confidence intervals <code>ci</code> (a K-vector of px2 matrices).</p><p>The overall empirical coverage is obtained by averaging the resulting 0-1 matrix over all data sets.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/bootstrap.jl#L207-L215">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotrisk" href="#NeuralEstimators.plotrisk"><code>NeuralEstimators.plotrisk</code></a> — <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/plotting.jl#L9-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotdistribution" href="#NeuralEstimators.plotdistribution"><code>NeuralEstimators.plotdistribution</code></a> — <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/plotting.jl#L3-L5">source</a></section></article><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.bootstrap" href="#NeuralEstimators.bootstrap"><code>NeuralEstimators.bootstrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bootstrap(θ̂, parameters::P, Z̃) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, parameters::P, m::Integer; B = 400) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, Z; B = 400, blocks = nothing)</code></pre><p>Generates <code>B</code> bootstrap estimates from an estimator <code>θ̂</code>.</p><p>Parametric bootstrapping is facilitated by passing a single parameter configuration, <code>parameters</code>, and corresponding simulated data, <code>Z̃</code>, whose length implicitly defines <code>B</code>. Alternatively, if the user has defined a method <code>simulate(parameters, m)</code>, one may simply pass the desired sample size <code>m</code> for the simulated data sets.</p><p>Non-parametric bootstrapping is facilitated by passing a single data set, <code>Z</code>. The argument <code>blocks</code> caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>[1, 1, 2, 2, 2]</code>. The resampling algorithm aims to produce resampled data sets that are of a similar size to <code>Z</code>, but this can only be achieved exactly if all blocks are equal in length.</p><p>The keyword argument <code>use_gpu</code> is a flag determining whether to use the GPU, if it is available (default <code>true</code>).</p><p>The return type is a p × <code>B</code> matrix, where p is the number of parameters in the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/bootstrap.jl#L98-L123">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.interval" href="#NeuralEstimators.interval"><code>NeuralEstimators.interval</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">interval(θ̃, θ̂ = nothing; type, probs = [0.05, 0.95], parameter_names)</code></pre><p>Compute a confidence interval using the quantiles of the p × B matrix of bootstrap samples, <code>θ̃</code>, where p is the number of parameters in the model.</p><p>The quantile levels are controlled with the argument <code>probs</code>. The rows can be named with a vector of strings <code>parameter_names</code> (sensible defaults provided).</p><p>The return type is a p × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the confidence interval.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
p = 3
B = 50
θ̃ = rand(p, B)
θ̂ = rand(p)
interval(θ̃)
interval(θ̃, θ̂, type = &quot;basic&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/270881445f98ce948364cfe9efd3183be496627f/src/bootstrap.jl#L1-L23">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">« Advanced usage</a><a class="docs-footer-nextpage" href="../simulation/">Model-specific functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Thursday 27 April 2023 23:47">Thursday 27 April 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
