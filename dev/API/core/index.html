<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core functions · NeuralEstimators.jl</title><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.045/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.4/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.13.24/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core functions</a><ul class="internal"><li><a class="tocitem" href="#Sampling-parameters"><span>Sampling parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Neural-estimator-representations"><span>Neural-estimator representations</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessing-a-neural-estimator"><span>Assessing a neural estimator</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Utility functions</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/core.md" title="Edit on GitHub"><span class="docs-icon fab"></span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Core-functions"><a class="docs-heading-anchor" href="#Core-functions">Core functions</a><a id="Core-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Core-functions" title="Permalink"></a></h1><p>This page documents the functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.</p><h2 id="Sampling-parameters"><a class="docs-heading-anchor" href="#Sampling-parameters">Sampling parameters</a><a id="Sampling-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> are stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution.</p><p>It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the matrix of parameters. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with <a href="#NeuralEstimators.simulate"><code>simulate</code></a>.</p><p>The user-defined type must have a field <code>θ</code> that stores the <span>$p$</span> × <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p>See <a href="../utility/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a> for the generic function for subsetting these objects.  </p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">struct P &lt;: ParameterConfigurations
	θ
	# ...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/Parameters.jl#L1-L22">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model (by overloading the generic function <code>simulate</code>).</p><p>The data should be stored as a <code>Vector{A}</code>, where each element of the vector is associated with one parameter configuration, and where <code>A</code> depends on the representation of the neural estimator. For example, if the neural estimator is a <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> object, the data should be stored as a <code>Vector{Array}</code>, where each array may store independent replicates in its final dimension. Similarly, if the neural estimator is a <a href="#NeuralEstimators.GNNEstimator"><code>GNNEstimator</code></a>, the data should be stored as a <code>Vector{GNNGraph}</code>, where each graph may store independent replicates in sub-graphs.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate" href="#NeuralEstimators.simulate"><code>NeuralEstimators.simulate</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Generic function that may be overloaded to implicitly define a statistical model. Specifically, the user should provide a method <code>simulate(parameters, m)</code> that returns <code>m</code> simulated replicates for each element in the given set of <code>parameters</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/simulate.jl#L1-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate-Tuple{Any, Any, Integer}" href="#NeuralEstimators.simulate-Tuple{Any, Any, Integer}"><code>NeuralEstimators.simulate</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">simulate(parameters, m, J::Integer)</code></pre><p>Simulates <code>J</code> sets of <code>m</code> independent replicates for each parameter vector in <code>parameters</code> by calling <code>simulate(parameters, m)</code> a total of <code>J</code> times.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/simulate.jl#L9-L14">source</a></section></article><h2 id="Neural-estimator-representations"><a class="docs-heading-anchor" href="#Neural-estimator-representations">Neural-estimator representations</a><a id="Neural-estimator-representations-1"></a><a class="docs-heading-anchor-permalink" href="#Neural-estimator-representations" title="Permalink"></a></h2><p>Although the user is free to construct their neural estimator however they see fit, <code>NeuralEstimators</code> provides several useful representations described below.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">DeepSet(ψ, ϕ, a)
DeepSet(ψ, ϕ; a::String = &quot;mean&quot;)</code></pre><p>A neural estimator in the <code>DeepSet</code> representation,</p><p class="math-container">\[θ̂(𝐙) ≡ ϕ(𝐚(\{ψ(𝐙ᵢ) : i = 1, …, m\})),\]</p><p>where 𝐙 ≡ (𝐙₁&#39;, …, 𝐙ₘ&#39;)&#39; are independent replicates from the model, <code>ψ</code> and <code>ϕ</code> are neural networks, and <code>𝐚</code> is a permutation-invariant aggregation function.</p><p>The function <code>𝐚</code> must aggregate over the last dimension  of an array (i.e., the replicates dimension). It can be specified as a positional argument of type <code>Function</code>, or as a keyword argument of type <code>String</code> with permissible values <code>&quot;mean&quot;</code>, <code>&quot;sum&quot;</code>, and <code>&quot;logsumexp&quot;</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Apply the estimator to a single set of 3 realisations:
Z = rand(n, 3);
θ̂(Z)

# Apply the estimator to two sets each containing 3 realisations:
Z = [rand(n, m) for m ∈ (3, 3)];
θ̂(Z)

# Apply the estimator to two sets containing 3 and 4 realisations, respectively:
Z = [rand(n, m) for m ∈ (3, 4)];
θ̂(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/DeepSet.jl#L25-L69">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PiecewiseEstimator(estimators, m_breaks)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of sample-size changepoints, <code>m_breaks</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Suppose that we&#39;ve trained two neural estimators. The first, θ̂₁, is trained
# for small sample sizes (e.g., m ≤ 30), and the second, `θ̂₂`, is trained for
# moderate-to-large sample sizes (e.g., m &gt; 30). Then we construct a piecewise
# estimator with a sample-size changepoint of 30, which dispatches θ̂₁ if m ≤ 30
# and θ̂₂ if m &gt; 30.

n = 2  # bivariate data
p = 3  # number of parameters in the model
w = 8  # width of each layer

ψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₁ = DeepSet(ψ₁, ϕ₁)

ψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu));
ϕ₂ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, p));
θ̂₂ = DeepSet(ψ₂, ϕ₂)

θ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], [30])
Z = [rand(n, 1, m) for m ∈ (10, 50)]
θ̂(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/PiecewiseEstimator.jl#L1-L31">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.GNNEstimator" href="#NeuralEstimators.GNNEstimator"><code>NeuralEstimators.GNNEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">GNNEstimator(propagation, globalpool, deepset)</code></pre><p>A neural estimator based on a graph neural network (GNN). The <code>propagation</code> module transforms graphical input data into a set of hidden feature graphs; the <code>globalpool</code> module aggregates the feature graphs (graph-wise) into a single hidden feature vector; and the <code>deepset</code> module maps the hidden feature vectors onto the parameter space.</p><p>The data should be a <code>GNNGraph</code> or <code>AbstractVector{GNNGraph}</code>, where each graph is associated with a single parameter vector. The graphs may contain sub-graphs corresponding to independent replicates from the model.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Flux: batch
using GraphNeuralNetworks
using Statistics: mean

# Create some graphs
d = 1             # dimension of the response variable
n₁, n₂ = 11, 27   # number of nodes
e₁, e₂ = 30, 50   # number of edges
g₁ = rand_graph(n₁, e₁, ndata=rand(d, n₁))
g₂ = rand_graph(n₂, e₂, ndata=rand(d, n₂))
g  = batch([g₁, g₂])

# propagation module
w = 5; o = 7
propagation = GNNChain(GraphConv(d =&gt; w), GraphConv(w =&gt; w), GraphConv(w =&gt; o))

# global pooling module
meanpool = GlobalPool(mean)

# Deep Set module
w = 32
p = 3
ψ₂ = Chain(Dense(o, w, relu), Dense(w, w, relu), Dense(w, w, relu))
ϕ₂ = Chain(Dense(w, w, relu), Dense(w, p))
deepset = DeepSet(ψ₂, ϕ₂)

# GNN estimator
est = GNNEstimator(propagation, meanpool, deepset)

# Apply the estimator to a single graph, a single graph containing sub-graphs,
# and a vector of graphs:
θ̂ = est(g₁)
θ̂ = est(g)
θ̂ = est([g₁, g₂, g])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/GNNEstimator.jl#L1-L53">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Function</span></header><section><div><p>Generic function for training a neural estimator.</p><p>The methods are designed to cater for different forms of &quot;on-the-fly simulation&quot; (see the online documentation). In all methods, the validation data are held fixed to reduce noise when evaluating the validation risk function (which is used to monitor the performance of the estimator during training).</p><p><strong>Keyword arguments</strong></p><p>Arguments common to all methods:</p><ul><li><code>loss = mae</code>: the loss function, which should return the average loss when applied to multiple replicates.</li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM(1e-4)</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the trained <code>θ̂</code> and other information; if savepath is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 5</code>: cease training if the risk doesn&#39;t improve in <code>stopping_epochs</code> epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p>Arguments common to <code>train(θ̂, P)</code> and <code>train(θ̂, θ_train, θ_val)</code>:</p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>).</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: should we simulate the data &quot;just-in-time&quot;? If <code>true</code>, the user must overload the generic function <code>simulate</code> with a method <code>simulate(parameters, m)</code>.</li></ul><p>Arguments unique to <code>train(θ̂, P)</code>:</p><ul><li><code>K::Integer = 10000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K ÷ 5</code>.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices); if <code>ξ</code> is provided, the constructor <code>P</code> is called as <code>P(K, ξ)</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/train.jl#L1-L29">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Tuple{Any, Any}" href="#NeuralEstimators.train-Tuple{Any, Any}"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, P; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>θ̂</code> by providing a constructor, <code>P</code>, where <code>P &lt;: Union{AbstractMatrix, ParameterConfigurations}</code>, to automatically sample training and validation parameter sets at each epoch.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/train.jl#L33-L39">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P&lt;:Union{ParameterConfigurations, AbstractMatrix}" href="#NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P&lt;:Union{ParameterConfigurations, AbstractMatrix}"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, θ_train::P, θ_val::P; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>θ̂</code> by providing the training and validation parameter sets explicitly as <code>θ_train</code> and <code>θ_val</code>, both of which are held fixed during training.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/train.jl#L139-L144">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}" href="#NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>θ̂</code> by providing the training and validation parameter sets, <code>θ_train</code> and <code>θ_val</code>, and the training and validation data sets, <code>Z_train</code> and <code>Z_val</code>, all of which are held fixed during training.</p><p>If the elements of <code>Z_train</code> and <code>Z_val</code> are equally replicated, and the number of replicates for each element of <code>Z_train</code> is a multiple of the number of replicates for each element of <code>Z_val</code>, then the training data will then be recycled throughout training to imitate on-the-fly simulation. For example, if each element of <code>Z_train</code> consists of 50 replicates, and each element of <code>Z_val</code> consists of 10 replicates, the first epoch uses the first 10 replicates in <code>Z_train</code>, the second epoch uses the next 10 replicates, and so on, until epoch 6 again uses the first 10 replicates. Note that this requires the data to be subsetted throughout training with the function <code>subsetdata</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/train.jl#L264-L280">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}" href="#NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T, M; &lt;keyword args&gt;)
train(θ̂, θ_train::P, θ_val::P, Z_train::V, Z_val::V; &lt;keyword args&gt;) where {V &lt;: Vector{Vector{T}}}</code></pre><p>Train several neural estimators with the architecture <code>θ̂</code> under different sample sizes.</p><p>The first method accepts sample sizes as a vector of integers, <code>M</code>. Each estimator is pre-trained with the estimator trained for the previous sample size. For example, if <code>M = [m₁, m₂]</code>, with <code>m₂</code> &gt; <code>m₁</code>, the estimator for sample size <code>m₂</code> is pre-trained with the estimator for sample size <code>m₁</code>.</p><p>The second method requires <code>Z_train</code> and <code>Z_val</code> to be a <code>Vector{Vector{T}}</code>, where <code>T</code> is arbitrary. In this method, a separate estimator is trained for each element in <code>Z_train</code> and <code>Z_val</code> and, importantly, it does not invoke <code>subsetdata()</code>, which can be slow for graphical data. Again, pre-training is used.</p><p>These methods wrap <code>train(θ̂, θ_train, θ_val, Z_train, Z_val)</code> and, hence, they inherit its keyword arguments. Further, certain keyword arguments can be given as vectors. For instance, if we are training two neural estimators, we can use a different number of epochs by providing <code>epochs = [e₁, e₂]</code>. Other arguments that allow vectors are <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/train.jl#L399-L421">source</a></section></article><h2 id="Assessing-a-neural-estimator"><a class="docs-heading-anchor" href="#Assessing-a-neural-estimator">Assessing a neural estimator</a><a id="Assessing-a-neural-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-a-neural-estimator" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">assess(estimators, parameters, Z; &lt;keyword args&gt;)
assess(estimators, parameters; &lt;keyword args&gt;)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data simulated from a set of <code>parameters</code>.</p><p>Testing data can be automatically simulated by overloading <code>simulate</code> with a method <code>simulate(parameters, m::Integer)</code>, and using the keyword argument <code>m</code> to specify the desired sample sizes to assess. Alternatively, one may provide testing data <code>Z</code> as an iterable collection, where each element contains the testing data for a given sample size. If there are more simulated data sets than unique parameter vectors, the data should be stored in an &#39;outer&#39; fashion, so that the parameter vectors run faster than the replicated data.</p><p><strong>Keyword arguments</strong></p><p><strong>Arguments common to both methods</strong></p><ul><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible defaults provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible defaults provided).</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices).</li><li><code>use_ξ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses <code>ξ</code>: if it does, the estimator will be applied as <code>estimator(Z, ξ)</code>. This argument is useful when multiple <code>estimators</code> are provided, only some of which need <code>ξ</code>; hence, if only one estimator is provided and <code>ξ</code> is not <code>nothing</code>, <code>use_ξ</code> is automatically set to <code>true</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul><p><strong>Arguments unique to <code>assess(estimators, parameters)</code></strong></p><ul><li><code>m::Vector{Integer}</code>: sample sizes to estimate from.</li><li><code>J::Integer = 1</code>: the number of times to replicate each parameter in <code>parameters</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Generate fake parameters and corresponding data for a range of sample sizes:
K = 100        # number of parameter vectors in the test set
θ = rand(p, K)
Z = [[rand(n, m) for _ ∈ 1:K] for m ∈ (1, 10, 20)]

assessment = assess([θ̂], θ, Z)
risk(assessment)
risk(assessment, average_over_parameters = false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/assess.jl#L99-L151">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Assessment(θandθ̂::DataFrame, runtime::DataFrame)</code></pre><p>An type for storing the output of <code>assess()</code>. It contains two fields. The field <code>runtime</code> contains the total <code>time</code> taken for each <code>estimator</code> for each sample size <code>m</code>. The field <code>θandθ̂</code> is a long-form <code>DataFrame</code> containing the true parameters and corresponding estimates. Specifically, its columns are:</p><ul><li><code>estimator</code>: the name of the estimator</li><li><code>parameter</code>: the name of the parameter</li><li><code>truth</code>:     the true value of the parameter</li><li><code>estimate</code>:  the estimated value of the parameter</li><li><code>m</code>:         the sample size</li><li><code>k</code>:         the index of the parameter vector in the test set</li><li><code>j</code>: the index of the data set</li></ul><p>Multiple <code>Assessment</code> objects can be combined with the function <code>merge</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/assess.jl#L3-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.risk" href="#NeuralEstimators.risk"><code>NeuralEstimators.risk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk(assessment::Assessment; loss = (x, y) -&gt; abs(x - y), average_over_parameters = true)</code></pre><p>Estimates the Bayes risk with respect to the <code>loss</code> function for each estimator, parameter, and sample size considered in <code>assessment</code>.</p><p>The argument <code>loss</code> should be a binary operator (default absolute-error loss).</p><p>If <code>average_over_parameters = true</code> (default), the risk is averaged over all parameters; otherwise, the risk is evaluated over each parameter separately.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/assess.jl#L37-L47">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.coverage" href="#NeuralEstimators.coverage"><code>NeuralEstimators.coverage</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">coverage(θ̂, Z::V, θ, α; kwargs...) where  {V &lt;: AbstractArray{A}} where A</code></pre><p>For each data set contained in <code>Z</code>, compute a non-parametric bootstrap confidence interval with nominal coverage <code>α</code>, and determine if the true parameters, <code>θ</code>, are contained within this interval. The overall empirical coverage is then obtained by averaging the resulting 0-1 matrix over all data sets.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/assess.jl#L282-L289">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotrisk" href="#NeuralEstimators.plotrisk"><code>NeuralEstimators.plotrisk</code></a> — <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/plotting.jl#L9-L11">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.plotdistribution" href="#NeuralEstimators.plotdistribution"><code>NeuralEstimators.plotdistribution</code></a> — <span class="docstring-category">Function</span></header><section><div><p>TODO</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/plotting.jl#L3-L5">source</a></section></article><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.bootstrap" href="#NeuralEstimators.bootstrap"><code>NeuralEstimators.bootstrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bootstrap(θ̂, parameters::P, Z̃) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, parameters::P, m::Integer; B = 400) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, Z; B = 400)
bootstrap(θ̂, Z, blocks::Vector{Integer}; B = 400)</code></pre><p>Generates <code>B</code> bootstrap estimates from an estimator <code>θ̂</code>.</p><p>Parametric bootstrapping is facilitated by passing a single parameter configuration, <code>parameters</code>, and corresponding simulated data, <code>Z̃</code>, whose length implicitly defines <code>B</code>. Alternatively, if the user has defined a method <code>simulate(parameters, m)</code>, one may simply pass the desired sample size <code>m</code> for the simulated data sets.</p><p>Non-parametric bootstrapping is facilitated by passing a single data set, <code>Z</code>. The argument <code>blocks</code> caters for block bootstrapping, and it should be an integer vector specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>[1, 1, 2, 2, 2]</code>. The resampling algorithm aims to produce resampled data sets that are of a similar size to <code>Z</code>, but this can only be achieved exactly if all blocks are equal in length.</p><p>The keyword argument <code>use_gpu</code> is a flag determining whether to use the GPU, if it is available (default <code>true</code>).</p><p>The return type is a p × <code>B</code> matrix, where p is the number of parameters in the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/bootstrap.jl#L20-L45">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.confidenceinterval" href="#NeuralEstimators.confidenceinterval"><code>NeuralEstimators.confidenceinterval</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">confidenceinterval(θ̃; probs = [0.05, 0.95], parameter_names)</code></pre><p>Compute a confidence interval using the quantiles of the p × B matrix of bootstrap samples, <code>θ̃</code>, where p is the number of parameters in the model.</p><p>The quantile levels are controlled with the argument <code>probs</code>. The rows can be named with <code>parameter_names</code> (sensible defaults provided), which shold be a vector.</p><p>The return type is a p × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the confidence interval.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/e7ad7978fad473d88402bea78611dec29ac752f5/src/bootstrap.jl#L1-L12">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">« Advanced usage</a><a class="docs-footer-nextpage" href="../simulation/">Model-specific functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 0.27.24 on <span class="colophon-date" title="Wednesday 22 February 2023 03:32">Wednesday 22 February 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
