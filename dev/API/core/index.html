<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core · NeuralEstimators.jl</title><meta name="title" content="Core · NeuralEstimators.jl"/><meta property="og:title" content="Core · NeuralEstimators.jl"/><meta property="twitter:title" content="Core · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Theoretical framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core</a><ul class="internal"><li><a class="tocitem" href="#Sampling-parameters"><span>Sampling parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Types-of-estimators"><span>Types of estimators</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessing-a-neural-estimator"><span>Assessing a neural estimator</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li><li><a class="tocitem" href="../architectures/">Architectures and activations functions</a></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/core.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Core"><a class="docs-heading-anchor" href="#Core">Core</a><a id="Core-1"></a><a class="docs-heading-anchor-permalink" href="#Core" title="Permalink"></a></h1><p>This page documents the functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from sampling parameters from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.</p><h2 id="Sampling-parameters"><a class="docs-heading-anchor" href="#Sampling-parameters">Sampling parameters</a><a id="Sampling-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Sampling-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> are stored as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution.</p><p>It can sometimes be helpful to wrap the parameter matrix in a user-defined type that also stores expensive intermediate objects needed for data simulated (e.g., Cholesky factors). In this case, the user-defined type should be a subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>, whose only requirement is a field <code>θ</code> that stores the matrix of parameters. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation.</p><p>The user-defined type must have a field <code>θ</code> that stores the <span>$p$</span> × <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p>See <a href="../utility/#NeuralEstimators.subsetparameters"><code>subsetparameters</code></a> for the generic function for subsetting these objects.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">struct P &lt;: ParameterConfigurations
	θ
	# other expensive intermediate objects...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Parameters.jl#L1-L22">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define the model via simulated data. The user may provide simulated data directly, or provide a function that simulates data from the model.</p><p>The data should be stored as a <code>Vector{A}</code>, where each element of the vector is associated with one parameter configuration, and where <code>A</code> depends on the architecture of the neural estimator.</p><h2 id="Types-of-estimators"><a class="docs-heading-anchor" href="#Types-of-estimators">Types of estimators</a><a id="Types-of-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Types-of-estimators" title="Permalink"></a></h2><p>See also <a href="../architectures/#Architectures-and-activations-functions">Architectures and activations functions</a> that are often used when constructing neural estimators.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.NeuralEstimator" href="#NeuralEstimators.NeuralEstimator"><code>NeuralEstimators.NeuralEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">NeuralEstimator</code></pre><p>An abstract supertype for neural estimators.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L3-L7">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PointEstimator" href="#NeuralEstimators.PointEstimator"><code>NeuralEstimators.PointEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PointEstimator(arch)</code></pre><p>A simple point estimator, that is, a mapping from the sample space to the parameter space, defined by the given architecture <code>arch</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L12-L17">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.IntervalEstimator" href="#NeuralEstimators.IntervalEstimator"><code>NeuralEstimators.IntervalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IntervalEstimator(arch_lower, arch_upper)
IntervalEstimator(arch)</code></pre><p>A neural interval estimator that jointly estimates credible intervals constructed as,</p><p class="math-container">\[[l(Z), l(Z) + \mathrm{exp}(u(Z))],\]</p><p>where <span>$l(⋅)$</span> and <span>$u(⋅)$</span> are the neural networks <code>arch_lower</code> and <code>arch_upper</code>, both of which should transform data into <span>$p$</span>-dimensional vectors, where <span>$p$</span> is the number of parameters in the statistical model. If only a single neural network architecture <code>arch</code> is provided, it will be used for both <code>arch_lower</code> and <code>arch_upper</code>.</p><p>The returned value is a matrix with <span>$2p$</span> rows, where the first and second <span>$p$</span> rows correspond to estimates of the lower and upper bound, respectively.</p><p>See also <a href="#NeuralEstimators.IntervalEstimatorCompactPrior"><code>IntervalEstimatorCompactPrior</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

# Generate some toy data
n = 2   # bivariate data
m = 100 # number of independent replicates
Z = rand(n, m)

# Create an architecture
p = 3  # number of parameters in the statistical model
w = 8  # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
architecture = DeepSet(ψ, ϕ)

# Initialise the interval estimator
estimator = IntervalEstimator(architecture)

# Apply the interval estimator
estimator(Z)
interval(estimator, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L27-L71">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.IntervalEstimatorCompactPrior" href="#NeuralEstimators.IntervalEstimatorCompactPrior"><code>NeuralEstimators.IntervalEstimatorCompactPrior</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">IntervalEstimatorCompactPrior(u, v, min_supp::Vector, max_supp::Vector)
IntervalEstimatorCompactPrior(u, v, compress::Compress)</code></pre><p>Uses the neural networks <code>u</code> and <code>v</code> to jointly estimate credible intervals that are guaranteed to be within the support of the prior distributon. This support is defined by the <span>$p$</span>-dimensional vectors <code>min_supp</code> and <code>max_supp</code> (or a single <span>$p$</span>-dimensional object of type <code>Compress</code>), where <span>$p$</span> is the number of parameters in the statistical model.</p><p>Given data <span>$Z$</span>, the intervals are constructed as</p><p class="math-container">\[[g(u(Z)), 	g(u(Z)) + \mathrm{exp}(v(Z)))],\]</p><p>where</p><ul><li><span>$u(⋅)$</span> and <span>$v(⋅)$</span> are neural networks, both of which should transform data into <span>$p$</span>-dimensional vectors;</li><li><span>$g(⋅)$</span> is a logistic function that maps its input to the prior support.</li></ul><p>Note that, in addition to ensuring that the interval remains in the prior support, this constructions also ensures that the intervals are valid (i.e., it prevents quantile crossing, in the sense that the upper bound is always greater than the lower bound).</p><p>The returned value is a matrix with <span>$2p$</span> rows, where the first and second <span>$p$</span> rows correspond to estimates of the lower and upper bound, respectively.</p><p>See also <a href="#NeuralEstimators.IntervalEstimator"><code>IntervalEstimator</code></a> and <a href="../architectures/#NeuralEstimators.Compress"><code>Compress</code></a>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

# prior support
min_supp = [25, 0.5, -pi/2]
max_supp = [500, 2.5, 0]
p = length(min_supp)  # number of parameters in the statistical model

# Generate some toy data
n = 2   # bivariate data
m = 100 # number of independent replicates
Z = rand(n, m)

# Create an architecture
w = 8  # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
u = DeepSet(ψ, ϕ)
v = deepcopy(u) # use the same architecture for both u and v

# Initialise the interval estimator
estimator = IntervalEstimatorCompactPrior(u, v, min_supp, max_supp)

# Apply the interval estimator
estimator(Z)
interval(estimator, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L90-L149">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PointIntervalEstimator" href="#NeuralEstimators.PointIntervalEstimator"><code>NeuralEstimators.PointIntervalEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PointIntervalEstimator(arch_point, arch_lower, arch_upper)
PointIntervalEstimator(arch_point, arch_bound)
PointIntervalEstimator(arch)</code></pre><p>A neural estimator that jointly produces point estimates, θ̂(Z), where θ̂(Z) is a neural point estimator with architecture <code>arch_point</code>, and credible intervals constructed as,</p><p class="math-container">\[[θ̂(Z) - \mathrm{exp}(l(Z)), θ̂(Z) + \mathrm{exp}(u(Z))],\]</p><p>where <span>$l(⋅)$</span> and <span>$u(⋅)$</span> are the neural networks <code>arch_lower</code> and <code>arch_upper</code>, both of which should transform data into <span>$p$</span>-dimensional vectors, where <span>$p$</span> is the number of parameters in the statistical model.</p><p>If only a single neural network architecture <code>arch</code> is provided, it will be used for all architectures; similarly, if two architectures are provided, the second will be used for both <code>arch_lower</code> and <code>arch_upper</code>.</p><p>Internally, the point estimates, lower-bound estimates, and upper-bound estimates are concatenated, so that <code>PointIntervalEstimator</code> objects transform data into matrices with <span>$3p$</span> rows.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

# Generate some toy data
n = 2   # bivariate data
m = 100 # number of independent replicates
Z = rand(n, m)

# Create an architecture
p = 3  # number of parameters in the statistical model
w = 8  # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
architecture = DeepSet(ψ, ϕ)

# Initialise the estimator
estimator = PointIntervalEstimator(architecture)

# Apply the estimator
estimator(Z)
interval(estimator, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L187-L233">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.QuantileEstimator" href="#NeuralEstimators.QuantileEstimator"><code>NeuralEstimators.QuantileEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">QuantileEstimator()</code></pre><p>Coming soon: this structure will allow for the simultaneous estimation of an arbitrary number of marginal quantiles of the posterior distribution.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L261-L266">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">PiecewiseEstimator(estimators, breaks)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of changepoints, <code>breaks</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p>Any estimator can be included in <code>estimators</code>, including any of the subtypes of <code>NeuralEstimator</code> exported with the package <code>NeuralEstimators</code> (e.g., <code>PointEstimator</code>, <code>IntervalEstimator</code>, etc.).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Suppose that we&#39;ve trained two neural estimators. The first, θ̂₁, is trained
# for small sample sizes (e.g., m ≤ 30), and the second, `θ̂₂`, is trained for
# moderate-to-large sample sizes (e.g., m &gt; 30). We construct a piecewise
# estimator with a sample-size changepoint of 30, which dispatches θ̂₁ if m ≤ 30
# and θ̂₂ if m &gt; 30.

using NeuralEstimators
using Flux

n = 2  # bivariate data
p = 3  # number of parameters in the statistical model
w = 8  # width of each layer

ψ₁ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₁ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₁ = DeepSet(ψ₁, ϕ₁)

ψ₂ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ₂ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂₂ = DeepSet(ψ₂, ϕ₂)

θ̂ = PiecewiseEstimator([θ̂₁, θ̂₂], [30])
Z = [rand(n, 1, m) for m ∈ (10, 50)]
θ̂(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/Estimators.jl#L278-L315">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><p>The function <code>train</code> is used to train a single neural estimator, while the wrapper function <code>trainx</code> is useful for training multiple neural estimators over a range of sample sizes, making using of the technique known as pre-training.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">train(θ̂, sampler, simulator; )
train(θ̂, θ_train::P, θ_val::P, simulator; ) where {P &lt;: Union{AbstractMatrix, ParameterConfigurations}}
train(θ̂, θ_train::P, θ_val::P, Z_train::T, Z_val::T; ) where {T, P &lt;: Union{AbstractMatrix, ParameterConfigurations}}</code></pre><p>Train a neural estimator with architecture <code>θ̂</code>.</p><p>The methods cater for different forms of on-the-fly simulation. The method that takes functions <code>sampler</code> and <code>simulator</code> for sampling parameters and simulating data, respectively, allows for both the parameters and the data to be simulated on-the-fly. Note that <code>simulator</code> is called as <code>simulator(θ, m)</code>, where <code>θ</code> is a set of parameters and <code>m</code> is the sample size (see keyword arguments below). If provided with specific instances of parameters (<code>θ_train</code> and <code>θ_val</code>) or data (<code>Z_train</code> and <code>Z_val</code>), they will be held fixed during training.</p><p>In all methods, the validation set is held fixed to reduce noise when evaluating the validation risk function, which is used to monitor the performance of the estimator during training.</p><p>If the number of replicates in <code>Z_train</code> is a multiple of the number of replicates for each element of <code>Z_val</code>, the training data will be recycled throughout training. For example, if each element of <code>Z_train</code> consists of 50 replicates, and each element of <code>Z_val</code> consists of 10 replicates, the first epoch uses the first 10 replicates in <code>Z_train</code>, the second epoch uses the next 10 replicates, and so on, until the sixth epoch again uses the first 10 replicates. Note that this requires the data to be subsettable with the function <code>subsetdata</code>.</p><p><strong>Keyword arguments</strong></p><p>Arguments common to all methods:</p><ul><li><code>loss = mae</code>: the loss function, which should return the average loss when applied to multiple replicates.</li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM(1e-4)</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the neural-network weights during training (as <code>bson</code> files) and other information, such as the risk function per epoch (the risk function evaluated over the training and validation sets are saved in the first and second columns of <code>loss_per_epoch.csv</code>). If savepath is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 5</code>: cease training if the risk doesn&#39;t improve in this number of epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p>Arguments common to <code>train(θ̂, P, simulator)</code> and <code>train(θ̂, θ_train, θ_val, simulator)</code>:</p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>).</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: flag indicating whether we should simulate just-in-time, in the sense that only a <code>batchsize</code> number of parameter vectors and corresponding data are in memory at a given time.</li></ul><p>Arguments unique to <code>train(θ̂, P, simulator)</code>:</p><ul><li><code>K::Integer = 10000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K ÷ 5</code>.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices); if <code>ξ</code> is provided, the parameter sampler is called as <code>sampler(K, ξ)</code>.</li><li><code>epochs_per_θ_refresh::Integer = 1</code>: how often to refresh the training parameters. Must be a multiple of <code>epochs_per_Z_refresh</code>.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux
using Distributions
import NeuralEstimators: simulate

# data simulator
m = 15
function simulate(θ_set, m)
	[Float32.(rand(Normal(θ[1], θ[2]), 1, m)) for θ ∈ eachcol(θ_set)]
end

# parameter sampler
K = 10000
Ω = (μ = Normal(0, 1), σ = Uniform(0.1, 1)) # prior
struct sampler
	μ
	σ
end
function (s::sampler)(K)
	μ = rand(s.μ, K)
	σ = rand(s.σ, K)
	θ = hcat(μ, σ)&#39;
	return θ
end
smplr = sampler(Ω.μ, Ω.σ)

# architecture
p = length(Ω)   # number of parameters in the statistical model
w = 32          # width of each layer
ψ = Chain(Dense(1, w, relu), Dense(w, w, relu))
ϕ = Chain(Dense(w, w, relu), Dense(w, p))
θ̂ = DeepSet(ψ, ϕ)

# training: full simulation on-the-fly
θ̂ = train(θ̂, smplr, simulate, m = m, K = K, epochs = 5)
θ̂ = train(θ̂, smplr, simulate, m = m, K = K, epochs = 5)
θ̂ = train(θ̂, smplr, simulate, m = m, K = K, epochs = 10, epochs_per_θ_refresh = 4, epochs_per_Z_refresh = 2)

# training: simulation on-the-fly but with fixed parameters
θ_train = smplr(K)
θ_val   = smplr(K ÷ 5)
θ̂ = train(θ̂, θ_train, θ_val, simulate, m = m, epochs = 5)
θ̂ = train(θ̂, θ_train, θ_val, simulate, m = m, epochs = 5, epochs_per_Z_refresh = 2)

# training: fixed parameters and fixed data
Z_train = simulate(θ_train, m)
Z_val   = simulate(θ_val, m)
θ̂ = train(θ̂, θ_train, θ_val, Z_train, Z_val, epochs = 5)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/train.jl#L1-L103">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.trainx" href="#NeuralEstimators.trainx"><code>NeuralEstimators.trainx</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">trainx(θ̂, P, simulator, M; )
trainx(θ̂, θ_train, θ_val, simulator, M; )
trainx(θ̂, θ_train, θ_val, Z_train::T, Z_val::T, M; )
trainx(θ̂, θ_train, θ_val, Z_train::V, Z_val::V; ) where {V &lt;: AbstractVector{AbstractVector{Any}}}</code></pre><p>A wrapper around <code>train</code> to construct neural estimators for different sample sizes.</p><p>The collection <code>M</code> specifies the desired sample sizes. Each estimator is pre-trained with the estimator for the previous sample size. For example, if <code>M = [m₁, m₂]</code>, the estimator for sample size <code>m₂</code> is pre-trained with the estimator for sample size <code>m₁</code>.</p><p>The method for <code>Z_train::T</code> and <code>Z_val::T</code> subsets the data using <code>subsetdata(Z, 1:mᵢ)</code> for each <code>mᵢ ∈ M</code>. The method for <code>Z_train::V</code> and <code>Z_val::V</code> trains an estimator for each element of <code>Z_train</code> and <code>Z_val</code>; hence, it does not need to invoke <code>subsetdata</code>, which can be slow or difficult to define in some cases (e.g., for graphical data).</p><p>The keyword arguments inherit from <code>train</code>, and certain keyword arguments can be given as vectors. For example, if we are training two estimators, we can use a different number of epochs by providing <code>epochs = [e₁, e₂]</code>. Other arguments that allow vectors are <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/train.jl#L460-L483">source</a></section></article><h2 id="Assessing-a-neural-estimator"><a class="docs-heading-anchor" href="#Assessing-a-neural-estimator">Assessing a neural estimator</a><a id="Assessing-a-neural-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-a-neural-estimator" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">assess(estimators, θ, Z; &lt;keyword args&gt;)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data <code>Z</code> simulated based on true parameter vectors stored in <code>θ</code>.</p><p>If <code>Z</code> contains more data sets than parameter vectors, the parameter matrix will be recycled by horizontal concatenation.</p><p>The output is of type <code>Assessment</code>; see <code>?Assessment</code> for details.</p><p><strong>Keyword arguments</strong></p><ul><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible defaults provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible defaults provided). If <code>ξ</code> is provided with a field <code>parameter_names</code>, those names will be used.</li><li><code>ξ = nothing</code>: an arbitrary collection of objects that are fixed (e.g., distance matrices).</li><li><code>use_ξ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses <code>ξ</code>: if it does, the estimator will be applied as <code>estimator(Z, ξ)</code>. This argument is useful when multiple <code>estimators</code> are provided, only some of which need <code>ξ</code>; hence, if only one estimator is provided and <code>ξ</code> is not <code>nothing</code>, <code>use_ξ</code> is automatically set to <code>true</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Flux

n = 10 # number of observations in each realisation
p = 4  # number of parameters in the statistical model

# Construct the neural estimator
w = 32 # width of each layer
ψ = Chain(Dense(n, w, relu), Dense(w, w, relu));
ϕ = Chain(Dense(w, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)

# Generate testing parameters
K = 100
θ = rand(p, K)

# Data for a single sample size
m = 30
Z = [rand(n, m) for _ ∈ 1:K];
assessment = assess([θ̂], θ, Z);
risk(assessment)

# Multiple data sets for each parameter vector
J = 5
Z = repeat(Z, J);
assessment = assess([θ̂], θ, Z);
risk(assessment)

# With set-level information
qₓ = 2
ϕ  = Chain(Dense(w + qₓ, w, relu), Dense(w, p));
θ̂ = DeepSet(ψ, ϕ)
x = [rand(qₓ) for _ ∈ eachindex(Z)]
assessment = assess([θ̂], θ, (Z, x));
risk(assessment)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/assess.jl#L75-L132">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">Assessment(df::DataFrame, runtime::DataFrame)</code></pre><p>A type for storing the output of <code>assess()</code>. The field <code>runtime</code> contains the total time taken for each estimator. The field <code>df</code> is a long-form <code>DataFrame</code> with columns:</p><ul><li><code>estimator</code>: the name of the estimator</li><li><code>parameter</code>: the name of the parameter</li><li><code>truth</code>:     the true value of the parameter</li><li><code>estimate</code>:  the estimated value of the parameter</li><li><code>m</code>:         the sample size (number of iid replicates)</li><li><code>k</code>:         the index of the parameter vector in the test set</li><li><code>j</code>:         the index of the data set</li></ul><p>Multiple <code>Assessment</code> objects can be combined with <code>merge()</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/assess.jl#L3-L19">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.risk" href="#NeuralEstimators.risk"><code>NeuralEstimators.risk</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">risk(assessment::Assessment; loss = (x, y) -&gt; abs(x - y), average_over_parameters = true)</code></pre><p>Computes a Monte Carlo approximation of the Bayes risk,</p><p class="math-container">\[r_{\Omega}(\hat{\boldsymbol{\theta}}(\cdot))
\approx
\frac{1}{K} \sum_{\boldsymbol{\theta} \in \vartheta} \frac{1}{J} \sum_{\boldsymbol{Z} \in \mathcal{Z}_{\boldsymbol{\theta}}} L(\boldsymbol{\theta}, \hat{\boldsymbol{\theta}}(\boldsymbol{Z})).\]</p><p>where <span>$\vartheta$</span> denotes a set of <span>$K$</span> parameter vectors sampled from the prior <span>$\Omega(\cdot)$</span> and, for each <span>$\boldsymbol{\theta} \in \vartheta$</span>, we have <span>$J$</span> sets of <span>$m$</span> mutually independent realisations from the model collected in <span>$\mathcal{Z}_{\boldsymbol{\theta}}$</span>.</p><p><strong>Keyword arguments</strong></p><ul><li><code>loss = (x, y) -&gt; abs(x - y)</code>: a binary operator defining the loss function (default absolute-error loss).</li><li><code>average_over_parameters::Bool = true</code>: if true (default), the loss is averaged over all parameters; otherwise, the loss is averaged over each parameter separately.</li><li><code>average_over_sample_sizes::Bool = true</code>: if true (default), the loss is averaged over all sample sizes <span>$m$</span>; otherwise, the loss is averaged over each sample size separately.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/assess.jl#L35-L55">source</a></section></article><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.bootstrap" href="#NeuralEstimators.bootstrap"><code>NeuralEstimators.bootstrap</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">bootstrap(θ̂, parameters::P, Z) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, parameters::P, simulator, m::Integer; B = 400) where P &lt;: Union{AbstractMatrix, ParameterConfigurations}
bootstrap(θ̂, Z; B = 400, blocks = nothing)</code></pre><p>Generates <code>B</code> bootstrap estimates from an estimator <code>θ̂</code>.</p><p>Parametric bootstrapping is facilitated by passing a single parameter configuration, <code>parameters</code>, and corresponding simulated data, <code>Z</code>, whose length implicitly defines <code>B</code>. Alternatively, one may provide a <code>simulator</code> and the desired sample size, in which case the data will be simulated using <code>simulator(parameters, m)</code>.</p><p>Non-parametric bootstrapping is facilitated by passing a single data set, <code>Z</code>. The argument <code>blocks</code> caters for block bootstrapping, and it should be a vector of integers specifying the block for each replicate. For example, with 5 replicates, the first two corresponding to block 1 and the remaining three corresponding to block 2, <code>blocks</code> should be <code>[1, 1, 2, 2, 2]</code>. The resampling algorithm aims to produce resampled data sets that are of a similar size to <code>Z</code>, but this can only be achieved exactly if all blocks are equal in length.</p><p>The keyword argument <code>use_gpu</code> is a flag determining whether to use the GPU, if it is available (default <code>true</code>).</p><p>The return type is a p × <code>B</code> matrix, where p is the number of parameters in the model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/bootstrap.jl#L106-L131">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.interval" href="#NeuralEstimators.interval"><code>NeuralEstimators.interval</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">interval(θ̃, θ̂ = nothing; type::String, probs = [0.05, 0.95], parameter_names)</code></pre><p>Compute a confidence interval using the p × B matrix of bootstrap samples, <code>θ̃</code>, where p is the number of parameters in the model.</p><p>If <code>type = &quot;quantile&quot;</code>, the interval is constructed by simply taking the quantiles of <code>θ̃</code>, and if <code>type = &quot;reverse-quantile&quot;</code>, the so-called <a href="https://en.wikipedia.org/wiki/Bootstrapping_(statistics)#Methods_for_bootstrap_confidence_intervals">reverse-quantile</a> method is used. In both cases, the quantile levels are controlled by the argument <code>probs</code>.</p><p>The rows can be named with a vector of strings <code>parameter_names</code>.</p><p>The return type is a p × 2 matrix, whose first and second columns respectively contain the lower and upper bounds of the interval.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
p = 3
B = 50
θ̃ = rand(p, B)
θ̂ = rand(p)
interval(θ̃)
interval(θ̃, θ̂, type = &quot;basic&quot;)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/70f1c06a8b6f6d784dfc4ab64cdc7ae21f3e8822/src/bootstrap.jl#L1-L27">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">« Advanced usage</a><a class="docs-footer-nextpage" href="../architectures/">Architectures and activations functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="auto">Automatic (OS)</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.1.2 on <span class="colophon-date" title="Tuesday 28 November 2023 03:06">Tuesday 28 November 2023</span>. Using Julia version 1.7.3.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
