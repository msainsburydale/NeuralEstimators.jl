<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Miscellaneous · NeuralEstimators.jl</title><meta name="title" content="Miscellaneous · NeuralEstimators.jl"/><meta property="og:title" content="Miscellaneous · NeuralEstimators.jl"/><meta property="twitter:title" content="Miscellaneous · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../methodology/">Methodology</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li><a class="tocitem" href="../architectures/">Architectures</a></li><li><a class="tocitem" href="../approximatedistributions/">Approximate distributions</a></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li class="is-active"><a class="tocitem" href>Miscellaneous</a><ul class="internal"><li><a class="tocitem" href="#Core"><span>Core</span></a></li><li><a class="tocitem" href="#Downstream-inference-algorithms"><span>Downstream-inference algorithms</span></a></li><li><a class="tocitem" href="#Utility-functions"><span>Utility functions</span></a></li></ul></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Miscellaneous</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Miscellaneous</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/utility.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Miscellaneous"><a class="docs-heading-anchor" href="#Miscellaneous">Miscellaneous</a><a id="Miscellaneous-1"></a><a class="docs-heading-anchor-permalink" href="#Miscellaneous" title="Permalink"></a></h1><ul><li><a href="#NeuralEstimators.EM"><code>NeuralEstimators.EM</code></a></li><li><a href="#NeuralEstimators.IndicatorWeights"><code>NeuralEstimators.IndicatorWeights</code></a></li><li><a href="#NeuralEstimators.KernelWeights"><code>NeuralEstimators.KernelWeights</code></a></li><li><a href="#NeuralEstimators.adjacencymatrix"><code>NeuralEstimators.adjacencymatrix</code></a></li><li><a href="#NeuralEstimators.containertype"><code>NeuralEstimators.containertype</code></a></li><li><a href="#NeuralEstimators.encodedata"><code>NeuralEstimators.encodedata</code></a></li><li><a href="#NeuralEstimators.expandgrid"><code>NeuralEstimators.expandgrid</code></a></li><li><a href="#NeuralEstimators.maternchols"><code>NeuralEstimators.maternchols</code></a></li><li><a href="#NeuralEstimators.numberreplicates"><code>NeuralEstimators.numberreplicates</code></a></li><li><a href="#NeuralEstimators.removedata"><code>NeuralEstimators.removedata</code></a></li><li><a href="#NeuralEstimators.rowwisenorm"><code>NeuralEstimators.rowwisenorm</code></a></li><li><a href="#NeuralEstimators.spatialgraph"><code>NeuralEstimators.spatialgraph</code></a></li><li><a href="#NeuralEstimators.stackarrays"><code>NeuralEstimators.stackarrays</code></a></li><li><a href="#NeuralEstimators.subsetdata"><code>NeuralEstimators.subsetdata</code></a></li><li><a href="#NeuralEstimators.subsetparameters"><code>NeuralEstimators.subsetparameters</code></a></li><li><a href="#NeuralEstimators.vectotril"><code>NeuralEstimators.vectotril</code></a></li></ul><h2 id="Core"><a class="docs-heading-anchor" href="#Core">Core</a><a id="Core-1"></a><a class="docs-heading-anchor-permalink" href="#Core" title="Permalink"></a></h2><p>These functions can appear during the core workflow, and may need to be overloaded in some applications.</p><article><details class="docstring" open="true"><summary id="NeuralEstimators.numberreplicates"><a class="docstring-binding" href="#NeuralEstimators.numberreplicates"><code>NeuralEstimators.numberreplicates</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">numberreplicates(Z)</code></pre><p>Generic function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an <code>Array</code> or as a <code>GNNGraph</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L131-L137">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.subsetdata"><a class="docstring-binding" href="#NeuralEstimators.subsetdata"><code>NeuralEstimators.subsetdata</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">subsetdata(Z::V, i) where {V &lt;: AbstractArray{A}} where {A &lt;: Any}
subsetdata(Z::A, i) where {A &lt;: AbstractArray{T, N}} where {T, N}
subsetdata(Z::G, i) where {G &lt;: AbstractGraph}</code></pre><p>Return replicate(s) <code>i</code> from each data set in <code>Z</code>.</p><p>If working with data that are not covered by the default methods, overload the function with the appropriate type for <code>Z</code>.</p><p>For graphical data, calls <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/dev/api/gnngraph/#GraphNeuralNetworks.GNNGraphs.getgraph-Tuple{GNNGraph,%20Int64}"><code>getgraph()</code></a>, where the replicates are assumed be to stored as batched graphs. Since this can be slow, one should consider using a method of <a href="../core/#NeuralEstimators.train"><code>train()</code></a> that does not require the data to be subsetted when working with graphical data (use <a href="#NeuralEstimators.numberreplicates"><code>numberreplicates()</code></a> to check that the training and validation data sets are equally replicated, which prevents subsetting).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using GraphNeuralNetworks
using Flux: batch

d = 1  # dimension of the response variable
n = 4  # number of observations in each realisation
m = 6  # number of replicates in each data set
K = 2  # number of data sets

# Array data
Z = [rand(n, d, m) for k ∈ 1:K]
subsetdata(Z, 2)   # extract second replicate from each data set
subsetdata(Z, 1:3) # extract first 3 replicates from each data set

# Graphical data
e = 8 # number of edges
Z = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ∈ 1:m]) for k ∈ 1:K]
subsetdata(Z, 2)   # extract second replicate from each data set
subsetdata(Z, 1:3) # extract first 3 replicates from each data set</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L173-L211">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.subsetparameters"><a class="docstring-binding" href="#NeuralEstimators.subsetparameters"><code>NeuralEstimators.subsetparameters</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">subsetparameters(parameters::M, indices) where {M &lt;: AbstractMatrix}
subsetparameters(parameters::P, indices) where {P &lt;: ParameterConfigurations}</code></pre><p>Subset <code>parameters</code> using a collection of <code>indices</code>.</p><p>Arrays in <code>parameters::P</code> with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using <code>indices</code>. All other fields are left unchanged. To modify this default behaviour, overload <code>subsetparameters</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/Parameters.jl#L33-L43">source</a></section></details></article><h2 id="Downstream-inference-algorithms"><a class="docs-heading-anchor" href="#Downstream-inference-algorithms">Downstream-inference algorithms</a><a id="Downstream-inference-algorithms-1"></a><a class="docs-heading-anchor-permalink" href="#Downstream-inference-algorithms" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralEstimators.EM"><a class="docstring-binding" href="#NeuralEstimators.EM"><code>NeuralEstimators.EM</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">EM(simulateconditional::Function, MAP::Union{Function, NeuralEstimator}, θ₀ = nothing)</code></pre><p>Implements a (Bayesian) Monte Carlo expectation-maximization (EM) algorithm for  parameter estimation with missing data. The algorithm iteratively simulates missing  data conditional on current parameter estimates, then updates parameters using a  (neural) maximum a posteriori (MAP) estimator.</p><p>The <span>$l$</span>th iteration is given by:</p><p class="math-container">\[\boldsymbol{\theta}^{(l)} =
\underset{\boldsymbol{\theta}}{\mathrm{arg\,max}}
\sum_{h = 1}^H \ell(\boldsymbol{\theta};  \boldsymbol{Z}_1,  \boldsymbol{Z}_2^{(l, h)}) + H\log \pi(\boldsymbol{\theta})\]</p><p>where <span>$\ell((\boldsymbol{\theta};  \boldsymbol{Z}_1,  \boldsymbol{Z}_2)$</span> denotes the complete-data log-likelihood function,  <span>$\boldsymbol{Z} \equiv (\boldsymbol{Z}_1&#39;, \boldsymbol{Z}_2&#39;)&#39;$</span> denotes the complete  data with <span>$\boldsymbol{Z}_1$</span> and <span>$\boldsymbol{Z}_2$</span> the observed and missing  components respectively, <span>$\boldsymbol{Z}_2^{(l, h)}$</span>, <span>$h = 1, \dots, H$</span>, are simulated  from the distribution of <span>$\boldsymbol{Z}_2 \mid \boldsymbol{Z}_1, \boldsymbol{\theta}^{(l-1)}$</span>, and  <span>$\pi(\boldsymbol{\theta})$</span> is the prior density (which can be viewed as a penalty function).</p><p>The algorithm monitors convergence by computing:</p><p class="math-container">\[\max_i \left| \frac{\bar{\theta}_i^{(l+1)} - \bar{\theta}_i^{(l)}}{|\bar{\theta}_i^{(l)}| + \epsilon} \right|\]</p><p>where <span>$\bar{\theta}^{(l)}$</span> is the average of parameter estimates from iteration  <code>burnin+1</code> to iteration <span>$l$</span>, and <span>$\epsilon$</span> is machine precision. Convergence is  declared when this quantity is less than <code>tol</code> for <code>nconsecutive</code> consecutive iterations (see keyword arguments below).</p><p><strong>Fields</strong></p><ul><li><p><code>simulateconditional::Function</code>: Function for simulating missing data conditional on  observed data and current parameter estimates. Must have signature:</p><pre><code class="language-julia hljs">simulateconditional(Z::AbstractArray{Union{Missing, T}}, θ; nsims = 1, kwargs...)</code></pre><p>Returns completed data in the format expected by <code>MAP</code> (e.g., 4D array for CNNs).</p></li><li><p><code>MAP::NeuralEstimator</code>: MAP estimator applied to completed data. </p></li><li><p><code>θ₀</code>: Optional initial parameter values (vector). Can also be provided when calling  the <code>EM</code> object.</p></li></ul><p><strong>Methods</strong></p><p>Once constructed, objects of type <code>EM</code> can be applied to data via the following methods (corresponding to single or multiple data sets, respectively):</p><pre><code class="nohighlight hljs">(em::EM)(Z::A, θ₀::Union{Nothing, Vector} = nothing; ...) where {A &lt;: AbstractArray{Union{Missing, T}, N}} where {T, N}
(em::EM)(Z::V, θ₀::Union{Nothing, Vector, Matrix} = nothing; ...) where {V &lt;: AbstractVector{A}} where {A &lt;: AbstractArray{Union{Missing, T}, N}} where {T, N}</code></pre><p>where <code>Z</code> is the complete data containing the observed data and <code>Missing</code> values.</p><p>For multiple datasets, <code>θ₀</code> can be a vector (same initial values for all) or a matrix  with one column per dataset.</p><p><strong>Keyword Arguments</strong></p><ul><li><code>niterations::Integer = 50</code>: Maximum number of EM iterations.</li><li><code>nsims::Union{Integer, Vector{&lt;:Integer}} = 1</code>: Number of conditional simulations per  iteration. Can be fixed (scalar) or varying (vector of length <code>niterations</code>).</li><li><code>burnin::Integer = 1</code>: Number of initial iterations to discard before averaging  parameter estimates for convergence assessment.</li><li><code>nconsecutive::Integer = 3</code>: Number of consecutive iterations meeting the convergence  criterion required to halt.</li><li><code>tol = 0.01</code>: Convergence tolerance. Algorithm stops if the relative change in  post-burnin averaged parameters is less than <code>tol</code> for <code>nconsecutive</code> iterations.</li><li><code>use_gpu::Bool = true</code>: Whether to use a GPU (if available) for MAP estimation.</li><li><code>verbose::Bool = false</code>: Whether to print iteration details.</li><li><code>kwargs...</code>: Additional arguments passed to <code>simulateconditional</code>.</li></ul><p><strong>Returns</strong></p><p>For a single data set, returns a named tuple containing:</p><ul><li><code>estimate</code>: Final parameter estimate (post-burnin average).</li><li><code>iterates</code>: Matrix of all parameter estimates across iterations (each column is one iteration).</li><li><code>burnin</code>: The burnin value used.</li></ul><p>For multiple data set, returns a matrix with one column per dataset.</p><p><strong>Notes</strong></p><ul><li>If <code>Z</code> contains no missing values, the MAP estimator is applied directly (after  passing through <code>simulateconditional</code> to ensure correct format).</li><li>When using a GPU, data are moved to the GPU for MAP estimation and back to the CPU  for conditional simulation.</li></ul><p><strong>Examples</strong></p><pre><code class="language-julia hljs"># Below we give a pseudocode example; see the &quot;Missing data&quot; section in &quot;Advanced usage&quot; for a concrete example.

# Define conditional simulation function
function sim_conditional(Z, θ; nsims = 1)
    # Your implementation here
    # Returns completed data in format suitable for MAP estimator
end

# Define or load MAP estimator
MAP_estimator = ... # Neural MAP estimator

# Create EM object
em = EM(sim_conditional, MAP_estimator, θ₀ = [1.0, 2.0])

# Apply to data with missing values
Z = ... # Array with Missing entries
result = em(Z, niterations = 100, nsims = 5, tol = 0.001, verbose = true)

# Access results
θ_final = result.estimate
θ_sequence = result.iterates

# Multiple datasets
Z_list = [Z1, Z2, Z3]
estimates = em(Z_list, θ₀ = [1.0, 2.0])  # Matrix with 3 columns</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/missingdata.jl#L1-L120">source</a></section></details></article><h2 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralEstimators.adjacencymatrix"><a class="docstring-binding" href="#NeuralEstimators.adjacencymatrix"><code>NeuralEstimators.adjacencymatrix</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">adjacencymatrix(S::Matrix, k::Integer; maxmin = false, combined = false)
adjacencymatrix(S::Matrix, r::AbstractFloat)
adjacencymatrix(S::Matrix, r::AbstractFloat, k::Integer; random = true)
adjacencymatrix(M::Matrix; k, r, kwargs...)</code></pre><p>Computes a spatially weighted adjacency matrix from spatial locations <code>S</code> based  on either the <code>k</code>-nearest neighbours of each location; all nodes within a disc of fixed radius <code>r</code>; or, if both <code>r</code> and <code>k</code> are provided, a subset of <code>k</code> neighbours within a disc of fixed radius <code>r</code>.</p><p>If <code>S</code> is a square matrix, it is treated as a distance matrix; otherwise, it should be an <span>$n$</span> x <span>$d$</span> matrix, where <span>$n$</span> is the number of spatial locations and <span>$d$</span> is the spatial dimension (typically <span>$d$</span> = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that use of a  maxmin ordering currently requires a matrix of spatial locations (not a distance matrix).</p><p>When using the <code>k</code> nearest neighbours, if <code>maxmin=false</code> (default) the neighbours are chosen based on all points in the graph. If <code>maxmin=true</code>, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the <code>k</code>-nearest neighbours amongst the points that have already appeared in the ordering. If <code>combined=true</code>, the  neighbours are defined to be the union of the <code>k</code>-nearest neighbours and the  <code>k</code>-nearest neighbours subject to a maxmin ordering. </p><p>Two subsampling strategies are implemented when choosing a subset of <code>k</code> neighbours within  a disc of fixed radius <code>r</code>. If <code>random=true</code> (default), the neighbours are randomly selected from  within the disc. If <code>random=false</code>, a deterministic algorithm is used  that aims to preserve the distribution of distances within the neighbourhood set, by choosing  those nodes with distances to the central node corresponding to the  <span>$\{0, \frac{1}{k}, \frac{2}{k}, \dots, \frac{k-1}{k}, 1\}$</span> quantiles of the empirical  distribution function of distances within the disc (this in fact yields up to <span>$k+1$</span> neighbours,  since both the closest and furthest nodes are always included). </p><p>By convention with the functionality in <code>GraphNeuralNetworks.jl</code> which is based on directed graphs,  the neighbours of location <code>i</code> are stored in the column <code>A[:, i]</code> where <code>A</code> is the  returned adjacency matrix. Therefore, the number of neighbours for each location is given by <code>collect(mapslices(nnz, A; dims = 1))</code>, and the number of times each node is  a neighbour of another node is given by <code>collect(mapslices(nnz, A; dims = 2))</code>.</p><p>By convention, we do not consider a location to neighbour itself (i.e., the diagonal elements of the adjacency matrix are zero). </p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators, Distances, SparseArrays

n = 250
d = 2
S = rand(Float32, n, d)
k = 10
r = 0.10

# Memory efficient constructors
adjacencymatrix(S, k)
adjacencymatrix(S, k; maxmin = true)
adjacencymatrix(S, k; maxmin = true, combined = true)
adjacencymatrix(S, r)
adjacencymatrix(S, r, k)
adjacencymatrix(S, r, k; random = false)

# Construct from full distance matrix D
D = pairwise(Euclidean(), S, dims = 1)
adjacencymatrix(D, k)
adjacencymatrix(D, r)
adjacencymatrix(D, r, k)
adjacencymatrix(D, r, k; random = false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/Graphs.jl#L555-L623">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.containertype"><a class="docstring-binding" href="#NeuralEstimators.containertype"><code>NeuralEstimators.containertype</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">containertype(A::Type)
containertype(::Type{A}) where A &lt;: SubArray
containertype(a::A) where A</code></pre><p>Returns the container type of its argument.</p><p>If given a <code>SubArray</code>, returns the container type of the parent array.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">a = rand(3, 4)
containertype(a)
containertype(typeof(a))
[containertype(x) for x ∈ eachcol(a)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L111-L126">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.encodedata"><a class="docstring-binding" href="#NeuralEstimators.encodedata"><code>NeuralEstimators.encodedata</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">encodedata(Z::A; c::T = zero(T)) where {A &lt;: AbstractArray{Union{Missing, T}, N}} where T, N</code></pre><p>For data <code>Z</code> with missing entries, returns an encoded data set <code>(U, W)</code> where  <code>U</code> is the original data <code>Z</code> with missing entries replaced by a fixed constant <code>c</code>,  and <code>W</code> encodes the missingness pattern as an indicator array  equal to one if the corresponding element of <code>Z</code> is observed and zero otherwise.</p><p>The behavior depends on the dimensionality of <code>Z</code>. If <code>Z</code> has 1 or 2 dimensions,  the indicator array <code>W</code> is concatenated along the first dimension of <code>Z</code>. If <code>Z</code> has more than 2  dimensions, <code>W</code> is concatenated along the second-to-last dimension of <code>Z</code>. </p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

Z = rand(16, 16, 1, 1)
Z = removedata(Z, 0.25)	# remove 25% of the data at random
UW = encodedata(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/missingdata.jl#L425-L444">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.expandgrid"><a class="docstring-binding" href="#NeuralEstimators.expandgrid"><code>NeuralEstimators.expandgrid</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">expandgrid(xs, ys)</code></pre><p>Generates a grid of all possible combinations of the elements from two input vectors, <code>xs</code> and <code>ys</code>. </p><p>Same as <code>expand.grid()</code> in <code>R</code>, but currently caters for two dimensions only.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L304-L309">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.IndicatorWeights"><a class="docstring-binding" href="#NeuralEstimators.IndicatorWeights"><code>NeuralEstimators.IndicatorWeights</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">IndicatorWeights(h_max, n_bins::Integer)
(w::IndicatorWeights)(h::Matrix)</code></pre><p>For spatial locations <span>$\boldsymbol{s}$</span> and  <span>$\boldsymbol{u}$</span>, creates a spatial weight function defined as</p><p class="math-container">\[\boldsymbol{w}(\boldsymbol{s}, \boldsymbol{u}) \equiv (\mathbb{I}(h \in B_k) : k = 1, \dots, K)&#39;,\]</p><p>where <span>$\mathbb{I}(\cdot)$</span> denotes the indicator function,  <span>$h \equiv \|\boldsymbol{s} - \boldsymbol{u} \|$</span> is the spatial distance between <span>$\boldsymbol{s}$</span> and  <span>$\boldsymbol{u}$</span>, and <span>$\{B_k : k = 1, \dots, K\}$</span> is a set of <span>$K =$</span><code>n_bins</code> equally-sized distance bins covering the spatial distances between 0 and <code>h_max</code>. </p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators 

h_max = 1
n_bins = 10
w = IndicatorWeights(h_max, n_bins)
h = rand(1, 30) # distances between 30 pairs of spatial locations 
w(h)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/Graphs.jl#L150-L173">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.KernelWeights"><a class="docstring-binding" href="#NeuralEstimators.KernelWeights"><code>NeuralEstimators.KernelWeights</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">KernelWeights(h_max, n_bins::Integer)
(w::KernelWeights)(h::Matrix)</code></pre><p>For spatial locations <span>$\boldsymbol{s}$</span> and  <span>$\boldsymbol{u}$</span>, creates a spatial weight function defined as</p><p class="math-container">\[\boldsymbol{w}(\boldsymbol{s}, \boldsymbol{u}) \equiv (\exp(-(h - \mu_k)^2 / (2\sigma_k^2)) : k = 1, \dots, K)&#39;,\]</p><p>where <span>$h \equiv \|\boldsymbol{s} - \boldsymbol{u}\|$</span> is the spatial distance between <span>$\boldsymbol{s}$</span> and <span>$\boldsymbol{u}$</span>, and <span>${\mu_k : k = 1, \dots, K}$</span> and <span>${\sigma_k : k = 1, \dots, K}$</span> are the means and standard deviations of the Gaussian kernels for each bin, covering the spatial distances between 0 and h_max.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators 

h_max = 1
n_bins = 10
w = KernelWeights(h_max, n_bins)
h = rand(1, 30) # distances between 30 pairs of spatial locations 
w(h)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/Graphs.jl#L192-L213">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.maternchols"><a class="docstring-binding" href="#NeuralEstimators.maternchols"><code>NeuralEstimators.maternchols</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">maternchols(D, ρ, ν, σ² = 1; stack = true)</code></pre><p>Given a matrix <code>D</code> of distances, constructs the Cholesky factor of the covariance matrix under the Matérn covariance function with range parameter <code>ρ</code>, smoothness parameter <code>ν</code>, and marginal variance <code>σ²</code>.</p><p>Providing vectors of parameters will yield a three-dimensional array of Cholesky factors (note that the vectors must of the same length, but a mix of vectors and scalars is allowed). A vector of distance matrices <code>D</code> may also be provided.</p><p>If <code>stack = true</code>, the Cholesky factors will be &quot;stacked&quot; into a three-dimensional array (this is only possible if all distance matrices in <code>D</code> are the same size).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using LinearAlgebra: norm
n  = 10
S  = rand(n, 2)
D  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S), sⱼ ∈ eachrow(S)]
ρ  = [0.6, 0.5]
ν  = [0.7, 1.2]
σ² = [0.2, 0.4]
maternchols(D, ρ, ν)
maternchols([D], ρ, ν)
maternchols(D, ρ, ν, σ²; stack = false)

S̃  = rand(n, 2)
D̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]
maternchols([D, D̃], ρ, ν, σ²)
maternchols([D, D̃], ρ, ν, σ²; stack = false)

S̃  = rand(2n, 2)
D̃  = [norm(sᵢ - sⱼ) for sᵢ ∈ eachrow(S̃), sⱼ ∈ eachrow(S̃)]
maternchols([D, D̃], ρ, ν, σ²; stack = false)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/simulate.jl#L202-L239">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.removedata"><a class="docstring-binding" href="#NeuralEstimators.removedata"><code>NeuralEstimators.removedata</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">removedata(Z::Array, Iᵤ::Vector{T}) where T &lt;: Union{Integer, CartesianIndex}
removedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)
removedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)</code></pre><p>Replaces elements of <code>Z</code> with <code>missing</code>.</p><p>The simplest method accepts a vector <code>Iᵤ</code> that specifes the indices of the data to be removed.</p><p>Alternatively, there are two methods available to randomly generate missing data.</p><p>First, a vector <code>p</code> may be given that specifies the proportion of missingness for each element in the response vector. Hence, <code>p</code> should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If <code>prevent_complete_missing = true</code>, no replicates will contain 100% missingness (note that this can slightly alter the effective values of <code>p</code>).</p><p>Second, if an integer <code>n</code> is provided, all replicates will contain <code>n</code> observations after the data are removed. If <code>fixed_pattern = true</code>, the missingness pattern is fixed for all replicates. If <code>contiguous_pattern = true</code>, the data will be removed in a contiguous block based on a randomly selected starting index. If <code>variable_proportion = true</code>, the proportion of missingness will vary across replicates, with each replicate containing between 1 and <code>n</code> observations after data removal, sampled uniformly (note that <code>variable_proportion</code> overrides <code>fixed_pattern</code>).</p><p>The return type is <code>Array{Union{T, Missing}}</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">d = 5           # dimension of each replicate
m = 2000        # number of replicates
Z = rand(d, m)  # simulated data

# Passing a desired proportion of missingness
p = rand(d)
removedata(Z, p)

# Passing a desired final sample size
n = 3  # number of observed elements of each replicate: must have n &lt;= d
removedata(Z, n)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/missingdata.jl#L265-L307">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.rowwisenorm"><a class="docstring-binding" href="#NeuralEstimators.rowwisenorm"><code>NeuralEstimators.rowwisenorm</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">rowwisenorm(A)</code></pre><p>Computes the row-wise norm of a matrix <code>A</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L16-L19">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.spatialgraph"><a class="docstring-binding" href="#NeuralEstimators.spatialgraph"><code>NeuralEstimators.spatialgraph</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">spatialgraph(S)
spatialgraph(S, Z)
spatialgraph(g::GNNGraph, Z)</code></pre><p>Given spatial data <code>Z</code> measured at spatial locations <code>S</code>, constructs a <a href="https://carlolucibello.github.io/GraphNeuralNetworks.jl/stable/api/gnngraph/#GNNGraph-type"><code>GNNGraph</code></a> ready for use in a graph neural network that employs <a href="../architectures/#NeuralEstimators.SpatialGraphConv"><code>SpatialGraphConv</code></a> layers. </p><p>When <span>$m$</span> independent replicates are collected over the same set of <span>$n$</span> spatial locations,</p><p class="math-container">\[\{\boldsymbol{s}_1, \dots, \boldsymbol{s}_n\} \subset \mathcal{D},\]</p><p>where <span>$\mathcal{D} \subset \mathbb{R}^d$</span> denotes the spatial domain of interest,  <code>Z</code> should be given as an <span>$n \times m$</span> matrix and <code>S</code> should be given as an <span>$n \times d$</span> matrix.  Otherwise, when <span>$m$</span> independent replicates are collected over differing sets of spatial locations,</p><p class="math-container">\[\{\boldsymbol{s}_{ij}, \dots, \boldsymbol{s}_{in_i}\} \subset \mathcal{D}, \quad i = 1, \dots, m,\]</p><p><code>Z</code> should be given as an <span>$m$</span>-vector of <span>$n_i$</span>-vectors, and <code>S</code> should be given as an <span>$m$</span>-vector of <span>$n_i \times d$</span> matrices.</p><p>The spatial information between neighbours is stored as an edge feature, with the specific  information controlled by the keyword arguments <code>stationary</code> and <code>isotropic</code>.  Specifically, the edge feature between node <span>$j$</span> and node <span>$j&#39;$</span> stores the spatial  distance <span>$\|\boldsymbol{s}_{j&#39;} - \boldsymbol{s}_j\|$</span> (if <code>isotropic</code>), the spatial  displacement <span>$\boldsymbol{s}_{j&#39;} - \boldsymbol{s}_j$</span> (if <code>stationary</code>), or the matrix of   locations <span>$(\boldsymbol{s}_{j&#39;}, \boldsymbol{s}_j)$</span> (if <code>!stationary</code>).  </p><p>Additional keyword arguments inherit from <a href="#NeuralEstimators.adjacencymatrix"><code>adjacencymatrix()</code></a> to determine the neighbourhood of each node, with the default being a randomly selected set of  <code>k=30</code> neighbours within a disc of radius <code>r=0.15</code> units.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

# Number of replicates and spatial dimension
m = 5  
d = 2  

# Spatial locations fixed for all replicates
n = 100
S = rand(n, d)
Z = rand(n, m)
g = spatialgraph(S, Z)

# Spatial locations varying between replicates
n = rand(50:100, m)
S = rand.(n, d)
Z = rand.(n)
g = spatialgraph(S, Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/Graphs.jl#L1-L52">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.stackarrays"><a class="docstring-binding" href="#NeuralEstimators.stackarrays"><code>NeuralEstimators.stackarrays</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">stackarrays(v::Vector{&lt;:AbstractArray}; merge::Bool = true)</code></pre><p>Stack a vector of arrays <code>v</code> into a single higher-dimensional array.</p><p>If all arrays have the same size along the last dimension, stacks along a new final dimension. Then, if <code>merge = true</code>, merges the last two dimensions into one.</p><p>Alternatively, if sizes differ along the last dimension, concatenates along the last dimension.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Vector containing arrays of the same size:
Z = [rand(2, 3, m) for m ∈ (1, 1)];
stackarrays(Z)
stackarrays(Z, merge = false)

# Vector containing arrays with differing final dimension size:
Z = [rand(2, 3, m) for m ∈ (1, 2)];
stackarrays(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L324-L343">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.vectotril"><a class="docstring-binding" href="#NeuralEstimators.vectotril"><code>NeuralEstimators.vectotril</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">vectotril(v; strict = false)
vectotriu(v; strict = false)</code></pre><p>Converts a vector <code>v</code> of length <span>$d(d+1)÷2$</span> (a triangular number) into a <span>$d × d$</span> lower or upper triangular matrix.</p><p>If <code>strict = true</code>, the matrix will be <em>strictly</em> lower or upper triangular, that is, a <span>$(d+1) × (d+1)$</span> triangular matrix with zero diagonal.</p><p>Note that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if <code>v</code> is a GPU array. Note also that the return type is not of type <code>Triangular</code> matrix (i.e., the zeros are materialised) since <code>Triangular</code> matrices are not always compatible with other GPU operations.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

d = 4
n = d*(d+1)÷2
v = collect(range(1, n))
vectotril(v)
vectotriu(v)
vectotril(v; strict = true)
vectotriu(v; strict = true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/d43e785d5cdb441960eea2978c35e397acf4d56e/src/utility.jl#L52-L79">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../simulation/">« Model-specific functions</a><a class="docs-footer-nextpage" href="../">Index »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Tuesday 17 February 2026 23:02">Tuesday 17 February 2026</span>. Using Julia version 1.11.9.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
