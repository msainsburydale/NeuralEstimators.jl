<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Miscellaneous · NeuralEstimators.jl</title><meta name="title" content="Miscellaneous · NeuralEstimators.jl"/><meta property="og:title" content="Miscellaneous · NeuralEstimators.jl"/><meta property="twitter:title" content="Miscellaneous · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li><a class="tocitem" href="../architectures/">Architectures</a></li><li><a class="tocitem" href="../summarystatistics/">User-defined summary statistics</a></li><li><a class="tocitem" href="../activationfunctions/">Output activation functions</a></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li class="is-active"><a class="tocitem" href>Miscellaneous</a><ul class="internal"><li><a class="tocitem" href="#Core"><span>Core</span></a></li><li><a class="tocitem" href="#Utility-functions"><span>Utility functions</span></a></li><li><a class="tocitem" href="#Other"><span>Other</span></a></li></ul></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Miscellaneous</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Miscellaneous</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/utility.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Miscellaneous"><a class="docs-heading-anchor" href="#Miscellaneous">Miscellaneous</a><a id="Miscellaneous-1"></a><a class="docs-heading-anchor-permalink" href="#Miscellaneous" title="Permalink"></a></h1><ul><li><a href="#NeuralEstimators.EM"><code>NeuralEstimators.EM</code></a></li><li><a href="#NeuralEstimators.adjacencymatrix"><code>NeuralEstimators.adjacencymatrix</code></a></li><li><a href="#NeuralEstimators.containertype"><code>NeuralEstimators.containertype</code></a></li><li><a href="#NeuralEstimators.encodedata"><code>NeuralEstimators.encodedata</code></a></li><li><a href="#NeuralEstimators.estimateinbatches"><code>NeuralEstimators.estimateinbatches</code></a></li><li><a href="#NeuralEstimators.expandgrid"><code>NeuralEstimators.expandgrid</code></a></li><li><a href="#NeuralEstimators.initialise_estimator"><code>NeuralEstimators.initialise_estimator</code></a></li><li><a href="#NeuralEstimators.loadbestweights"><code>NeuralEstimators.loadbestweights</code></a></li><li><a href="#NeuralEstimators.numberreplicates"><code>NeuralEstimators.numberreplicates</code></a></li><li><a href="#NeuralEstimators.removedata"><code>NeuralEstimators.removedata</code></a></li><li><a href="#NeuralEstimators.rowwisenorm"><code>NeuralEstimators.rowwisenorm</code></a></li><li><a href="#NeuralEstimators.stackarrays"><code>NeuralEstimators.stackarrays</code></a></li><li><a href="#NeuralEstimators.subsetdata"><code>NeuralEstimators.subsetdata</code></a></li><li><a href="#NeuralEstimators.subsetparameters"><code>NeuralEstimators.subsetparameters</code></a></li><li><a href="#NeuralEstimators.vectotril"><code>NeuralEstimators.vectotril</code></a></li></ul><h2 id="Core"><a class="docs-heading-anchor" href="#Core">Core</a><a id="Core-1"></a><a class="docs-heading-anchor-permalink" href="#Core" title="Permalink"></a></h2><p>These functions can appear during the core workflow, and may need to be overloaded in some applications.</p><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.numberreplicates" href="#NeuralEstimators.numberreplicates"><code>NeuralEstimators.numberreplicates</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">numberofreplicates(Z)</code></pre><p>Generic function that returns the number of replicates in a given object. Default implementations are provided for commonly used data formats, namely, data stored as an <code>Array</code> or as a <code>GNNGraph</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L129-L135">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.subsetdata" href="#NeuralEstimators.subsetdata"><code>NeuralEstimators.subsetdata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">subsetdata(Z::V, m) where {V &lt;: AbstractArray{A}} where {A &lt;: Any}
subsetdata(Z::A, m) where {A &lt;: AbstractArray{T, N}} where {T, N}
subsetdata(Z::G, m) where {G &lt;: AbstractGraph}</code></pre><p>Subsets <code>m</code> replicates from data <code>Z</code>.</p><p>Note that <code>subsetdata</code> is slow for graphical data, and one should consider using a method of <code>train</code> that does not require the data to be subsetted when working with graphical data: use <code>numberreplicates</code> to check that the training and validation data sets are equally replicated, which prevents the invocation of <code>subsetdata</code>. Note also that <code>subsetdata</code> only applies to vectors of batched graphs.</p><p>If the user is working with data that is not covered by the default methods, simply overload <code>subsetdata</code> with the appropriate type for <code>Z</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using GraphNeuralNetworks
using Flux: batch

d = 1  # dimension of the response variable
n = 5  # number of observations in each realisation
m = 6  # number of replicates in each data set
K = 2  # number of data sets

# Array data
Z = [rand(n, d, m) for k ∈ 1:K]
subsetdata(Z, 1:3) # extract first 3 replicates from each data set

# Graphical data
e = 8 # number of edges
Z = [batch([rand_graph(n, e, ndata = rand(d, n)) for _ ∈ 1:m]) for k ∈ 1:K]
subsetdata(Z, 1:3) # extract first 3 replicates from each data set</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L174-L209">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.subsetparameters" href="#NeuralEstimators.subsetparameters"><code>NeuralEstimators.subsetparameters</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">subsetparameters(parameters::M, indices) where {M &lt;: AbstractMatrix}
subsetparameters(parameters::P, indices) where {P &lt;: ParameterConfigurations}</code></pre><p>Subset <code>parameters</code> using a collection of <code>indices</code>.</p><p>Arrays in <code>parameters::P</code> with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using <code>indices</code>. All other fields are left unchanged. To modify this default behaviour, overload <code>subsetparameters</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/Parameters.jl#L34-L44">source</a></section></article><h2 id="Utility-functions"><a class="docs-heading-anchor" href="#Utility-functions">Utility functions</a><a id="Utility-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Utility-functions" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.adjacencymatrix" href="#NeuralEstimators.adjacencymatrix"><code>NeuralEstimators.adjacencymatrix</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">adjacencymatrix(M::Matrix, k::Integer; maxmin::Bool = false)
adjacencymatrix(M::Matrix, r::Float)
adjacencymatrix(M::Matrix, r::Float, k::Integer)</code></pre><p>Computes a spatially weighted adjacency matrix from <code>M</code> based on either the <code>k</code>-nearest neighbours of each location; all nodes within a disc of radius <code>r</code>; or, if both <code>r</code> and <code>k</code> are provided, a random set of <code>k</code> neighbours with a disc of radius <code>r</code>.</p><p>If <code>maxmin=false</code> (default) the <code>k</code>-nearest neighbours are chosen based on all points in the graph. If <code>maxmin=true</code>, a so-called maxmin ordering is applied, whereby an initial point is selected, and each subsequent point is selected to maximise the minimum distance to those points that have already been selected. Then, the neighbours of each point are defined as the <code>k</code>-nearest neighbours amongst the points that have already appeared in the ordering.</p><p>If <code>M</code> is a square matrix, it is treated as a distance matrix; otherwise, it should be an <span>$n$</span> x d matrix, where <span>$n$</span> is the number of spatial locations and <span>$d$</span> is the spatial dimension (typically <span>$d$</span> = 2). In the latter case, the distance metric is taken to be the Euclidean distance. Note that the maxmin ordering currently requires a set of spatial locations (not a distance matrix).</p><p>By convention, we consider a location to neighbour itself and, hence, <code>k</code>-neighbour methods will yield <code>k</code>+1 neighbours for each location. Note that one may use <code>dropzeros!()</code> to remove these self-loops from the constructed adjacency matrix (see below).</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators
using Distances
using SparseArrays

n = 100
d = 2
S = rand(n, d)
k = 10
r = 0.1

# Memory efficient constructors (avoids constructing the full distance matrix D)
adjacencymatrix(S, k)
adjacencymatrix(S, k; maxmin = true)
adjacencymatrix(S, r)
adjacencymatrix(S, r, k)

# Construct from full distance matrix D
D = pairwise(Euclidean(), S, S, dims = 1)
adjacencymatrix(D, k)
adjacencymatrix(D, r)
adjacencymatrix(D, r, k)

# Removing self-loops so that a location is not its own neighbour
adjacencymatrix(S, k) |&gt; dropzeros!</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/Graphs.jl#L171-L226">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.containertype" href="#NeuralEstimators.containertype"><code>NeuralEstimators.containertype</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">containertype(A::Type)
containertype(::Type{A}) where A &lt;: SubArray
containertype(a::A) where A</code></pre><p>Returns the container type of its argument.</p><p>If given a <code>SubArray</code>, returns the container type of the parent array.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">a = rand(3, 4)
containertype(a)
containertype(typeof(a))
[containertype(x) for x ∈ eachcol(a)]</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L109-L124">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.encodedata" href="#NeuralEstimators.encodedata"><code>NeuralEstimators.encodedata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">encodedata(Z::A; fixed_constant::T = zero(T)) where {A &lt;: AbstractArray{Union{Missing, T}, N}} where T, N</code></pre><p>For data <code>Z</code> with missing entries, returns an augmented data set (U, W) where W encodes the missingness pattern as an indicator vector and U is the original data Z with missing entries replaced by a <code>fixed_constant</code>.</p><p>The indicator vector W is stored in the second-to-last dimension of <code>Z</code>, which should be a singleton. If the second-to-last dimension is not singleton, then two singleton dimensions will be added to the array, and W will be stored in the new second-to-last dimension.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

# Generate some missing data
Z = rand(16, 16, 1, 1)
Z = removedata(Z, 0.25)	 # remove 25% of the data

# Encode the data
UW = encodedata(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/missingdata.jl#L320-L342">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.estimateinbatches" href="#NeuralEstimators.estimateinbatches"><code>NeuralEstimators.estimateinbatches</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">estimateinbatches(θ̂, z, θ = nothing; batchsize::Integer = 32, use_gpu::Bool = true, kwargs...)</code></pre><p>Apply the estimator <code>θ̂</code> on minibatches of <code>z</code> (and optionally parameter vectors or other set-level information <code>θ</code>) of size <code>batchsize</code>.</p><p>This can prevent memory issues that can occur with large data sets, particularly on the GPU.</p><p>Minibatching will only be done if there are multiple data sets in <code>z</code>; this will be inferred by <code>z</code> being a vector, or a tuple whose first element is a vector.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L353-L365">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.expandgrid" href="#NeuralEstimators.expandgrid"><code>NeuralEstimators.expandgrid</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">expandgrid(xs, ys)</code></pre><p>Same as <code>expand.grid()</code> in <code>R</code>, but currently caters for two dimensions only.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L413-L417">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.initialise_estimator" href="#NeuralEstimators.initialise_estimator"><code>NeuralEstimators.initialise_estimator</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">initialise_estimator(p::Integer; ...)
initialise_estimator(p::Integer, data_type::String; ...)</code></pre><p>Initialise a neural estimator for a statistical model with <code>p</code> unknown parameters.</p><p>The estimator is couched in the DeepSets framework (see <a href="../architectures/#NeuralEstimators.DeepSet"><code>DeepSet</code></a>) so that it can be applied to data sets containing an arbitrary number of independent replicates (including the special case of a single replicate).</p><p>Note also that the user is free to initialise their neural estimator however they see fit using arbitrary <code>Flux</code> code; see <a href="https://fluxml.ai/Flux.jl/stable/models/layers/">here</a> for <code>Flux</code>&#39;s API reference.</p><p>Finally, the method with positional argument <code>data_type</code>is a wrapper that allows one to specify the type of their data (either &quot;unstructured&quot;, &quot;gridded&quot;, or &quot;irregular_spatial&quot;).</p><p><strong>Keyword arguments</strong></p><ul><li><code>architecture::String</code>: for unstructured multivariate data, one may use a densely-connected neural network (<code>&quot;DNN&quot;</code>); for data collected over a grid, a convolutional neural network (<code>&quot;CNN&quot;</code>); and for graphical or irregular spatial data, a graphical neural network (<code>&quot;GNN&quot;</code>).</li><li><code>d::Integer = 1</code>: for unstructured multivariate data (i.e., when <code>architecture = &quot;DNN&quot;</code>), the dimension of the data (e.g., <code>d = 3</code> for trivariate data); otherwise, if <code>architecture ∈ [&quot;CNN&quot;, &quot;GNN&quot;]</code>, the argument <code>d</code> controls the number of input channels (e.g., <code>d = 1</code> for univariate spatial processes).</li><li><code>estimator_type::String = &quot;point&quot;</code>: the type of estimator; either <code>&quot;point&quot;</code> or <code>&quot;interval&quot;</code>.</li><li><code>depth = 3</code>: the number of hidden layers; either a single integer or an integer vector of length two specifying the depth of the inner (summary) and outer (inference) network of the DeepSets framework.</li><li><code>width = 32</code>: a single integer or an integer vector of length <code>sum(depth)</code> specifying the width (or number of convolutional filters/channels) in each hidden layer.</li><li><code>activation::Function = relu</code>: the (non-linear) activation function of each hidden layer.</li><li><code>activation_output::Function = identity</code>: the activation function of the output layer.</li><li><code>variance_stabiliser::Union{Nothing, Function} = nothing</code>: a function that will be applied directly to the input, usually to stabilise the variance.</li><li><code>kernel_size = nothing</code>: (applicable only to CNNs) a vector of length <code>depth[1]</code> containing integer tuples of length <code>D</code>, where <code>D</code> is the dimension of the convolution (e.g., <code>D = 2</code> for two-dimensional convolution).</li><li><code>weight_by_distance::Bool = false</code>: (applicable only to GNNs) flag indicating whether the estimator will weight by spatial distance; if true, a <code>WeightedGraphConv</code> layer is used in the propagation module; otherwise, a regular <code>GraphConv</code> layer is used.</li></ul><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">## DNN, GNN, 1D CNN, and 2D CNN for a statistical model with two parameters:
p = 2
initialise_estimator(p, architecture = &quot;DNN&quot;)
initialise_estimator(p, architecture = &quot;GNN&quot;)
initialise_estimator(p, architecture = &quot;CNN&quot;, kernel_size = [10, 5, 3])
initialise_estimator(p, architecture = &quot;CNN&quot;, kernel_size = [(10, 10), (5, 5), (3, 3)])</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/Estimators.jl#L579-L617">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.loadbestweights" href="#NeuralEstimators.loadbestweights"><code>NeuralEstimators.loadbestweights</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">loadbestweights(path::String)</code></pre><p>Returns the weights of the neural network saved as &#39;best_network.bson&#39; in the given <code>path</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L332-L336">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.removedata" href="#NeuralEstimators.removedata"><code>NeuralEstimators.removedata</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">removedata(Z::Array, Iᵤ::Vector{Integer})
removedata(Z::Array, p::Union{Float, Vector{Float}}; prevent_complete_missing = true)
removedata(Z::Array, n::Integer; fixed_pattern = false, contiguous_pattern = false, variable_proportion = false)</code></pre><p>Replaces elements of <code>Z</code> with <code>missing</code>.</p><p>The simplest method accepts a vector of integers <code>Iᵤ</code> that give the specific indices of the data to be removed.</p><p>Alterntivaly, there are two methods available to generate data that are missing completely at random (MCAR).</p><p>First, a vector <code>p</code> may be given that specifies the proportion of missingness for each element in the response vector. Hence, <code>p</code> should have length equal to the dimension of the response vector. If a single proportion is given, it will be replicated accordingly. If <code>prevent_complete_missing = true</code>, no replicates will contain 100% missingness (note that this can slightly alter the effective values of <code>p</code>).</p><p>Second, if an integer <code>n</code> is provided, all replicates will contain <code>n</code> observations after the data are removed. If <code>fixed_pattern = true</code>, the missingness pattern is fixed for all replicates. If <code>contiguous_pattern = true</code>, the data will be removed in a contiguous block. If <code>variable_proportion = true</code>, the proportion of missingness will vary across replicates, with each replicate containing between 1 and <code>n</code> observations after data removal, sampled uniformly (note that <code>variable_proportion</code> overrides <code>fixed_pattern</code>).</p><p>The return type is <code>Array{Union{T, Missing}}</code>.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">d = 5           # dimension of each replicate
m = 2000        # number of replicates
Z = rand(d, m)  # simulated data

# Passing a desired proportion of missingness
p = rand(d)
removedata(Z, p)

# Passing a desired final sample size
n = 3  # number of observed elements of each replicate: must have n &lt;= d
removedata(Z, n)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/missingdata.jl#L158-L202">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.rowwisenorm" href="#NeuralEstimators.rowwisenorm"><code>NeuralEstimators.rowwisenorm</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">rowwisenorm(A)</code></pre><p>Computes the row-wise norm of a matrix <code>A</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L12-L15">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.stackarrays" href="#NeuralEstimators.stackarrays"><code>NeuralEstimators.stackarrays</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">stackarrays(v::V; merge = true) where {V &lt;: AbstractVector{A}} where {A &lt;: AbstractArray{T, N}} where {T, N}</code></pre><p>Stack a vector of arrays <code>v</code> along the last dimension of each array, optionally merging the final dimension of the stacked array.</p><p>The arrays must be of the same size for the first <code>N-1</code> dimensions. However, if <code>merge = true</code>, the size of the final dimension can vary.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs"># Vector containing arrays of the same size:
Z = [rand(2, 3, m) for m ∈ (1, 1)];
stackarrays(Z)
stackarrays(Z, merge = false)

# Vector containing arrays with differing final dimension size:
Z = [rand(2, 3, m) for m ∈ (1, 2)];
stackarrays(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L461-L480">source</a></section></article><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.vectotril" href="#NeuralEstimators.vectotril"><code>NeuralEstimators.vectotril</code></a> — <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia hljs">vectotril(v; strict = false)
vectotriu(v; strict = false)</code></pre><p>Converts a vector <code>v</code> of length <span>$d(d+1)÷2$</span> (a triangular number) into a <span>$d × d$</span> lower or upper triangular matrix.</p><p>If <code>strict = true</code>, the matrix will be <em>strictly</em> lower or upper triangular, that is, a <span>$(d+1) × (d+1)$</span> triangular matrix with zero diagonal.</p><p>Note that the triangular matrix is constructed on the CPU, but the returned matrix will be a GPU array if <code>v</code> is a GPU array. Note also that the return type is not of type <code>Triangular</code> matrix (i.e., the zeros are materialised) since <code>Traingular</code> matrices are not always compatible with other GPU operations.</p><p><strong>Examples</strong></p><pre><code class="nohighlight hljs">using NeuralEstimators

d = 4
n = d*(d+1)÷2
v = collect(range(1, n))
vectotril(v)
vectotriu(v)
vectotril(v; strict = true)
vectotriu(v; strict = true)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/utility.jl#L48-L75">source</a></section></article><h2 id="Other"><a class="docs-heading-anchor" href="#Other">Other</a><a id="Other-1"></a><a class="docs-heading-anchor-permalink" href="#Other" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-article-toggle-button fa-solid fa-chevron-down" href="javascript:;" title="Collapse docstring"></a><a class="docstring-binding" id="NeuralEstimators.EM" href="#NeuralEstimators.EM"><code>NeuralEstimators.EM</code></a> — <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia hljs">EM(simulateconditional::Function, MAP::Function, θ₀ = nothing)</code></pre><p>A type that implements the Monte Carlo variant of the expectation-maximisation (EM) algorithm, which at <span>$l$</span>th iteration finds the value of 𝛉 that maximises</p><p class="math-container">\[Σₕᴴ ℓ(𝛉;  𝐙₁,  𝐙₂ˡʰ) + log πᴴ(𝛉),\]</p><p>where ℓ(⋅) is the complete-data log-likelihood function, 𝐙 ≡ (𝐙₁&#39;, 𝐙₂&#39;)&#39; denotes the complete data with 𝐙₁ and 𝐙₂ the observed and missing components, respectively, the replicate 𝐙₂ˡʰ, h = 1, …, H, is sampled from the conditional probability distribution of 𝐙₂ given 𝐙₁ and the previous estimates 𝛉ˡ⁻¹, and πᴴ(⋅) ≡ {π(⋅)}ᴴ  is a concentrated version of the original prior density.</p><p><strong>Fields</strong></p><p>The function <code>simulateconditional</code> should be of the form,</p><pre><code class="nohighlight hljs">simulateconditional(Z::A, θ; nsims::Integer = 1) where {A &lt;: AbstractArray{Union{Missing, T}}} where T</code></pre><p>and the completed-data <code>Z</code> should be returned in whatever form is appropriate to be passed to the MAP estimator as <code>MAP(Z)</code>. For example, if the data are gridded and the <code>MAP</code> is a neural MAP estimator based on a CNN architecture, then <code>Z</code> should be returned as a four-dimensional array.</p><p>Note that the <code>MAP</code> estimator should return the <em>joint</em> posterior mode; therefore, a neural MAP estimator should be trained under (a surrogate for) the joint 0-1 loss function (see <a href="../loss/#NeuralEstimators.kpowerloss"><code>kpowerloss</code></a>).</p><p>The starting values <code>θ₀</code> should be a vector, which can be provided either during construction of the <code>EM</code> object, or when applying the <code>EM</code> object to data (see below). The starting values given in a function call take precedence over those stored in the object.</p><p><strong>Methods</strong></p><p>Once constructed, obects of type <code>EM</code> can be applied to data via the methods,</p><pre><code class="nohighlight hljs">(em::EM)(Z::A, θ₀::Union{Nothing, Vector} = nothing; ...) where {A &lt;: AbstractArray{Union{Missing, T}, N}} where {T, N}
(em::EM)(Z::V, θ₀::Union{Nothing, Vector, Matrix} = nothing; ...) where {V &lt;: AbstractVector{A}} where {A &lt;: AbstractArray{Union{Missing, T}, N}} where {T, N}</code></pre><p>where <code>Z</code> is the complete data containing the observed data and <code>Missing</code> values. Note that the second method caters for the case that one has multiple data sets. The keyword arguments are:</p><ul><li><code>niterations::Integer = 50</code>: the maximum number of iterations.</li><li><code>nsims::Integer = 1</code>: the number of conditional replicates used to approximate the conditional expectation.</li><li><code>ξ = nothing</code>: model information needed for conditional simulation (e.g., distance matrices) or in the MAP estimator.</li><li><code>use_ξ_in_simulateconditional::Bool = false</code>: if set to <code>true</code>, the conditional simulator is called as <code>simulateconditional(Z, θ, ξ; nsims = nsims)</code>.</li><li><code>use_ξ_in_MAP::Bool = false</code>: if set to <code>true</code>, the MAP estimator is applied to the conditionally-completed data as <code>MAP(Z, ξ)</code>.</li><li><code>ϵ = 0.01</code>: tolerance used to assess convergence; The algorithm if the relative change in parameter values from successive iterations is less than <code>ϵ</code>.</li><li><code>return_iterates</code>: if <code>true</code>, the estimate at each iteration of the algorithm is returned; otherwise, only the final estimate is returned.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = false</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/77b394653b5ef0c29debe396017148f6ecca5fa3/src/missingdata.jl#L3-L58">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../simulation/">« Model-specific functions</a><a class="docs-footer-nextpage" href="../">Index »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.3.0 on <span class="colophon-date" title="Thursday 4 April 2024 06:30">Thursday 4 April 2024</span>. Using Julia version 1.9.4.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
