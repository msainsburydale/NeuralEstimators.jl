<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Approximate distributions · NeuralEstimators.jl</title><meta name="title" content="Approximate distributions · NeuralEstimators.jl"/><meta property="og:title" content="Approximate distributions · NeuralEstimators.jl"/><meta property="twitter:title" content="Approximate distributions · NeuralEstimators.jl"/><meta name="description" content="Documentation for NeuralEstimators.jl."/><meta property="og:description" content="Documentation for NeuralEstimators.jl."/><meta property="twitter:description" content="Documentation for NeuralEstimators.jl."/><script data-outdated-warner src="../../assets/warner.js"></script><link href="https://cdnjs.cloudflare.com/ajax/libs/lato-font/3.0.0/css/lato-font.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/juliamono/0.050/juliamono.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.2/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../search_index.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-mocha.css" data-theme-name="catppuccin-mocha"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-macchiato.css" data-theme-name="catppuccin-macchiato"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-frappe.css" data-theme-name="catppuccin-frappe"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/catppuccin-latte.css" data-theme-name="catppuccin-latte"/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit"><a href="../../">NeuralEstimators.jl</a></span></div><button class="docs-search-query input is-rounded is-small is-clickable my-2 mx-auto py-1 px-2" id="documenter-search-query">Search docs (Ctrl + /)</button><ul class="docs-menu"><li><a class="tocitem" href="../../">NeuralEstimators</a></li><li><a class="tocitem" href="../../methodology/">Methodology</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/overview/">Overview</a></li><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li><a class="tocitem" href="../core/">Core</a></li><li><a class="tocitem" href="../architectures/">Architectures</a></li><li class="is-active"><a class="tocitem" href>Approximate distributions</a><ul class="internal"><li><a class="tocitem" href="#Distributions"><span>Distributions</span></a></li><li><a class="tocitem" href="#Methods"><span>Methods</span></a></li><li><a class="tocitem" href="#Building-blocks"><span>Building blocks</span></a></li></ul></li><li><a class="tocitem" href="../loss/">Loss functions</a></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Miscellaneous</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><a class="docs-sidebar-button docs-navbar-link fa-solid fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Approximate distributions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Approximate distributions</a></li></ul></nav><div class="docs-right"><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl" title="View the repository on GitHub"><span class="docs-icon fa-brands"></span><span class="docs-label is-hidden-touch">GitHub</span></a><a class="docs-navbar-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/main/docs/src/API/approximatedistributions.md" title="Edit source on GitHub"><span class="docs-icon fa-solid"></span></a><a class="docs-settings-button docs-navbar-link fa-solid fa-gear" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-article-toggle-button fa-solid fa-chevron-up" id="documenter-article-toggle-button" href="javascript:;" title="Collapse all docstrings"></a></div></header><article class="content" id="documenter-page"><h1 id="Approximate-distributions"><a class="docs-heading-anchor" href="#Approximate-distributions">Approximate distributions</a><a id="Approximate-distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Approximate-distributions" title="Permalink"></a></h1><p>When constructing a <a href="../core/#NeuralEstimators.PosteriorEstimator"><code>PosteriorEstimator</code></a>, one must choose an approximate distribution <span>$q(\boldsymbol{\theta}; \boldsymbol{\kappa})$</span>. These distributions are implemented as subtypes of the abstract supertype <a href="#NeuralEstimators.ApproximateDistribution">ApproximateDistribution</a>. </p><h2 id="Distributions"><a class="docs-heading-anchor" href="#Distributions">Distributions</a><a id="Distributions-1"></a><a class="docs-heading-anchor-permalink" href="#Distributions" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralEstimators.ApproximateDistribution"><a class="docstring-binding" href="#NeuralEstimators.ApproximateDistribution"><code>NeuralEstimators.ApproximateDistribution</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">ApproximateDistribution</code></pre><p>An abstract supertype for approximate posterior distributions used in conjunction with a <a href="../core/#NeuralEstimators.PosteriorEstimator"><code>PosteriorEstimator</code></a>. </p><p>Subtypes <code>A &lt;: ApproximateDistribution</code> must implement the following methods: </p><ul><li><code>logdensity(q::A, θ::AbstractMatrix, tz::AbstractMatrix)</code> <ul><li>Used during training and therefore must support automatic differentiation.</li><li><code>θ</code> is a <code>d × K</code> matrix of parameter vectors.</li><li><code>tz</code> is a <code>dstar × K</code> matrix of summary statistics obtained by applying the neural network in the <code>PosteriorEstimator</code> to a collection of <code>K</code> data sets. </li><li>Should return a <code>1 × K</code> matrix, where each entry is the log density <code>log q(θₖ | tₖ)</code> for the <code>k</code>-th data set evaluated at the <code>k</code>-th parameter vector <code>θ[:, k]</code>.</li></ul></li><li><code>sampleposterior(q::A, tz::AbstractMatrix, N::Integer)</code><ul><li>Used during inference and therefore does not need to be differentiable.</li><li>Should return a <code>Vector</code> of length <code>K</code>, where each element is a <code>d × N</code> matrix containing <code>N</code> samples from the approximate posterior <code>q(θ | tₖ)</code> for the <code>k</code>-th data set.</li></ul></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/00f868d60492705a82cba67a2119dd9337e3a035/src/ApproximateDistributions.jl#L1-L14">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.GaussianMixture"><a class="docstring-binding" href="#NeuralEstimators.GaussianMixture"><code>NeuralEstimators.GaussianMixture</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">GaussianMixture &lt;: ApproximateDistribution
GaussianMixture(d::Integer, dstar::Integer; num_components::Integer = 10, kwargs...)</code></pre><p>A mixture of Gaussian distributions for amortised posterior inference, where <code>d</code> is the dimension of the parameter vector. </p><p>The density of the distribution is: </p><p class="math-container">\[q(\boldsymbol{\theta}; \boldsymbol{\kappa}) = \sum_{j=1}^{J} \pi_j \cdot \mathcal{N}(\boldsymbol{\theta}; \boldsymbol{\mu}_j, \boldsymbol{\Sigma}_j), \]</p><p>where the parameters <span>$\boldsymbol{\kappa}$</span> comprise the mixture weights <span>$\pi_j \in [0, 1]$</span> subject to <span>$\sum_{j=1}^{J} \pi_j = 1$</span>, the mean vector <span>$\boldsymbol{\mu}_j$</span> of each component, and the variance parameters of the diagonal covariance matrix <span>$\boldsymbol{\Sigma}_j$</span>.</p><p>When using a <code>GaussianMixture</code> as the approximate distribution of a <a href="../core/#NeuralEstimators.PosteriorEstimator"><code>PosteriorEstimator</code></a>,  the neural network should be a mapping from the sample space to <span>$\mathbb{R}^{d^*}$</span>,  where <span>$d^*$</span> is an appropriate number of summary statistics for the parameter vector <span>$\boldsymbol{\theta}$</span>. The summary statistics are then mapped to the mixture parameters using a conventional multilayer perceptron (<a href="../architectures/#NeuralEstimators.MLP">MLP</a>) with approporiately chosen output activation functions (e.g., <a href="https://fluxml.ai/Flux.jl/stable/reference/models/nnlib/#NNlib.softmax">softmax</a> for the mixture weights, <a href="https://fluxml.ai/Flux.jl/stable/reference/models/activation/#NNlib.softplus">softplus</a> for the variance parameters).</p><p><strong>Keyword arguments</strong></p><ul><li><code>num_components::Integer = 10</code>: number of components in the mixture. </li><li><code>kwargs</code>: additional keyword arguments passed to <a href="../architectures/#NeuralEstimators.MLP"><code>MLP</code></a>. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/00f868d60492705a82cba67a2119dd9337e3a035/src/ApproximateDistributions.jl#L29-L47">source</a></section></details></article><article><details class="docstring" open="true"><summary id="NeuralEstimators.NormalisingFlow"><a class="docstring-binding" href="#NeuralEstimators.NormalisingFlow"><code>NeuralEstimators.NormalisingFlow</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">NormalisingFlow &lt;: ApproximateDistribution
NormalisingFlow(d::Integer, dstar::Integer; num_coupling_layers::Integer = 6, kwargs...)</code></pre><p>A normalising flow for amortised posterior inference (e.g., <a href="https://openreview.net/forum?id=rJed6j0cKX">Ardizzone et al., 2019</a>; <a href="https://ieeexplore.ieee.org/document/9298920">Radev et al., 2022</a>), where <code>d</code> is the dimension of  the parameter vector and <code>dstar</code> is the dimension of the summary statistics for the data. </p><p>Normalising flows are diffeomorphisms (i.e., invertible, differentiable transformations with differentiable inverses) that map a simple base distribution (e.g., standard Gaussian) to a more complex target distribution (e.g., the posterior). They achieve this by applying a sequence of learned transformations, the forms of which are chosen to be invertible and allow for tractable density computation via the change of variables formula. This allows for efficient density evaluation during the training stage, and efficient sampling during the inference stage. For further details, see the reviews by <a href="https://ieeexplore.ieee.org/document/9089305">Kobyzev et al. (2020)</a> and <a href="https://dl.acm.org/doi/abs/10.5555/3546258.3546315">Papamakarios (2021)</a>.</p><p><code>NormalisingFlow</code> uses affine coupling blocks (see <a href="#NeuralEstimators.AffineCouplingBlock"><code>AffineCouplingBlock</code></a>), with activation normalisation (<a href="https://dl.acm.org/doi/10.5555/3327546.3327685">Kingma and Dhariwal, 2018</a>) and permutations used between each block. The base distribution is taken to be a standard multivariate Gaussian distribution. </p><p>When using a <code>NormalisingFlow</code> as the approximate distribution of a <a href="../core/#NeuralEstimators.PosteriorEstimator"><code>PosteriorEstimator</code></a>,  the neural network should be a mapping from the sample space to <span>$\mathbb{R}^{d^*}$</span>,  where <span>$d^*$</span> is an appropriate number of summary statistics for the given parameter vector (e.g., <span>$d^* = d$</span>). The summary statistics are then mapped to the parameters of the affine coupling blocks using conventional multilayer perceptrons (see <a href="#NeuralEstimators.AffineCouplingBlock"><code>AffineCouplingBlock</code></a>).</p><p><strong>Keyword arguments</strong></p><ul><li><code>num_coupling_layers::Integer = 6</code>: number of coupling layers. </li><li><code>kwargs</code>: additional keyword arguments passed to <a href="#NeuralEstimators.AffineCouplingBlock"><code>AffineCouplingBlock</code></a>. </li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/00f868d60492705a82cba67a2119dd9337e3a035/src/ApproximateDistributions.jl#L314-L331">source</a></section></details></article><h2 id="Methods"><a class="docs-heading-anchor" href="#Methods">Methods</a><a id="Methods-1"></a><a class="docs-heading-anchor-permalink" href="#Methods" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralEstimators.numdistributionalparams"><a class="docstring-binding" href="#NeuralEstimators.numdistributionalparams"><code>NeuralEstimators.numdistributionalparams</code></a> — <span class="docstring-category">Function</span></summary><section><div><pre><code class="language-julia hljs">numdistributionalparams(q::ApproximateDistribution)
numdistributionalparams(estimator::PosteriorEstimator)</code></pre><p>The number of distributional parameters (i.e., the dimension of the space <span>$\mathcal{K}$</span> of approximate-distribution parameters <span>$\boldsymbol{\kappa}$</span>). </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/00f868d60492705a82cba67a2119dd9337e3a035/src/ApproximateDistributions.jl#L17-L21">source</a></section></details></article><h2 id="Building-blocks"><a class="docs-heading-anchor" href="#Building-blocks">Building blocks</a><a id="Building-blocks-1"></a><a class="docs-heading-anchor-permalink" href="#Building-blocks" title="Permalink"></a></h2><article><details class="docstring" open="true"><summary id="NeuralEstimators.AffineCouplingBlock"><a class="docstring-binding" href="#NeuralEstimators.AffineCouplingBlock"><code>NeuralEstimators.AffineCouplingBlock</code></a> — <span class="docstring-category">Type</span></summary><section><div><pre><code class="language-julia hljs">AffineCouplingBlock(κ₁::MLP, κ₂::MLP)
AffineCouplingBlock(d₁::Integer, dstar::Integer, d₂; kwargs...)</code></pre><p>An affine coupling block used in a <a href="#NeuralEstimators.NormalisingFlow"><code>NormalisingFlow</code></a>. </p><p>An affine coupling block splits its input <span>$\boldsymbol{\theta}$</span> into two disjoint components, <span>$\boldsymbol{\theta}_1$</span> and <span>$\boldsymbol{\theta}_2$</span>, with dimensions <span>$d_1$</span> and <span>$d_2$</span>, respectively. The block then applies the following transformation: </p><p class="math-container">\[\begin{aligned}
    \tilde{\boldsymbol{\theta}}_1 &amp;= \boldsymbol{\theta}_1,\\
    \tilde{\boldsymbol{\theta}}_2 &amp;= \boldsymbol{\theta}_2 \odot \exp\{\boldsymbol{\kappa}_{\boldsymbol{\gamma},1}(\tilde{\boldsymbol{\theta}}_1, \boldsymbol{T}(\boldsymbol{Z}))\} + \boldsymbol{\kappa}_{\boldsymbol{\gamma},2}(\tilde{\boldsymbol{\theta}}_1, \boldsymbol{T}(\boldsymbol{Z})),
\end{aligned}\]</p><p>where <span>$\boldsymbol{\kappa}_{\boldsymbol{\gamma},1}(\cdot)$</span> and <span>$\boldsymbol{\kappa}_{\boldsymbol{\gamma},2}(\cdot)$</span> are generic, non-invertible multilayer perceptrons (MLPs) that are functions of both the (transformed) first input component <span>$\tilde{\boldsymbol{\theta}}_1$</span> and the learned <span>$d^*$</span>-dimensional summary statistics <span>$\boldsymbol{T}(\boldsymbol{Z})$</span> (see <a href="../core/#NeuralEstimators.PosteriorEstimator"><code>PosteriorEstimator</code></a>). </p><p>To prevent numerical overflows and stabilise the training of the model, the scaling factors <span>$\boldsymbol{\kappa}_{\boldsymbol{\gamma},1}(\cdot)$</span> are clamped using the function </p><p class="math-container">\[f(\boldsymbol{s}) = \frac{2c}{\pi}\tan^{-1}(\frac{\boldsymbol{s}}{c}),\]</p><p>where <span>$c = 1.9$</span> is a fixed clamping threshold. This transformation ensures that the scaling factors do not grow excessively large.</p><p>Additional keyword arguments <code>kwargs</code> are passed to the <a href="../architectures/#NeuralEstimators.MLP"><code>MLP</code></a> constructor when creating <code>κ₁</code> and <code>κ₂</code>. </p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/00f868d60492705a82cba67a2119dd9337e3a035/src/ApproximateDistributions.jl#L184-L205">source</a></section></details></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../architectures/">« Architectures</a><a class="docs-footer-nextpage" href="../loss/">Loss functions »</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="auto">Automatic (OS)</option><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option><option value="catppuccin-latte">catppuccin-latte</option><option value="catppuccin-frappe">catppuccin-frappe</option><option value="catppuccin-macchiato">catppuccin-macchiato</option><option value="catppuccin-mocha">catppuccin-mocha</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> version 1.16.1 on <span class="colophon-date" title="Wednesday 28 January 2026 11:42">Wednesday 28 January 2026</span>. Using Julia version 1.11.8.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
