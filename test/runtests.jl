using NeuralEstimators
import NeuralEstimators: simulate
using NeuralEstimators: _getindices, _runondevice, _check_sizes, _extractŒ∏, nested_eltype, rowwisenorm
using CUDA
using DataFrames
using Distributions
using Distances
using Flux
using Flux: batch, DataLoader, mae, mse
using GaussianRandomFields
using Graphs
using GraphNeuralNetworks
using LinearAlgebra
using Random: seed!
using SparseArrays: nnz
using SpecialFunctions: gamma
using Statistics
using Statistics: mean, sum
using Test
using Zygote
array(size...; T = Float32) = T.(reshape(1:prod(size), size...) ./ prod(size))
arrayn(size...; T = Float32) = array(size..., T = T) .- mean(array(size..., T = T))
verbose = false # verbose used in NeuralEstimators code (not @testset)

if CUDA.functional()
	@info "Testing on both the CPU and the GPU... "
	CUDA.allowscalar(false)
	devices = (CPU = cpu, GPU = gpu)
else
	@info "The GPU is unavailable so we will test on the CPU only... "
	devices = (CPU = cpu,)
end

# ---- Stand-alone functions ----

# Start testing low-level functions, which form the base of the dependency tree.
@testset "UtilityFunctions" begin
	@testset "nested_eltype" begin
		@test nested_eltype([rand(5)]) == Float64
	end
	@testset "drop" begin
		@test drop((a = 1, b = 2, c = 3, d = 4), :b) == (a = 1, c = 3, d = 4)
		@test drop((a = 1, b = 2, c = 3), (:b, :d)) == (a = 1, c = 3)
	end
	@testset "expandgrid" begin
		@test expandgrid(1:2, 0:3) == [1 0; 2 0; 1 1; 2 1; 1 2; 2 2; 1 3; 2 3]
		@test expandgrid(1:2, 1:2) == expandgrid(2)
	end
	@testset "_getindices" begin
		m = (3, 4, 6)
		v = [array(16, 16, 1, m·µ¢) for m·µ¢ ‚àà m]
		@test _getindices(v) == [1:3, 4:7, 8:13]
	end
	@testset "stackarrays" begin
		# Vector containing arrays of the same size:
		A = array(2, 3, 4); v = [A, A]; N = ndims(A);
		@test stackarrays(v) == cat(v..., dims = N)
		@test stackarrays(v, merge = false) == cat(v..., dims = N + 1)

		# Vector containing arrays with differing final dimension size:
		A‚ÇÅ = array(2, 3, 4); A‚ÇÇ = array(2, 3, 5); v = [A‚ÇÅ, A‚ÇÇ];
		@test stackarrays(v) == cat(v..., dims = N)
	end
	@testset "subsetparameters" begin

		struct TestParameters <: ParameterConfigurations
			v
			Œ∏
			chols
		end

		K = 4
		parameters = TestParameters(array(K), array(3, K), array(2, 2, K))
		indices = 2:3
		parameters_subset = subsetparameters(parameters, indices)
		@test parameters_subset.Œ∏     == parameters.Œ∏[:, indices]
		@test parameters_subset.chols == parameters.chols[:, :, indices]
		@test parameters_subset.v     == parameters.v[indices]
		@test size(subsetparameters(parameters, 2), 2) == 1

		## Parameters stored as a simple matrix
		parameters = rand(3, K)
		indices = 2:3
		parameters_subset = subsetparameters(parameters, indices)
		@test size(parameters_subset) == (3, 2)
		@test parameters_subset       == parameters[:, indices]
		@test size(subsetparameters(parameters, 2), 2) == 1

	end
	@testset "containertype" begin
		a = rand(3, 4)
		T = Array
		@test containertype(a) == T
		@test containertype(typeof(a)) == T
		@test all([containertype(x) for x ‚àà eachcol(a)] .== T)
	end

	@test isnothing(_check_sizes(1, 1))
end


using NeuralEstimators: triangularnumber
@testset "summary statistics: $dvc" for dvc ‚àà devices
	d, m = 3, 5 # 5 independent replicates of a 3-dimensional vector
	z = rand(d, m) |> dvc
	@test samplesize(z) == m
	@test length(samplecovariance(z)) == triangularnumber(d)
	@test length(samplecorrelation(z)) == triangularnumber(d-1)

	# vector input
	z = rand(d) |> dvc
	@test samplesize(z) == 1
	@test_throws Exception samplecovariance(z)
	@test_throws Exception samplecorrelation(z)
end


@testset "maternclusterprocess" begin

	S = maternclusterprocess()
	@test size(S, 2) == 2

end

@testset "adjacencymatrix" begin

	n = 100
	d = 2
	S = rand(Float32, n, d) #TODO add test that adjacencymatrix is type stable when S or D are Float32 matrices
	k = 5
	r = 0.3

	# Memory efficient constructors (avoids constructing the full distance matrix D)
	A‚ÇÅ = adjacencymatrix(S, k)
	A‚ÇÇ = adjacencymatrix(S, r)
	A = adjacencymatrix(S, k, maxmin = true)
	A = adjacencymatrix(S, k, maxmin = true, moralise = true)
	A = adjacencymatrix(S, k, maxmin = true, combined = true)

	# Construct from full distance matrix D
	D = pairwise(Euclidean(), S, S, dims = 1)
	AÃÉ‚ÇÅ = adjacencymatrix(D, k)
	AÃÉ‚ÇÇ = adjacencymatrix(D, r)

	# Test that the matrices are the same irrespective of which method was used
	@test AÃÉ‚ÇÅ ‚âà A‚ÇÅ
	@test AÃÉ‚ÇÇ ‚âà A‚ÇÇ

	# Randomly selecting k nodes within a node's neighbourhood disc
	seed!(1); A‚ÇÉ = adjacencymatrix(S, k, r)
	@test A‚ÇÉ.n == A‚ÇÉ.m == n
	@test length(adjacencymatrix(S, k, 0.02).nzval) < k*n
	seed!(1); AÃÉ‚ÇÉ = adjacencymatrix(D, k, r)
	@test AÃÉ‚ÇÉ ‚âà A‚ÇÉ

	# Test that the number of neighbours is correct 
	f(A) = collect(mapslices(nnz, A; dims = 1))
	@test all(f(adjacencymatrix(S, k)) .== k) 
	@test all(0 .<= f(adjacencymatrix(S, k; maxmin = true)) .<= k) 
	@test all(k .<= f(adjacencymatrix(S, k; maxmin = true, combined = true)) .<= 2k) 
	@test all(1 .<= f(adjacencymatrix(S, r, k; random = true)) .<= k) 
	@test all(1 .<= f(adjacencymatrix(S, r, k; random = false)) .<= k+1)
	@test all(f(adjacencymatrix(S, 2.0, k; random = true)) .== k) 
	@test all(f(adjacencymatrix(S, 2.0, k; random = false)) .== k+1) 

	# Gridded locations (useful for checking functionality in the event of ties)
	pts = range(0, 1, length = 10) 
	S = expandgrid(pts, pts)
	@test all(f(adjacencymatrix(S, k)) .== k) 
	@test all(0 .<= f(adjacencymatrix(S, k; maxmin = true)) .<= k)
	@test all(k .<= f(adjacencymatrix(S, k; maxmin = true, combined = true)) .<= 2k) 
	@test all(1 .<= f(adjacencymatrix(S, r, k; random = true)) .<= k) 
	@test all(1 .<= f(adjacencymatrix(S, r, k; random = false)) .<= k+1) 
	@test all(f(adjacencymatrix(S, 2.0, k; random = true)) .== k) 
	@test all(f(adjacencymatrix(S, 2.0, k; random = false)) .== k+1) 

	# Check that k > n doesn't cause an error
	n = 3
	d = 2
	S = rand(n, d)
	adjacencymatrix(S, k)
	adjacencymatrix(S, r, k)
	D = pairwise(Euclidean(), S, S, dims = 1)
	adjacencymatrix(D, k)
	adjacencymatrix(D, r, k)
end

@testset "spatialgraph" begin 
	# Number of replicates, and spatial dimension
	m = 5  # number of replicates
	d = 2  # spatial dimension

	# Spatial locations fixed for all replicates
	n = 100
	S = rand(n, d)
	Z = rand(n, m)
	g = spatialgraph(S)
	g = spatialgraph(g, Z)
	g = spatialgraph(S, Z)

	# Spatial locations varying between replicates
	n = rand(50:100, m)
	S = rand.(n, d)
	Z = rand.(n)
	g = spatialgraph(S)
	g = spatialgraph(g, Z)
	g = spatialgraph(S, Z)

	# Mutlivariate processes: spatial locations fixed for all replicates
	q = 2 # bivariate spatial process
	n = 100
	S = rand(n, d)
	Z = rand(q, n, m)  
	g = spatialgraph(S)
	g = spatialgraph(g, Z)
	g = spatialgraph(S, Z)

	# Mutlivariate processes: spatial locations varying between replicates
	n = rand(50:100, m)
	S = rand.(n, d)
	Z = rand.(q, n)
	g = spatialgraph(S)
	g = spatialgraph(g, Z) 
	g = spatialgraph(S, Z) 
end


@testset "missingdata" begin

	# ---- removedata() ----
	d = 5     # dimension of each replicate
	n = 3     # number of observed elements of each replicate: must have n <= d
	m = 2000  # number of replicates
	p = rand(d)

	Z = rand(d)
	removedata(Z, n)
	removedata(Z, p[1])
	removedata(Z, p)

	Z = rand(d, m)
	removedata(Z, n)
	removedata(Z, d)
	removedata(Z, n; fixed_pattern = true)
	removedata(Z, n; contiguous_pattern = true)
	removedata(Z, n, variable_proportion = true)
	removedata(Z, n; contiguous_pattern = true, fixed_pattern = true)
	removedata(Z, n; contiguous_pattern = true, variable_proportion = true)
	removedata(Z, p)
	removedata(Z, p; prevent_complete_missing = false)
	# Check that the probability of missingness is roughly correct:
	mapslices(x -> sum(ismissing.(x))/length(x), removedata(Z, p), dims = 2)
	# Check that none of the replicates contain 100% missing:
	@test !(d ‚àà unique(mapslices(x -> sum(ismissing.(x)), removedata(Z, p), dims = 1)))


	# ---- encodedata() ----
	n = 16
	Z = rand(n)
	Z = removedata(Z, 0.25)
	UW = encodedata(Z);
	@test ndims(UW) == 3
	@test size(UW) == (n, 2, 1)

	Z = rand(n, n)
	Z = removedata(Z, 0.25)
	UW = encodedata(Z);
	@test ndims(UW) == 4
	@test size(UW) == (n, n, 2, 1)

	Z = rand(n, n, 1, 1)
	Z = removedata(Z, 0.25)
	UW = encodedata(Z);
	@test ndims(UW) == 4
	@test size(UW) == (n, n, 2, 1)

	m = 5
	Z = rand(n, n, 1, m)
	Z = removedata(Z, 0.25)
	UW = encodedata(Z);
	@test ndims(UW) == 4
	@test size(UW) == (n, n, 2, m)
end


#TODO update this
# @testset "SpatialGraphConv" begin
# 	m = 5            # number of replicates
# 	d = 2            # spatial dimension
# 	n = 100          # number of spatial locations
# 	S = rand(n, d)   # spatial locations
# 	Z = rand(n, m)   # toy data
# 	g = spatialgraph(S, Z)
# 	layer1 = SpatialGraphConv(1 => 16)
# 	layer2 = SpatialGraphConv(16 => 32)
# 	show(devnull, layer1)
# 	h = layer1(g)
# 	@test size(h.ndata.Z) == (16, m, n)
# 	layer2(h)
# end

@testset "loss functions: $dvc" for dvc ‚àà devices

	p = 3
	K = 10
	Œ∏ÃÇ = arrayn(p, K)       |> dvc
	Œ∏ = arrayn(p, K) * 0.9 |> dvc

	@testset "kpowerloss" begin
		@test kpowerloss(Œ∏ÃÇ, Œ∏, 2; safeorigin = false, joint=false) ‚âà mse(Œ∏ÃÇ, Œ∏)
		@test kpowerloss(Œ∏ÃÇ, Œ∏, 1; safeorigin = false, joint=false) ‚âà mae(Œ∏ÃÇ, Œ∏)
		@test kpowerloss(Œ∏ÃÇ, Œ∏, 1; safeorigin = true, joint=false) ‚âà mae(Œ∏ÃÇ, Œ∏)
		@test kpowerloss(Œ∏ÃÇ, Œ∏, 0.1) >= 0
	end

	@testset "quantileloss" begin
		q = 0.5
		@test quantileloss(Œ∏ÃÇ, Œ∏, q) >= 0
		@test quantileloss(Œ∏ÃÇ, Œ∏, q) ‚âà mae(Œ∏ÃÇ, Œ∏)/2

		q = [0.025, 0.975]
		@test_throws Exception quantileloss(Œ∏ÃÇ, Œ∏, q)
		Œ∏ÃÇ = arrayn(length(q) * p, K) |> dvc
		@test quantileloss(Œ∏ÃÇ, Œ∏, q) >= 0
	end

	@testset "intervalscore" begin
		Œ± = 0.025
		Œ∏ÃÇ = arrayn(2p, K) |> dvc
		@test intervalscore(Œ∏ÃÇ, Œ∏, Œ±) >= 0
	end

end

@testset "simulate" begin

	n = 10
	S = array(n, 2, T = Float32)
	D = [norm(s·µ¢ - s‚±º) for s·µ¢ ‚àà eachrow(S), s‚±º in eachrow(S)]
	œÅ = Float32.([0.6, 0.8])
	ŒΩ = Float32.([0.5, 0.7])
	L = maternchols(D, œÅ, ŒΩ)
	œÉ¬≤ = 0.5f0
	L = maternchols(D, œÅ, ŒΩ, œÉ¬≤)
	@test maternchols(D, œÅ, ŒΩ, œÉ¬≤) == maternchols([D, D], œÅ, ŒΩ, œÉ¬≤)
	L‚ÇÅ = L[:, :, 1]
	m = 5

	@test eltype(simulateschlather(L‚ÇÅ, m)) == Float32
	# @code_warntype simulateschlather(L‚ÇÅ, m)

	@test eltype(simulategaussianprocess(L‚ÇÅ, m)) == Float32
	# @code_warntype simulategaussianprocess(L‚ÇÅ, œÉ, m)

	# Passing GaussianRandomFields:
	cov = CovarianceFunction(2, Matern(œÅ[1], ŒΩ[1]))
	grf = GaussianRandomField(cov, GaussianRandomFields.Cholesky(), S)
	y‚ÇÅ  = simulategaussianprocess(L‚ÇÅ)
	y‚ÇÇ  = simulategaussianprocess(grf)
	y‚ÇÉ  = simulateschlather(grf)
	@test length(y‚ÇÅ) == length(y‚ÇÇ) == length(y‚ÇÉ)
	@test size(grf) == size(grf, 1) == n
end

# Testing the function simulate(): Univariate Gaussian model with unknown mean and standard deviation
p = 2
K = 10
m = 15
parameters = rand(p, K)
simulate(parameters, m) = [Œ∏[1] .+ Œ∏[2] .* randn(1, m) for Œ∏ ‚àà eachcol(parameters)]
simulate(parameters, m)
simulate(parameters, m, 2)
simulate(parameters, m) = ([Œ∏[1] .+ Œ∏[2] .* randn(1, m) for Œ∏ ‚àà eachcol(parameters)], rand(2)) # Tuple (used for passing set-level covariate information)
simulate(parameters, m)
simulate(parameters, m, 2)


@testset "densities" begin

	# "scaledlogistic"
	@test all(4 .<= scaledlogistic.(-10:10, 4, 5) .<= 5)
	@test all(scaledlogit.(scaledlogistic.(-10:10, 4, 5), 4, 5) .‚âà -10:10)
	Œ© = (œÉ = 1:10, œÅ = (2, 7))
	Œ© = [Œ©...] # convert to array since broadcasting over dictionaries and NamedTuples is reserved
	Œ∏ = [-10, 15]
	@test all(minimum.(Œ©) .<= scaledlogistic.(Œ∏, Œ©) .<= maximum.(Œ©))
	@test all(scaledlogit.(scaledlogistic.(Œ∏, Œ©), Œ©) .‚âà Œ∏)

	# Check that the pdf is consistent with the cdf using finite differences
	using NeuralEstimators: _schlatherbivariatecdf
	function finitedifference(z‚ÇÅ, z‚ÇÇ, œà, œµ = 0.0001)
		(_schlatherbivariatecdf(z‚ÇÅ + œµ, z‚ÇÇ + œµ, œà) - _schlatherbivariatecdf(z‚ÇÅ - œµ, z‚ÇÇ + œµ, œà) - _schlatherbivariatecdf(z‚ÇÅ + œµ, z‚ÇÇ - œµ, œà) + _schlatherbivariatecdf(z‚ÇÅ - œµ, z‚ÇÇ - œµ, œà)) / (4 * œµ^2)
	end
	function finitedifference_check(z‚ÇÅ, z‚ÇÇ, œà)
		@test abs(finitedifference(z‚ÇÅ, z‚ÇÇ, œà) - schlatherbivariatedensity(z‚ÇÅ, z‚ÇÇ, œà; logdensity=false)) < 0.0001
	end
	finitedifference_check(0.3, 0.8, 0.2)
	finitedifference_check(0.3, 0.8, 0.9)
	finitedifference_check(3.3, 3.8, 0.2)
	finitedifference_check(3.3, 3.8, 0.9)

	# Other small tests
	@test schlatherbivariatedensity(3.3, 3.8, 0.9; logdensity = false) ‚âà exp(schlatherbivariatedensity(3.3, 3.8, 0.9))
	y = [0.2, 0.4, 0.3]
	n = length(y)
	# construct a diagonally dominant covariance matrix (pos. def. guaranteed via Gershgorins Theorem)
	Œ£ = array(n, n)
	Œ£[diagind(Œ£)] .= diag(Œ£) + sum(Œ£, dims = 2)
	L  = cholesky(Symmetric(Œ£)).L
	@test gaussiandensity(y, L, logdensity = false) ‚âà exp(gaussiandensity(y, L))
	@test gaussiandensity(y, Œ£) ‚âà gaussiandensity(y, L)
	@test gaussiandensity(hcat(y, y), Œ£) ‚âà 2 * gaussiandensity(y, L)
end

@testset "vectotri: $dvc" for dvc ‚àà devices

	d = 4
	n = d*(d+1)√∑2

	v = arrayn(n) |> dvc
	L = vectotril(v)
	@test istril(L)
	@test all([cpu(v)[i] ‚àà cpu(L) for i ‚àà 1:n])
	@test containertype(L) == containertype(v)
	U = vectotriu(v)
	@test istriu(U)
	@test all([cpu(v)[i] ‚àà cpu(U) for i ‚àà 1:n])
	@test containertype(U) == containertype(v)

	# testing that it works for views of arrays
	V = arrayn(n, 2) |> dvc
	L = [vectotril(v) for v ‚àà eachcol(V)]
	@test all(istril.(L))
	@test all(containertype.(L) .== containertype(v))

	# strict variants
	n = d*(d-1)√∑2
	v = arrayn(n) |> dvc
	L = vectotril(v; strict = true)
	@test istril(L)
	@test all(L[diagind(L)] .== 0)
	@test all([cpu(v)[i] ‚àà cpu(L) for i ‚àà 1:n])
	@test containertype(L) == containertype(v)
	U = vectotriu(v; strict = true)
	@test istriu(U)
	@test all(U[diagind(U)] .== 0)
	@test all([cpu(v)[i] ‚àà cpu(U) for i ‚àà 1:n])
	@test containertype(U) == containertype(v)

end

# ---- Activation functions ----

function testbackprop(l, dvc, p::Integer, K::Integer, d::Integer)
	Z = arrayn(d, K) |> dvc
	Œ∏ = arrayn(p, K) |> dvc
	dense = Dense(d, p)
	Œ∏ÃÇ = Chain(dense, l) |> dvc
	Flux.gradient(() -> mae(Œ∏ÃÇ(Z), Œ∏), Flux.params(Œ∏ÃÇ)) # "implicit" style of Flux <= 0.14
	# Flux.gradient(Œ∏ÃÇ -> mae(Œ∏ÃÇ(Z), Œ∏), Œ∏ÃÇ)                 # "explicit" style of Flux >= 0.15
end

@testset "Activation functions: $dvc" for dvc ‚àà devices

	@testset "Compress" begin
		Compress(1, 2)
		p = 3
		K = 10
		a = Float32.([0.1, 4, 2])
		b = Float32.([0.9, 9, 3])
		l = Compress(a, b) |> dvc
		Œ∏ = arrayn(p, K)   |> dvc
		Œ∏ÃÇ = l(Œ∏)
		@test size(Œ∏ÃÇ) == (p, K)
		@test typeof(Œ∏ÃÇ) == typeof(Œ∏)
		@test all([all(a .< cpu(x) .< b) for x ‚àà eachcol(Œ∏ÃÇ)])
		testbackprop(l, dvc, p, K, 20)
	end

	@testset "CovarianceMatrix" begin

		d = 4
		K = 100
		p = d*(d+1)√∑2
		Œ∏ = arrayn(p, K) |> dvc

		l = CovarianceMatrix(d) |> dvc
		Œ∏ÃÇ = l(Œ∏)
		@test_throws Exception l(vcat(Œ∏, Œ∏))
		@test size(Œ∏ÃÇ) == (p, K)
		@test length(l(Œ∏[:, 1])) == p
		@test typeof(Œ∏ÃÇ) == typeof(Œ∏)

		Œ£ = [Symmetric(cpu(vectotril(x)), :L) for x ‚àà eachcol(Œ∏ÃÇ)]
		Œ£ = convert.(Matrix, Œ£);
		@test all(isposdef.(Œ£))

		L = l(Œ∏, true)
		L = [LowerTriangular(cpu(vectotril(x))) for x ‚àà eachcol(L)]
		@test all(Œ£ .‚âà L .* permutedims.(L))

		# testbackprop(l, dvc, p, K, d) # FIXME TODO broken
	end

	A = rand(5,4)
	@test rowwisenorm(A) == mapslices(norm, A; dims = 2)

	@testset "CorrelationMatrix" begin
		d = 4
		K = 100
		p = d*(d-1)√∑2
		Œ∏ = arrayn(p, K) |> dvc
		l = CorrelationMatrix(d) |> dvc
		Œ∏ÃÇ = l(Œ∏)
		@test_throws Exception l(vcat(Œ∏, Œ∏))
		@test size(Œ∏ÃÇ) == (p, K)
		@test length(l(Œ∏[:, 1])) == p
		@test typeof(Œ∏ÃÇ) == typeof(Œ∏)
		@test all(-1 .<= Œ∏ÃÇ .<= 1)

		R = map(eachcol(l(Œ∏))) do x
			R = Symmetric(cpu(vectotril(x; strict=true)), :L)
			R[diagind(R)] .= 1
			R
		end
		@test all(isposdef.(R))

		L = l(Œ∏, true)
		L = map(eachcol(L)) do x
			# Only the strict lower diagonal elements are returned
			L = LowerTriangular(cpu(vectotril(x, strict = true)))

			# Diagonal elements are determined under the constraint diag(L*L') = ùüè
			L[diagind(L)] .= sqrt.(1 .- rowwisenorm(L).^2)
			L
		end
		@test all(R .‚âà L .* permutedims.(L))

		# testbackprop(l, dvc, p, K, d) # FIXME TODO broken on the GPU
	end
end


# ---- Architectures ----

S = samplesize # Expert summary statistic used in DeepSet
parameter_names = ["Œº", "œÉ"]
struct Parameters <: ParameterConfigurations
	Œ∏
end
Œ© = product_distribution([Normal(0, 1), Uniform(0.1, 1.5)])
Œæ = (Œ© = Œ©, parameter_names = parameter_names)
K = 100
Parameters(K::Integer, Œæ) = Parameters(Float32.(rand(Œæ.Œ©, K)))
parameters = Parameters(K, Œæ)
show(devnull, parameters)
@test size(parameters) == (2, 100)
@test _extractŒ∏(parameters.Œ∏) == _extractŒ∏(parameters)
p = length(parameter_names)

#### Array data

n = 1  # univariate data
simulatearray(parameters::Parameters, m) = [Œ∏[1] .+ Œ∏[2] .* randn(Float32, n, m) for Œ∏ ‚àà eachcol(parameters.Œ∏)]
function simulatorwithcovariates(parameters::Parameters, m)
	Z = simulatearray(parameters, m)
	x = [rand(Float32, q‚Çì) for _ ‚àà eachindex(Z)]
	(Z, x)
end
function simulatorwithcovariates(parameters, m, J::Integer)
	v = [simulatorwithcovariates(parameters, m) for i ‚àà 1:J]
	z = vcat([v[i][1] for i ‚àà eachindex(v)]...)
	x = vcat([v[i][2] for i ‚àà eachindex(v)]...)
	(z, x)
end
function simulatornocovariates(parameters::Parameters, m)
	simulatearray(parameters, m)
end
function simulatornocovariates(parameters, m, J::Integer)
	v = [simulatornocovariates(parameters, m) for i ‚àà 1:J]
	vcat(v...)
end

# Traditional estimator that may be used for comparison
MLE(Z) = permutedims(hcat(mean.(Z), var.(Z)))
MLE(Z::Tuple) = MLE(Z[1])
MLE(Z, Œæ) = MLE(Z) # the MLE doesn't need Œæ, but we include it for testing

w  = 32 # width of each layer
q‚Çì = 2  # number of set-level covariates
m  = 10 # default sample size

@testset "DeepSet" begin
	@testset "$covar" for covar ‚àà ["no set-level covariates" "set-level covariates"]
		q = w
		if covar == "set-level covariates"
			q = q + q‚Çì
			simulator = simulatorwithcovariates
		else
			simulator = simulatornocovariates
		end
		œà = Chain(Dense(n, w), Dense(w, w), Flux.flatten)
		œï = Chain(Dense(q + 1, w), Dense(w, p))
		Œ∏ÃÇ = DeepSet(œà, œï, S = S)

		show(devnull, Œ∏ÃÇ)

		@testset "$dvc" for dvc ‚àà devices

			Œ∏ÃÇ = Œ∏ÃÇ |> dvc

			loss = Flux.Losses.mae |> dvc
			Œ∏    = array(p, K)     |> dvc

			Z = simulator(parameters, m) |> dvc
			@test size(Œ∏ÃÇ(Z), 1) == p
			@test size(Œ∏ÃÇ(Z), 2) == K
			@test isa(loss(Œ∏ÃÇ(Z), Œ∏), Number)

			# Single data set methods
			z = simulator(subsetparameters(parameters, 1), m) |> dvc
			if covar == "set-level covariates"
				z = (z[1][1], z[2][1])
			end
			Œ∏ÃÇ(z)

			# Test that we can update the neural-network parameters
			# "Implicit" style used by Flux <= 0.14.
			optimiser = Flux.Adam()
			Œ≥ = Flux.params(Œ∏ÃÇ)
			‚àá = Flux.gradient(() -> loss(Œ∏ÃÇ(Z), Œ∏), Œ≥)
			Flux.update!(optimiser, Œ≥, ‚àá)
			ls, ‚àá = Flux.withgradient(() -> loss(Œ∏ÃÇ(Z), Œ∏), Œ≥)
			Flux.update!(optimiser, Œ≥, ‚àá)
			# "Explicit" style required by Flux >= 0.15.
			# optimiser = Flux.setup(Flux.Adam(), Œ∏ÃÇ)
			# ‚àá = Flux.gradient(Œ∏ÃÇ -> loss(Œ∏ÃÇ(Z), Œ∏), Œ∏ÃÇ)
			# Flux.update!(optimiser, Œ∏ÃÇ, ‚àá[1])
			# ls, ‚àá = Flux.withgradient(Œ∏ÃÇ -> loss(Œ∏ÃÇ(Z), Œ∏), Œ∏ÃÇ)
			# Flux.update!(optimiser, Œ∏ÃÇ, ‚àá[1])

		    use_gpu = dvc == gpu
			@testset "train" begin

				# train: single estimator
				Œ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulator, m = m, epochs = 1, use_gpu = use_gpu, verbose = verbose, Œæ = Œæ)
				Œ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulator, m = m, epochs = 1, use_gpu = use_gpu, verbose = verbose, Œæ = Œæ, savepath = "testing-path")
				Œ∏ÃÇ = train(Œ∏ÃÇ, Parameters, simulator, m = m, epochs = 1, use_gpu = use_gpu, verbose = verbose, Œæ = Œæ, simulate_just_in_time = true)
				Œ∏ÃÇ = train(Œ∏ÃÇ, parameters, parameters, simulator, m = m, epochs = 1, use_gpu = use_gpu, verbose = verbose)
				Œ∏ÃÇ = train(Œ∏ÃÇ, parameters, parameters, simulator, m = m, epochs = 1, use_gpu = use_gpu, verbose = verbose, savepath = "testing-path")
				Œ∏ÃÇ = train(Œ∏ÃÇ, parameters, parameters, simulator, m = m, epochs = 4, epochs_per_Z_refresh = 2, use_gpu = use_gpu, verbose = verbose)
				Œ∏ÃÇ = train(Œ∏ÃÇ, parameters, parameters, simulator, m = m, epochs = 3, epochs_per_Z_refresh = 1, simulate_just_in_time = true, use_gpu = use_gpu, verbose = verbose)
				Z_train = simulator(parameters, 2m);
				Z_val   = simulator(parameters, m);
				train(Œ∏ÃÇ, parameters, parameters, Z_train, Z_val; epochs = 1, use_gpu = use_gpu, verbose = verbose, savepath = "testing-path")
				train(Œ∏ÃÇ, parameters, parameters, Z_train, Z_val; epochs = 1, use_gpu = use_gpu, verbose = verbose)

				# trainx: Multiple estimators
				trainx(Œ∏ÃÇ, Parameters, simulator, [1, 2, 5]; Œæ = Œæ, epochs = [3, 2, 1], use_gpu = use_gpu, verbose = verbose)
				trainx(Œ∏ÃÇ, parameters, parameters, simulator, [1, 2, 5]; epochs = [3, 2, 1], use_gpu = use_gpu, verbose = verbose)
				trainx(Œ∏ÃÇ, parameters, parameters, Z_train, Z_val, [1, 2, 5]; epochs = [3, 2, 1], use_gpu = use_gpu, verbose = verbose)
				Z_train = [simulator(parameters, m) for m ‚àà [1, 2, 5]];
				Z_val   = [simulator(parameters, m) for m ‚àà [1, 2, 5]];
				trainx(Œ∏ÃÇ, parameters, parameters, Z_train, Z_val; epochs = [3, 2, 1], use_gpu = use_gpu, verbose = verbose)
			end

			@testset "assess" begin

				# J == 1
				Z_test = simulator(parameters, m)
				assessment = assess([Œ∏ÃÇ], parameters, Z_test, use_gpu = use_gpu, verbose = verbose)
				assessment = assess(Œ∏ÃÇ, parameters, Z_test, use_gpu = use_gpu, verbose = verbose)
				@test typeof(assessment)         == Assessment
				@test typeof(assessment.df)      == DataFrame
				@test typeof(assessment.runtime) == DataFrame

				@test typeof(merge(assessment, assessment)) == Assessment
				risk(assessment)
				risk(assessment, loss = (x, y) -> (x - y)^2)
				risk(assessment; average_over_parameters = false)
				risk(assessment; average_over_sample_sizes = false)
				risk(assessment; average_over_parameters = false, average_over_sample_sizes = false)

				bias(assessment)
				bias(assessment; average_over_parameters = false)
				bias(assessment; average_over_sample_sizes = false)
				bias(assessment; average_over_parameters = false, average_over_sample_sizes = false)

				rmse(assessment)
				rmse(assessment; average_over_parameters = false)
				rmse(assessment; average_over_sample_sizes = false)
				rmse(assessment; average_over_parameters = false, average_over_sample_sizes = false)

				# J == 5 > 1
				Z_test = simulator(parameters, m, 5)
				assessment = assess([Œ∏ÃÇ], parameters, Z_test, use_gpu = use_gpu, verbose = verbose)
				@test typeof(assessment)         == Assessment
				@test typeof(assessment.df)      == DataFrame
				@test typeof(assessment.runtime) == DataFrame

				# Test that estimators needing invariant model information can be used:
				assess([MLE], parameters, Z_test, verbose = verbose)
				assess([MLE], parameters, Z_test, verbose = verbose, Œæ = Œæ)
			end


			@testset "bootstrap" begin

				# parametric bootstrap functions are designed for a single parameter configuration
				pars = Parameters(1, Œæ)
				m = 20
				B = 400
				ZÃÉ = simulator(pars, m, B)
				size(bootstrap(Œ∏ÃÇ, pars, ZÃÉ; use_gpu = use_gpu)) == (p, K)
				size(bootstrap(Œ∏ÃÇ, pars, simulator, m; use_gpu = use_gpu)) == (p, K)

				if covar == "no set-level covariates" # TODO non-parametric bootstrapping does not work for tuple data
					# non-parametric bootstrap is designed for a single parameter configuration and a single data set
					if typeof(ZÃÉ) <: Tuple
						Z = ([ZÃÉ[1][1]], [ZÃÉ[2][1]]) # NB not ideal that we need to still store these a vectors, given that the estimator doesn't require it
					else
						Z = ZÃÉ[1]
					end
					Z = Z |> dvc

					@test size(bootstrap(Œ∏ÃÇ, Z; use_gpu = use_gpu)) == (p, B)
					@test size(bootstrap(Œ∏ÃÇ, [Z]; use_gpu = use_gpu)) == (p, B)
					@test_throws Exception bootstrap(Œ∏ÃÇ, [Z, Z]; use_gpu = use_gpu)
					@test size(bootstrap(Œ∏ÃÇ, Z, use_gpu = use_gpu, blocks = rand(1:2, size(Z)[end]))) == (p, B)

					# interval
					Œ∏ÃÉ = bootstrap(Œ∏ÃÇ, pars, simulator, m; use_gpu = use_gpu)
					@test size(interval(Œ∏ÃÉ)) == (p, 2)
				end
			end
		end
	end
end


#### Graph data

#TODO need to test training
@testset "GNN" begin

	# Propagation module
    d = 1      # dimension of response variable
    nh = 32    # dimension of node feature vectors
    propagation = GNNChain(GraphConv(d => nh), GraphConv(nh => nh), GraphConv(nh => nh))

    # Readout module
    nt = 32   # dimension of the summary vector for each node
    no = 128  # dimension of the final summary vector for each graph
    readout = UniversalPool(
    	Chain(Dense(nh, nt), Dense(nt, nt)),
    	Chain(Dense(nt, nt), Dense(nt, no))
    	)
	show(devnull, readout)

	# Summary network
	œà = GNNSummary(propagation, readout)

    # Mapping module
    p = 3     # number of parameters in the statistical model
    w = 64    # width of layers used for the outer network œï
    œï = Chain(Dense(no, w, relu), Dense(w, w, relu), Dense(w, p))

    # Construct the estimator
    Œ∏ÃÇ = DeepSet(œà, œï)
	show(devnull, Œ∏ÃÇ)

    # Apply the estimator to:
    # 1. a single graph,
    # 2. a single graph with sub-graphs (corresponding to independent replicates), and
    # 3. a vector of graphs (corresponding to multiple spatial data sets, each
    #    possibly containing independent replicates).
    g‚ÇÅ = rand_graph(11, 30, ndata=rand(Float32, d, 11))
    g‚ÇÇ = rand_graph(13, 40, ndata=rand(Float32, d, 13))
    g‚ÇÉ = batch([g‚ÇÅ, g‚ÇÇ])
    Œ∏ÃÇ(g‚ÇÅ)
    Œ∏ÃÇ(g‚ÇÉ)
    Œ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])

	@test size(Œ∏ÃÇ(g‚ÇÅ)) == (p, 1)
	@test size(Œ∏ÃÇ(g‚ÇÉ)) == (p, 1)
	@test size(Œ∏ÃÇ([g‚ÇÅ, g‚ÇÇ, g‚ÇÉ])) == (p, 3)
end

# ---- Estimators ----

@testset "initialise_estimator" begin
	p = 2
	initialise_estimator(p, architecture = "DNN")
	initialise_estimator(p, architecture = "GNN")
	initialise_estimator(p, architecture = "CNN", kernel_size = [(10, 10), (5, 5), (3, 3)])
	initialise_estimator(p, "unstructured")
	initialise_estimator(p, "irregular_spatial")
	initialise_estimator(p, "gridded", kernel_size = [(10, 10), (5, 5), (3, 3)])

	@test typeof(initialise_estimator(p, architecture = "DNN", estimator_type = "interval")) <: IntervalEstimator
	@test typeof(initialise_estimator(p, architecture = "GNN", estimator_type = "interval")) <: IntervalEstimator
	@test typeof(initialise_estimator(p, architecture = "CNN", kernel_size = [(10, 10), (5, 5), (3, 3)], estimator_type = "interval")) <: IntervalEstimator

	@test_throws Exception initialise_estimator(0, architecture = "DNN")
	@test_throws Exception initialise_estimator(p, d = 0, architecture = "DNN")
	@test_throws Exception initialise_estimator(p, architecture = "CNN")
	@test_throws Exception initialise_estimator(p, architecture = "CNN", kernel_size = [(10, 10), (5, 5)])
end

@testset "PiecewiseEstimator" begin
	@test_throws Exception PiecewiseEstimator((MLE, MLE), (30, 50))
	@test_throws Exception PiecewiseEstimator((MLE, MLE, MLE), (50, 30))
	Œ∏ÃÇ_piecewise = PiecewiseEstimator((MLE, MLE), (30))
	show(devnull, Œ∏ÃÇ_piecewise)
	Z = [array(n, 1, 10, T = Float32), array(n, 1, 50, T = Float32)]
	Œ∏ÃÇ‚ÇÅ = hcat(MLE(Z[[1]]), MLE(Z[[2]]))
	Œ∏ÃÇ‚ÇÇ = Œ∏ÃÇ_piecewise(Z)
	@test Œ∏ÃÇ‚ÇÅ ‚âà Œ∏ÃÇ‚ÇÇ
end


@testset "IntervalEstimator" begin
	# Generate some toy data and a basic architecture
	d = 2  # bivariate data
	m = 64 # number of independent replicates
	Z = rand(Float32, d, m)
	parameter_names = ["œÅ", "œÉ", "œÑ"]
	p = length(parameter_names)
	w = 8  # width of each layer
	arch = initialise_estimator(p, architecture = "DNN", d = d, width = 8)

	# IntervalEstimator
	estimator = IntervalEstimator(arch)
	estimator = IntervalEstimator(arch, arch)
	Œ∏ÃÇ = estimator(Z)
	@test size(Œ∏ÃÇ) == (2p, 1)
	@test all(Œ∏ÃÇ[1:p] .< Œ∏ÃÇ[(p+1):end])
	ci = interval(estimator, Z)
	ci = interval(estimator, Z, parameter_names = parameter_names)
	@test size(ci) == (p, 2)

	# IntervalEstimator with a compact prior
	min_supp = [25, 0.5, -pi/2]
	max_supp = [500, 2.5, 0]
	g = Compress(min_supp, max_supp)
	estimator = IntervalEstimator(arch, g)
	estimator = IntervalEstimator(arch, arch, g)
	Œ∏ÃÇ = estimator(Z)
	@test size(Œ∏ÃÇ) == (2p, 1)
	@test all(Œ∏ÃÇ[1:p] .< Œ∏ÃÇ[(p+1):end])
	@test all(min_supp .< Œ∏ÃÇ[1:p] .< max_supp)
	@test all(min_supp .< Œ∏ÃÇ[p+1:end] .< max_supp)
	ci = interval(estimator, Z)
	ci = interval(estimator, Z, parameter_names = parameter_names)
	@test size(ci) == (p, 2)

	# assess()
	# assessment = assess(estimator, rand(p, 2), [Z, Z]) # not sure why this isn't working
	# coverage(assessment)
end

@testset "EM" begin

	# Set the prior distribution
	Œ© = (œÑ = Uniform(0.01, 0.3), œÅ = Uniform(0.01, 0.3))

	p = length(Œ©)    # number of parameters in the statistical model

	# Set the (gridded) spatial domain
	points = range(0.0, 1.0, 16)
	S = expandgrid(points, points)

	# Model information that is constant (and which will be passed into later functions)
	Œæ = (
		Œ© = Œ©,
		ŒΩ = 1.0, 	# fixed smoothness
		S = S,
		D = pairwise(Euclidean(), S, S, dims = 1),
		p = p
	)

	# Sampler from the prior
	struct GPParameters <: ParameterConfigurations
		Œ∏
		cholesky_factors
	end

	function GPParameters(K::Integer, Œæ)

		# Sample parameters from the prior
		Œ© = Œæ.Œ©
		œÑ = rand(Œ©.œÑ, K)
		œÅ = rand(Œ©.œÅ, K)

		# Compute Cholesky factors
		cholesky_factors = maternchols(Œæ.D, œÅ, Œæ.ŒΩ)

		# Concatenate into a matrix
		Œ∏ = permutedims(hcat(œÑ, œÅ))
		Œ∏ = Float32.(Œ∏)

		GPParameters(Œ∏, cholesky_factors)
	end

	function simulate(parameters, m::Integer)

		K = size(parameters, 2)
		œÑ = parameters.Œ∏[1, :]

		Z = map(1:K) do k
			L = parameters.cholesky_factors[:, :, k]
			z = simulategaussianprocess(L, m)
			z = z + œÑ[k] * randn(size(z)...)
			z = Float32.(z)
			z = reshape(z, 16, 16, 1, :)
			z
		end

		return Z
	end

	function simulateconditional(Z::M, Œ∏, Œæ; nsims::Integer = 1) where {M <: AbstractMatrix{Union{Missing, T}}} where T

		# Save the original dimensions
		dims = size(Z)

		# Convert to vector
		Z = vec(Z)

		# Compute the indices of the observed and missing data
		I‚ÇÅ = findall(z -> !ismissing(z), Z) # indices of observed data
		I‚ÇÇ = findall(z -> ismissing(z), Z)  # indices of missing data
		n‚ÇÅ = length(I‚ÇÅ)
		n‚ÇÇ = length(I‚ÇÇ)

		# Extract the observed data and drop Missing from the eltype of the container
		Z‚ÇÅ = Z[I‚ÇÅ]
		Z‚ÇÅ = [Z‚ÇÅ...]

		# Distance matrices needed for covariance matrices
		D   = Œæ.D # distance matrix for all locations in the grid
		D‚ÇÇ‚ÇÇ = D[I‚ÇÇ, I‚ÇÇ]
		D‚ÇÅ‚ÇÅ = D[I‚ÇÅ, I‚ÇÅ]
		D‚ÇÅ‚ÇÇ = D[I‚ÇÅ, I‚ÇÇ]

		# Extract the parameters from Œ∏
		œÑ = Œ∏[1]
		œÅ = Œ∏[2]

		# Compute covariance matrices
		ŒΩ = Œæ.ŒΩ
		Œ£‚ÇÇ‚ÇÇ = matern.(UpperTriangular(D‚ÇÇ‚ÇÇ), œÅ, ŒΩ); Œ£‚ÇÇ‚ÇÇ[diagind(Œ£‚ÇÇ‚ÇÇ)] .+= œÑ^2
		Œ£‚ÇÅ‚ÇÅ = matern.(UpperTriangular(D‚ÇÅ‚ÇÅ), œÅ, ŒΩ); Œ£‚ÇÅ‚ÇÅ[diagind(Œ£‚ÇÅ‚ÇÅ)] .+= œÑ^2
		Œ£‚ÇÅ‚ÇÇ = matern.(D‚ÇÅ‚ÇÇ, œÅ, ŒΩ)

		# Compute the Cholesky factor of Œ£‚ÇÅ‚ÇÅ and solve the lower triangular system
		L‚ÇÅ‚ÇÅ = cholesky(Symmetric(Œ£‚ÇÅ‚ÇÅ)).L
		x = L‚ÇÅ‚ÇÅ \ Œ£‚ÇÅ‚ÇÇ

		# Conditional covariance matrix, cov(Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏),  and its Cholesky factor
		Œ£ = Œ£‚ÇÇ‚ÇÇ - x'x
		L = cholesky(Symmetric(Œ£)).L

		# Conditonal mean, E(Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏)
		y = L‚ÇÅ‚ÇÅ \ Z‚ÇÅ
		Œº = x'y

		# Simulate from the distribution Z‚ÇÇ ‚à£ Z‚ÇÅ, Œ∏ ‚àº N(Œº, Œ£)
		z = randn(n‚ÇÇ, nsims)
		Z‚ÇÇ = Œº .+ L * z

		# Combine the observed and missing data to form the complete data
		Z = map(1:nsims) do l
			z = Vector{T}(undef, n‚ÇÅ + n‚ÇÇ)
			z[I‚ÇÅ] = Z‚ÇÅ
			z[I‚ÇÇ] = Z‚ÇÇ[:, l]
			z
		end
		Z = stackarrays(Z, merge = false)

		# Convert Z to an array with appropriate dimensions
		Z = reshape(Z, dims..., 1, nsims)

		return Z
	end

	Œ∏ = GPParameters(1, Œæ)
	Z = simulate(Œ∏, 1)[1][:, :]		# simulate a single gridded field
	Z = removedata(Z, 0.25)			# remove 25% of the data

	neuralMAPestimator = initialise_estimator(p, architecture = "CNN", kernel_size = [(10, 10), (5, 5), (3, 3)], activation_output = exp)
	neuralem = EM(simulateconditional, neuralMAPestimator)
	Œ∏‚ÇÄ = mean.([Œ©...]) 						# initial estimate, the prior mean
	H = 5
	Œ∏ÃÇ   = neuralem(Z, Œ∏‚ÇÄ, Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)
	Œ∏ÃÇ2  = neuralem([Z, Z], Œ∏‚ÇÄ, Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)

	@test size(Œ∏ÃÇ)  == (2, 1)
	@test size(Œ∏ÃÇ2) == (2, 2)

	## Test initial-value handling
	@test_throws Exception neuralem(Z)
	@test_throws Exception neuralem([Z, Z])
	neuralem = EM(simulateconditional, neuralMAPestimator, Œ∏‚ÇÄ)
	neuralem(Z, Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)
	neuralem([Z, Z], Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)

	## Test edge cases (no missingness and complete missingness)
	Z = simulate(Œ∏, 1)[1]		# simulate a single gridded field
	@test_warn "Data has been passed to the EM algorithm that contains no missing elements... the MAP estimator will be applied directly to the data" neuralem(Z, Œ∏‚ÇÄ, Œæ = Œæ, nsims = H)
	Z = Z[:, :]
	Z = removedata(Z, 1.0)
	@test_throws Exception neuralem(Z, Œ∏‚ÇÄ, Œæ = Œæ, nsims = H, use_Œæ_in_simulateconditional = true)
	@test_throws Exception neuralem(Z, Œ∏‚ÇÄ, nsims = H, use_Œæ_in_simulateconditional = true)
end

@testset "QuantileEstimatorContinuous" begin
	using NeuralEstimators, Flux, Distributions, InvertedIndices, Statistics

	# Simple model Z|Œ∏ ~ N(Œ∏, 1) with prior Œ∏ ~ N(0, 1)
	d = 1         # dimension of each independent replicate
	p = 1         # number of unknown parameters in the statistical model
	m = 30        # number of independent replicates in each data set
	prior(K) = randn32(p, K)
	simulateZ(Œ∏, m) = [Œº .+ randn32(d, m) for Œº ‚àà eachcol(Œ∏)]
	simulateœÑ(K)    = [rand32(1) for k in 1:K]
	simulate(Œ∏, m)  = simulateZ(Œ∏, m), simulateœÑ(size(Œ∏, 2))

	# Architecture: partially monotonic network to preclude quantile crossing
	w = 64  # width of each hidden layer
	q = 16  # number of learned summary statistics
	œà = Chain(
		Dense(d, w, relu),
		Dense(w, w, relu),
		Dense(w, q, relu)
		)
	œï = Chain(
		DensePositive(Dense(q + 1, w, relu); last_only = true),
		DensePositive(Dense(w, w, relu)),
		DensePositive(Dense(w, p))
		)
	deepset = DeepSet(œà, œï)

	# Initialise the estimator
	qÃÇ = QuantileEstimatorContinuous(deepset)

	# Train the estimator
	qÃÇ = train(qÃÇ, prior, simulate, m = m, epochs = 1, verbose = false)

	# Closed-form posterior for comparison
	function posterior(Z; Œº‚ÇÄ = 0, œÉ‚ÇÄ = 1, œÉ¬≤ = 1)

		# Parameters of posterior distribution
		ŒºÃÉ = (1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1 * (Œº‚ÇÄ/œÉ‚ÇÄ^2 + sum(Z)/œÉ¬≤)
		œÉÃÉ = sqrt((1/œÉ‚ÇÄ^2 + length(Z)/œÉ¬≤)^-1)

		# Posterior
		Normal(ŒºÃÉ, œÉÃÉ)
	end

	# Estimate the posterior 0.1-quantile for 1000 test data sets
	Œ∏ = prior(1000)
	Z = simulateZ(Œ∏, m)
	œÑ = 0.1f0
	qÃÇ(Z, œÑ)                        # neural quantiles
	quantile.(posterior.(Z), œÑ)'   # true quantiles

	# Estimate several quantiles for a single data set
	z = Z[1]
	œÑ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])
	reduce(vcat, qÃÇ.(Ref(z), œÑ))    # neural quantiles
	quantile.(posterior(z), œÑ)     # true quantiles

	# Check monotonicty
	@test all(qÃÇ(z, 0.1f0) .<= qÃÇ(z, 0.11f0) .<= qÃÇ(z, 0.9f0) .<= qÃÇ(z, 0.91f0))

	# ---- Full conditionals ----

	# Simple model Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with Œº ~ N(0, 1), œÉ ‚àº IG(3,1)
	d = 1         # dimension of each independent replicate
	p = 2         # number of unknown parameters in the statistical model
	m = 30        # number of independent replicates in each data set
	function sample(K)
		Œº = randn32(K)
		œÉ = rand(InverseGamma(3, 1), K)
		Œ∏ = hcat(Œº, œÉ)'
		Œ∏ = Float32.(Œ∏)
		return Œ∏
	end
	simulateZ(Œ∏, m) = Œ∏[1] .+ Œ∏[2] .* randn32(1, m)
	simulateZ(Œ∏::Matrix, m) = simulateZ.(eachcol(Œ∏), m)
	simulateœÑ(K)    = [rand32(1) for k in 1:K]
	simulate(Œ∏, m)  = simulateZ(Œ∏, m), simulateœÑ(size(Œ∏, 2))

	# Architecture: partially monotonic network to preclude quantile crossing
	w = 64  # width of each hidden layer
	q = 16  # number of learned summary statistics
	œà = Chain(
		Dense(d, w, relu),
		Dense(w, w, relu),
		Dense(w, q, relu)
		)
	œï = Chain(
		DensePositive(Dense(q + p, w, relu); last_only = true),
		DensePositive(Dense(w, w, relu)),
		DensePositive(Dense(w, 1))
		)
	deepset = DeepSet(œà, œï)

	# Initialise the estimator for the first parameter, targetting Œº‚à£Z,œÉ
	i = 1
	qÃÇ = QuantileEstimatorContinuous(deepset; i = i)

	# Train the estimator
	qÃÇ = train(qÃÇ, sample, simulate, m = m, epochs = 1, verbose = false)

	# Estimate quantiles of Œº‚à£Z,œÉ with œÉ = 0.5 and for 1000 data sets
	Œ∏ = prior(1000)
	Z = simulateZ(Œ∏, m)
	Œ∏‚Çã·µ¢ = 0.5f0    # for mulatiparameter scenarios, use Œ∏[Not(i), :] to determine the order that the conditioned parameters should be given
	œÑ = Float32.([0.1, 0.25, 0.5, 0.75, 0.9])
	qÃÇ(Z, Œ∏‚Çã·µ¢, œÑ)

	# Estimate quantiles for a single data set
	qÃÇ(Z[1], Œ∏‚Çã·µ¢, œÑ)
end

@testset "RatioEstimator" begin

	# Generate data from Z|Œº,œÉ ~ N(Œº, œÉ¬≤) with Œº, œÉ ~ U(0, 1)
	p = 2     # number of unknown parameters in the statistical model
	d = 1     # dimension of each independent replicate
	m = 100   # number of independent replicates

	prior(K) = rand32(p, K)
	simulate(Œ∏, m) = Œ∏[1] .+ Œ∏[2] .* randn32(d, m)
	simulate(Œ∏::AbstractMatrix, m) = simulate.(eachcol(Œ∏), m)

	# Architecture
	w = 64 # width of each hidden layer
	q = 2p # number of learned summary statistics
	œà = Chain(
		Dense(d, w, relu),
		Dense(w, w, relu),
		Dense(w, q, relu)
		)
	œï = Chain(
		Dense(q + p, w, relu),
		Dense(w, w, relu),
		Dense(w, 1)
		)
	deepset = DeepSet(œà, œï)

	# Initialise the estimator
	rÃÇ = RatioEstimator(deepset)

	# Train the estimator
	rÃÇ = train(rÃÇ, prior, simulate, m = m, epochs = 1, verbose = false)

	# Inference with "observed" data set
	Œ∏ = prior(1)
	z = simulate(Œ∏, m)[1]
	Œ∏‚ÇÄ = [0.5, 0.5]                           # initial estimate
	mlestimate(rÃÇ, z;  Œ∏‚ÇÄ = Œ∏‚ÇÄ)                # maximum-likelihood estimate
	mapestimate(rÃÇ, z; Œ∏‚ÇÄ = Œ∏‚ÇÄ)                # maximum-a-posteriori estimate
	Œ∏_grid = expandgrid(0:0.01:1, 0:0.01:1)'  # fine gridding of the parameter space
	Œ∏_grid = Float32.(Œ∏_grid)
	rÃÇ(z, Œ∏_grid)                              # likelihood-to-evidence ratios over grid
	sampleposterior(rÃÇ, z; Œ∏_grid = Œ∏_grid)    # posterior samples

	# Estimate ratio for many data sets and parameter vectors
	Œ∏ = prior(1000)
	Z = simulate(Œ∏, m)
	@test all(rÃÇ(Z, Œ∏) .>= 0)                          # likelihood-to-evidence ratios
	@test all(0 .<= rÃÇ(Z, Œ∏; classifier = true) .<= 1) # class probabilities
end