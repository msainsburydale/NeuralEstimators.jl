var documenterSearchIndex = {"docs":
[{"location":"workflow/simple/#Simple-example","page":"Simple example","title":"Simple example","text":"","category":"section"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Perhaps the simplest estimation task involves inferring Œº from N(Œº, œÉ) data, where œÉ = 1 is known.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The first step is to define an object that can be used to sample parameters and pass on information to the data simulation function. Here, we define the prior distribution, Œ©, of Œ∏, which we take to be a mean-zero Normal distribution standard deviation 0.5. In this simple example, only Œ© is needed, but we wrap it in a Tuple for consistency with the general workflow.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Œ© = Normal(0, 0.5)  \nŒæ = (Œ© = Œ©)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we define a sub-type of ParameterConfigurations with a field Œ∏, which stores parameters as a p √ó K matrix, where p is the dimension of Œ∏ (here, p = 1).","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"struct Parameters{T} <: ParameterConfigurations\n\tŒ∏::AbstractMatrix{T, 2}\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Storing parameter information in a struct is useful for storing intermediates objects needed for data simulation, such as Cholesky factors, and for implementing variants of on-the-fly and just-in-time simulation.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"We then define a Parameters constructor, which returns K draws from Œ©.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"function Parameters(Œæ, K::Integer)\n\tŒ∏ = rand(Œæ.Œ©, 1, K)\n\tParameters(Œ∏)\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we implicitly define the statistical model by providing a method simulate(), which defines data simulation conditional on Œ∏. The method must take two arguments; a Parameters object and m, the sample size. There is some flexibility in the permitted type of m (e.g., Integer, IntegerRange, etc.), but simulate() must return an AbstractVector of (multi-dimensional) AbstractArrays, where each array is associated with one parameter vector.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"import NeuralEstimators: simulate\nfunction simulate(params::Parameters, m::Integer)\n\tn = 1\n\tœÉ = 1\n\tŒ∏ = vec(params.Œ∏)\n\tZ = [rand(Normal(Œº, œÉ), n, 1, m) for Œº ‚àà Œ∏]\nend","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Note that the size of each array must be amenable to Flux neural networks; for instance, above we return a 3-dimensional array, even though the second dimension is redundant.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"We then choose an architecture for modelling œà(‚ãÖ) and œï(‚ãÖ) in the Deep Set framework, and initialise the neural estimator as a DeepSet object.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"p = 1\nw = 32\nq = 16\nœà = Chain(Dense(n, w, relu), Dense(w, q, relu))\nœï = Chain(Dense(q, w, relu), Dense(w, p), flatten)\nŒ∏ÃÇ = DeepSet(œà, œï)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Next, we train the neural estimator using train(). This optimisation is performed with respect to an arbitrary loss function (default absolute error loss). The argument m specifies the sample size used during training; the type of m should be consistent with the simulate() method defined above.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"Œ∏ÃÇ = train(Œ∏ÃÇ, Œ©, m = 10)","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The estimator Œ∏ÃÇ now approximates the Bayes estimator for Œ∏. It's usually a good idea to assess the performance of the estimator before putting it into practice. Since the performance of Œ∏ÃÇ for particular values of Œ∏ may be of particular interest, estimate() takes an instance of Parameters.","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"parameters = Parameters(Œ©, 500)      # test set with 500 parameters\nm  = [1, 10, 30]                     # sample sizes we wish to test\ndf = estimate(Œ∏ÃÇ, parameters, m = m)  ","category":"page"},{"location":"workflow/simple/","page":"Simple example","title":"Simple example","text":"The true parameters, estimates, and timings from this test run are returned in a convenient DataFrame, ready for visualisation.","category":"page"},{"location":"API/#Index","page":"Index","title":"Index","text":"","category":"section"},{"location":"API/","page":"Index","title":"Index","text":"","category":"page"},{"location":"API/utility/#Utility-functions","page":"Utility functions","title":"Utility functions","text":"","category":"section"},{"location":"API/utility/","page":"Utility functions","title":"Utility functions","text":"stack\n\nexpandgrid","category":"page"},{"location":"API/utility/#NeuralEstimators.stack","page":"Utility functions","title":"NeuralEstimators.stack","text":"stack(v::V; merge::Bool = true) where {V <: AbstractVector{A}} where {A <: AbstractArray{T, N}} where {T, N}\n\nStack a vector of arrays v along the last dimension of each array, optionally merging the final dimension of the stacked array.\n\nThe arrays must be of the same for the first N-1 dimensions. However, if merge = true, the size of the final dimension can vary between arrays.\n\nExamples\n\n# Vector containing arrays of the same size:\nZ = [rand(2, 3, m) for m ‚àà (1, 1)];\nstack(Z)\nstack(Z, merge = false)\n\n# Vector containing arrays with differing final dimension size:\nZ = [rand(2, 3, m) for m ‚àà (1, 2)];\nstack(Z)\n\n\n\n\n\n","category":"function"},{"location":"API/utility/#NeuralEstimators.expandgrid","page":"Utility functions","title":"NeuralEstimators.expandgrid","text":"expandgrid(xs, ys)\n\nSame as expand.grid() in R, but currently caters for two dimensions only.\n\n\n\n\n\n","category":"function"},{"location":"workflow/advanced/#Advanced-usage","page":"Advanced usage","title":"Advanced usage","text":"","category":"section"},{"location":"workflow/advanced/#Reusing-intermediate-objects-(e.g.,-Cholesky-factors)-for-multiple-parameter-configurations","page":"Advanced usage","title":"Reusing intermediate objects (e.g., Cholesky factors) for multiple parameter configurations","text":"","category":"section"},{"location":"workflow/advanced/#Balancing-time-and-memory-complexity","page":"Advanced usage","title":"Balancing time and memory complexity","text":"","category":"section"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"\"On-the-fly\" simulation refers to simulating new values for the parameters, Œ∏, and/or the data, Z, continuously during training. \"Just-in-time\" simulation refers to simulating small batches of parameters and data, training the neural estimator with this small batch, and then removing the batch from memory.   ","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"There are three variants of on-the-fly and just-in-time simulation, each with advantages and disadvantages.","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"Resampling Œ∏ and Z every epoch. This approach is the most theoretically justified and has the best memory complexity, since both Œ∏ and Z can be simulated just-in-time, but it has the worst time complexity.\nResampling Œ∏ every x epochs, resampling Z every epoch. This approach can reduce time complexity if generating Œ∏ (or intermediate objects thereof) dominates the computational cost. Further, memory complexity may be kept low since Z can still be simulated just-in-time.\nResampling Œ∏ every x epochs, resampling Z every y epochs, where x is a multiple of y. This approach minimises time complexity but has the largest memory complexity, since both Œ∏ and Z must be stored in full. Note that fixing Œ∏ and Z (i.e., setting y = ‚àû) often leads to worse out-of-sample performance and, hence, is generally discouraged.","category":"page"},{"location":"workflow/advanced/","page":"Advanced usage","title":"Advanced usage","text":"The keyword arguments epochs_per_Œ∏_refresh and epochs_per_Z_refresh in train() are intended to cater for these simulation variants.","category":"page"},{"location":"workflow/advanced/#Loading-previously-saved-estimators","page":"Advanced usage","title":"Loading previously saved estimators","text":"","category":"section"},{"location":"backgroundtheory/#Background-theory","page":"Background theory","title":"Background theory","text":"","category":"section"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Definition of an estimator:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"hatmathbftheta  mathcalS^m to Theta","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Permutation invariance:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"hatmathbftheta(mathbfZ_1 dots mathbfZ_m) = hatmathbftheta(mathbfZ_pi(1) dots mathbfZ_pi(m))","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Under some arbitrary loss function L(mathbftheta hatmathbftheta(mathcalZ)), the risk function:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"R(mathbftheta hatmathbftheta(cdot)) equiv int_mathcalS^m  L(mathbftheta hatmathbftheta(mathcalZ))p(mathcalZ mid mathbftheta) d mathcalZ","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Weighted average risk function:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"r_Omega(hatmathbftheta(cdot))\nequiv int_Theta R(mathbftheta hatmathbftheta(cdot)) dOmega(mathbftheta)  ","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Deep Set (Zaheer et al., 2017) representation of an estimator:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"beginaligned\nhatmathbftheta(mathcalZ) = mathbfphi(mathbfT(mathcalZ)) \nmathbfT(mathcalZ)  = sum_mathbfZ in mathcalZ mathbfpsi(mathbfZ)\nendaligned","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Optimisation task:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"hatmathbftheta_mathbfgamma^*(cdot)\nmathbfgamma^*\nequiv\nundersetmathbfgammamathrmargmin  r_Omega(hatmathbftheta_mathbfgamma(cdot))","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"Monte Carlo approximation of the weighted average risk:","category":"page"},{"location":"backgroundtheory/","page":"Background theory","title":"Background theory","text":"r_Omega(hatmathbftheta(cdot))\napprox\nfrac1K sum_k = 1^K frac1J sum_j = 1^J L(mathbftheta_k hatmathbftheta(mathcalZ_kj))  ","category":"page"},{"location":"workflow/overview/#Workflow-overview","page":"Overview","title":"Workflow overview","text":"","category":"section"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"A general overview for developing a neural estimator with NeuralEstimators.jl is as follows.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"Create an object Œæ that contains the information needed to sample the p-dimensional parameter vector Œ∏.\nDefine a type Parameters <: ParameterConfigurations used to store information needed for data simulation. Parameters must contain a field Œ∏, which stores K parameter vectors as a p √ó K matrix.\nDefine a Parameters constructor, Parameters(Œæ, K::Integer).  \nImplicitly define the statistical model by providing a method simulate(parameters::Parameters, m) which simulates m independent realisations from the statistical model.\nInitialise neural networks œà and œï. These will typically be Flux.jl networks.\nInitialise a DeepSet object, Œ∏ÃÇ = DeepSet(œà, œï).\nTrain Œ∏ÃÇ using train() under an arbitrary loss function.\nTest Œ∏ÃÇ using estimate().\nApply Œ∏ÃÇ to real-world data.","category":"page"},{"location":"workflow/overview/","page":"Overview","title":"Overview","text":"For clarity, see a Simple example and a More complicated example.","category":"page"},{"location":"API/simulation/#Data-simulation","page":"Data simulation","title":"Data simulation","text":"","category":"section"},{"location":"API/simulation/#Model-simulators","page":"Data simulation","title":"Model simulators","text":"","category":"section"},{"location":"API/simulation/","page":"Data simulation","title":"Data simulation","text":"simulategaussianprocess\n\nsimulateschlather\n\nsimulateconditionalextremes","category":"page"},{"location":"API/simulation/#NeuralEstimators.simulategaussianprocess","page":"Data simulation","title":"NeuralEstimators.simulategaussianprocess","text":"simulategaussianprocess(L::AbstractArray{T, 2}, œÉ¬≤::T, m::Integer)\n\nSimulates m realisations from a Gau(0, ùö∫ + œÉ¬≤ùêà) distribution, where ùö∫ ‚â° LL'.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateschlather","page":"Data simulation","title":"NeuralEstimators.simulateschlather","text":"simulateschlather(L::AbstractArray{T, 2}; C = 3.5)\nsimulateschlather(L::AbstractArray{T, 2}, m::Integer; C = 3.5)\n\nSimulates from Schlather's max-stable model. Based on Algorithm 1.2.2 of Dey DK, Yan J (2016). Extreme value modeling and risk analysis: methods and applications. CRC Press, Boca Raton, Florida.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.simulateconditionalextremes","page":"Data simulation","title":"NeuralEstimators.simulateconditionalextremes","text":"simulateconditionalextremes(L::AbstractArray{T, 2}, h, s‚ÇÄ_idx, u; <keyword args>)\nsimulateconditionalextremes(L::AbstractArray{T, 2}, h, s‚ÇÄ_idx, m::Integer; <keyword args>)\n\nSimulates from the spatial conditional extremes model.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#Intermediate-functions","page":"Data simulation","title":"Intermediate functions","text":"","category":"section"},{"location":"API/simulation/","page":"Data simulation","title":"Data simulation","text":"matern\n\nincgammalower\n\nf‚Çõ","category":"page"},{"location":"API/simulation/#NeuralEstimators.matern","page":"Data simulation","title":"NeuralEstimators.matern","text":"matern(h, œÅ, ŒΩ, œÉ¬≤ = 1)\n\nFor two points separated by h units, compute the Mat√©rn covariance function with range œÅ, smoothness ŒΩ, and marginal variance œÉ¬≤.\n\nWe use the parametrisation C(mathbfh) = sigma^2 frac2^1 - nuGamma(nu) left(fracmathbfhrhoright) K_nu left(fracmathbfhrhoright), where Gamma(cdot) is the gamma function, and K_nu(cdot) is the modified Bessel function of the second kind of order nu. This parameterisation is the same as used by the R package fields, but differs to the parametrisation given by Wikipedia.\n\nNote that the Julia functions for Gamma(cdot) and K_nu(cdot), respectively gamma() and besselk(), do not work on the GPU and, hence, nor does matern().\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.incgammalower","page":"Data simulation","title":"NeuralEstimators.incgammalower","text":"incgammalower(a, x)\n\nFor positive a and x, computes the lower incomplete gamma function, gamma(a x) = int_0^x t^a-1e^-tdt.\n\n\n\n\n\n","category":"function"},{"location":"API/simulation/#NeuralEstimators.f‚Çõ","page":"Data simulation","title":"NeuralEstimators.f‚Çõ","text":"f‚Çõ(x, Œº, œÑ, Œ¥)\nF‚Çõ(q, Œº, œÑ, Œ¥)\nF‚Çõ‚Åª¬π(p, Œº, œÑ, Œ¥)\n\nThe density, distribution, and quantile functions Subbotin (delta-Laplace) distribution with location parameter Œº, scale parameter œÑ, and shape parameter Œ¥:\n\n f_S(y mu tau delta) = fracdelta2tau Gamma(1delta) expleft(-leftfracy - mutauright^deltaright)\n F_S(y mu tau delta) = frac12 + textrmsign(y - mu) frac12 Gamma(1delta) gammaleft(1delta leftfracy - mutauright^deltaright)\n F_S^-1(p mu tau delta) = textsign(p - 05)G^-1left(2p - 05 frac1delta frac1(ktau)^deltaright)^1delta + mu\n\nwith gamma(cdot) and G^-1(cdot) the unnormalised incomplete lower gamma function and quantile function of the Gamma distribution, respectively.\n\nExamples\n\np = [0.025, 0.05, 0.5, 0.9, 0.95, 0.975]\n\n# Standard Gaussian:\nŒº = 0.0; œÑ = sqrt(2); Œ¥ = 2.0\nF‚Çõ‚Åª¬π.(p, Œº, œÑ, Œ¥)\n\n# Standard Laplace:\nŒº = 0.0; œÑ = 1.0; Œ¥ = 1.0\nF‚Çõ‚Åª¬π.(p, Œº, œÑ, Œ¥)\n\n\n\n\n\n","category":"function"},{"location":"#NeuralEstimators.jl-documentation","page":"Home","title":"NeuralEstimators.jl documentation","text":"","category":"section"},{"location":"","page":"Home","title":"Home","text":"Landing page and brief description of neural estimators as a recent likelihood-inference approach, and an alternative to ABC.","category":"page"},{"location":"API/core/#Core-functions","page":"Core functions","title":"Core functions","text":"","category":"section"},{"location":"API/core/#Deep-Set-representation","page":"Core functions","title":"Deep Set representation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"DeepSet\n\nDeepSet(œà, œï; aggregation::String)","category":"page"},{"location":"API/core/#NeuralEstimators.DeepSet","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï, agg)\n\nImplementation of the Deep Set framework, where œà and œï are neural networks (e.g., Flux networks) and agg is a symmetric function that pools data over the last dimension (the replicates/batch dimension) of an array.\n\nDeepSet objects are applied to AbstractVectors of AbstractArrays, where each array is associated with one parameter vector.\n\nExamples\n\nn = 10 # observations in each realisation\np = 5  # number of parameters in the statistical model \nw = 32 # width of each layer\nœà = Chain(Dense(n, w, relu), Dense(w, w, relu));\nœï = Chain(Dense(w, w, relu), Dense(w, p));\nagg(X) = sum(X, dims = ndims(X))\nŒ∏ÃÇ  = DeepSet(œà, œï, agg)\n\n# A single set of m=3 realisations:\nZ = [rand(n, 1, 3)];\nŒ∏ÃÇ (Z)\n\n# Two sets each containing m=3 realisations:\nZ = [rand(n, 1, m) for m ‚àà (3, 3)];\nŒ∏ÃÇ (Z)\n\n# Two sets respectivaly containing m=3 and m=4 realisations:\nZ = [rand(n, 1, m) for m ‚àà (3, 4)];\nŒ∏ÃÇ (Z)\n\n\n\n\n\n","category":"type"},{"location":"API/core/#NeuralEstimators.DeepSet-Tuple{Any, Any}","page":"Core functions","title":"NeuralEstimators.DeepSet","text":"DeepSet(œà, œï; aggregation::String = \"mean\")\n\nConvenient constructor for a DeepSet object with agg equal to the \"mean\", \"sum\", or \"log-sum-exp\" function.\n\n\n\n\n\n","category":"method"},{"location":"API/core/#Simulation","page":"Core functions","title":"Simulation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"simulate","category":"page"},{"location":"API/core/#NeuralEstimators.simulate","page":"Core functions","title":"NeuralEstimators.simulate","text":"simulate(parameters::P, m::Integer, num_rep::Integer) where {P <: ParameterConfigurations}\n\nGeneric method that simulates m independent replicates for each parameter configuration (by internally calling simulate(parameters, m)), repeated a total of num_rep times.\n\nSee also Data simulation.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Training","page":"Core functions","title":"Training","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"There are two training methods. For both methods, the validation parameters and validation data are held fixed so that the validation risk is interpretable. There are a number of practical considerations to keep in mind: In particular, see Balancing time and memory complexity.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"train","category":"page"},{"location":"API/core/#NeuralEstimators.train","page":"Core functions","title":"NeuralEstimators.train","text":"train(Œ∏ÃÇ, Œæ, P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator Œ∏ÃÇ by providing the objects Œæ needed for the constructor P to automatically sample the set of training and validation parameters, which may be refreshed periodically throughout training via the keyword argument epochs_per_Œ∏_refresh.\n\nKeyword arguments common to both train methods:\n\nm: sample sizes (either an Integer or a collection of Integers).\nbatchsize::Integer = 32\nepochs::Integer = 100: the maximum number of epochs used during training.\nepochs_per_Z_refresh::Integer = 1: how often to refresh the training data.\nloss = mae: the loss function, which should return an average loss when applied to multiple replicates.\noptimiser = ADAM(1e-4)\nsavepath::String = \"runs/\": path to save the trained Œ∏ÃÇ and other information.\nstopping_epochs::Integer = 10: halt training if the risk doesn't improve in stopping_epochs epochs.\nuse_gpu::Bool = true\n\nSimulator keyword arguments only:\n\nconfigs_per_epoch::Integer = 10_000: how many parameters constitute a single epoch.\nepochs_per_Œ∏_refresh::Integer = 1: how often to refresh the training parameters; this must be a multiple of epochs_per_Z_refresh.\n\n\n\n\n\ntrain(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P; <keyword args>) where {P <: ParameterConfigurations}\n\nTrain the neural estimator Œ∏ÃÇ by providing the training and validation sets explicitly as Œ∏_train and Œ∏_val, which are both held fixed during training.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Estimation","page":"Core functions","title":"Estimation","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"estimate","category":"page"},{"location":"API/core/#NeuralEstimators.estimate","page":"Core functions","title":"NeuralEstimators.estimate","text":"estimate(estimators, parameters::P, m; <keyword args>) where {P <: ParameterConfigurations}\n\nUsing a collection of estimators, compute estimates from data simulated from a set of parameters.\n\nestimate() requires the user to have defined a method simulate(parameters, m::Integer).\n\nKeyword arguments\n\nm::Vector{Integer} where I <: Integer: sample sizes to estimate from.\nestimator_names::Vector{String}: estimator names used when combining estimates into a DataFrame (e.g., [\"NeuralEstimator\", \"BayesEstimator\", \"MLE\"]), with sensible default values provided.\nparameter_names::Vector{String}: parameter names used when combining estimates into a DataFrame (e.g., [\"Œº\", \"œÉ\"]), with sensible default values provided.\nnum_rep::Integer = 1: the number of times to replicate each parameter in parameters to reduce the effect of sample variability when assessing the estimators.\nuse_gpu = true: should be a Bool or a collection of Bool objects with length equal to the number of estimators.\n\n\n\n\n\n","category":"function"},{"location":"API/core/#Bootstrapping","page":"Core functions","title":"Bootstrapping","text":"","category":"section"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"Note that all bootstrapping functions are currently implemented for a single parameter configuration only.","category":"page"},{"location":"API/core/","page":"Core functions","title":"Core functions","text":"parametricbootstrap\n\nnonparametricbootstrap","category":"page"},{"location":"API/core/#NeuralEstimators.parametricbootstrap","page":"Core functions","title":"NeuralEstimators.parametricbootstrap","text":"parametricbootstrap(Œ∏ÃÇ, parameters::P, m::Integer; B::Integer = 100, use_gpu::Bool = true) where {P <: ParameterConfigurations}\n\nReturns B parameteric bootstrap samples of an estimator Œ∏ÃÇ as a p √ó B matrix, where p is the number of parameters in the statistical model, based on simulated data sets of size m.\n\nThis function requires a method simulate(parameters::P, m::Integer).\n\n\n\n\n\n","category":"function"},{"location":"API/core/#NeuralEstimators.nonparametricbootstrap","page":"Core functions","title":"NeuralEstimators.nonparametricbootstrap","text":"nonparametricbootstrap(Œ∏ÃÇ, Z; B::Integer = 100, use_gpu::Bool = true)\nnonparametricbootstrap(Œ∏ÃÇ, Z, blocks; B::Integer = 100, use_gpu::Bool = true)\n\nReturns B non-parametric bootstrap samples of an estimator Œ∏ÃÇ as a p √ó B matrix, where p is the number of parameters in the statistical model.\n\nThe argument blocks caters for block bootstrapping, and should be an integer vector specifying the block for each replicate. For example, if we have 5 replicates with the first two replicates corresponding to block 1 and the remaining replicates corresponding to block 2, then blocks should be [1, 1, 2, 2, 2]. The resampling algorithm tries to produce resampled data sets of a similar size to the original data, but this can only be achieved exactly if the blocks are the same length.\n\n\n\n\n\n","category":"function"},{"location":"workflow/complex/#More-complicated-example","page":"More complicated example","title":"More complicated example","text":"","category":"section"},{"location":"workflow/complex/","page":"More complicated example","title":"More complicated example","text":"Show example requiring information to be passed around.","category":"page"}]
}
