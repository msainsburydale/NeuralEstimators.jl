<!DOCTYPE html>
<html lang="en"><head><meta charset="UTF-8"/><meta name="viewport" content="width=device-width, initial-scale=1.0"/><title>Core functions ¬∑ NeuralEstimators.jl</title><link href="https://fonts.googleapis.com/css?family=Lato|Roboto+Mono" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/fontawesome.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/solid.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.15.0/css/brands.min.css" rel="stylesheet" type="text/css"/><link href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.11.1/katex.min.css" rel="stylesheet" type="text/css"/><script>documenterBaseURL="../.."</script><script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" data-main="../../assets/documenter.js"></script><script src="../../siteinfo.js"></script><script src="../../../versions.js"></script><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-dark.css" data-theme-name="documenter-dark" data-theme-primary-dark/><link class="docs-theme-link" rel="stylesheet" type="text/css" href="../../assets/themes/documenter-light.css" data-theme-name="documenter-light" data-theme-primary/><script src="../../assets/themeswap.js"></script></head><body><div id="documenter"><nav class="docs-sidebar"><a class="docs-logo" href="../../"><img src="../../assets/logo.png" alt="NeuralEstimators.jl logo"/></a><div class="docs-package-name"><span class="docs-autofit">NeuralEstimators.jl</span></div><form class="docs-search" action="../../search/"><input class="docs-search-query" id="documenter-search-query" name="q" type="text" placeholder="Search docs"/></form><ul class="docs-menu"><li><a class="tocitem" href="../../">Home</a></li><li><a class="tocitem" href="../../framework/">Framework</a></li><li><span class="tocitem">Workflow</span><ul><li><a class="tocitem" href="../../workflow/examples/">Examples</a></li><li><a class="tocitem" href="../../workflow/advancedusage/">Advanced usage</a></li></ul></li><li><span class="tocitem">API</span><ul><li class="is-active"><a class="tocitem" href>Core functions</a><ul class="internal"><li><a class="tocitem" href="#Storing-parameters"><span>Storing parameters</span></a></li><li><a class="tocitem" href="#Simulating-data"><span>Simulating data</span></a></li><li><a class="tocitem" href="#Representations-for-neural-estimators"><span>Representations for neural estimators</span></a></li><li><a class="tocitem" href="#Training"><span>Training</span></a></li><li><a class="tocitem" href="#Assessing-a-neural-estimator"><span>Assessing a neural estimator</span></a></li><li><a class="tocitem" href="#Bootstrapping"><span>Bootstrapping</span></a></li></ul></li><li><a class="tocitem" href="../simulation/">Model-specific functions</a></li><li><a class="tocitem" href="../utility/">Utility functions</a></li><li><a class="tocitem" href="../">Index</a></li></ul></li></ul><div class="docs-version-selector field has-addons"><div class="control"><span class="docs-label button is-static is-size-7">Version</span></div><div class="docs-selector control is-expanded"><div class="select is-fullwidth is-size-7"><select id="documenter-version-selector"></select></div></div></div></nav><div class="docs-main"><header class="docs-navbar"><nav class="breadcrumb"><ul class="is-hidden-mobile"><li><a class="is-disabled">API</a></li><li class="is-active"><a href>Core functions</a></li></ul><ul class="is-hidden-tablet"><li class="is-active"><a href>Core functions</a></li></ul></nav><div class="docs-right"><a class="docs-edit-link" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/master/docs/src/API/core.md" title="Edit on GitHub"><span class="docs-icon fab">ÔÇõ</span><span class="docs-label is-hidden-touch">Edit on GitHub</span></a><a class="docs-settings-button fas fa-cog" id="documenter-settings-button" href="#" title="Settings"></a><a class="docs-sidebar-button fa fa-bars is-hidden-desktop" id="documenter-sidebar-button" href="#"></a></div></header><article class="content" id="documenter-page"><h1 id="Core-functions"><a class="docs-heading-anchor" href="#Core-functions">Core functions</a><a id="Core-functions-1"></a><a class="docs-heading-anchor-permalink" href="#Core-functions" title="Permalink"></a></h1><p>This page documents the functions that are central to the workflow of <code>NeuralEstimators</code>. Its organisation reflects the order in which these functions appear in a standard implementation; that is, from storing parameters sampled from the prior distribution, to uncertainty quantification of the final estimates via bootstrapping.</p><h2 id="Storing-parameters"><a class="docs-heading-anchor" href="#Storing-parameters">Storing parameters</a><a id="Storing-parameters-1"></a><a class="docs-heading-anchor-permalink" href="#Storing-parameters" title="Permalink"></a></h2><p>Parameters sampled from the prior distribution <span>$\Omega(\cdot)$</span> may be stored i) as a <span>$p \times K$</span> matrix, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution, or ii) in a user-defined subtype of the abstract type <a href="#NeuralEstimators.ParameterConfigurations"><code>ParameterConfigurations</code></a>. See <a href="../../workflow/advancedusage/#Storing-expensive-intermediate-objects-for-data-simulation">Storing expensive intermediate objects for data simulation</a> for further discussion.   </p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.ParameterConfigurations" href="#NeuralEstimators.ParameterConfigurations"><code>NeuralEstimators.ParameterConfigurations</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">ParameterConfigurations</code></pre><p>An abstract supertype for user-defined types that store parameters and any intermediate objects needed for data simulation with <a href="#NeuralEstimators.simulate"><code>simulate</code></a>.</p><p>The user-defined type must have a field <code>Œ∏</code> that stores the <span>$p$</span> √ó <span>$K$</span> matrix of parameters, where <span>$p$</span> is the number of parameters in the model and <span>$K$</span> is the number of parameter vectors sampled from the prior distribution. There are no other restrictions.</p><p><strong>Examples</strong></p><pre><code class="language-none">struct P &lt;: ParameterConfigurations
	Œ∏
	# ...
end</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Parameters.jl#L1-L20">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.subsetparameters" href="#NeuralEstimators.subsetparameters"><code>NeuralEstimators.subsetparameters</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">subsetparameters(parameters::M, indices) where {M &lt;: AbstractMatrix}
subsetparameters(parameters::P, indices) where {P &lt;: ParameterConfigurations}</code></pre><p>Subset <code>parameters</code> using a collection of <code>indices</code>.</p><p>Arrays in <code>parameters::P</code> with last dimension equal in size to the number of parameter configurations, K, are also subsetted (over their last dimension) using <code>indices</code>. All other fields are left unchanged. To modify this default behaviour, redefine <code>subsetparameters</code> after running <code>import NeuralEstimators: subsetparameters</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Parameters.jl#L36-L47">source</a></section></article><h2 id="Simulating-data"><a class="docs-heading-anchor" href="#Simulating-data">Simulating data</a><a id="Simulating-data-1"></a><a class="docs-heading-anchor-permalink" href="#Simulating-data" title="Permalink"></a></h2><p><code>NeuralEstimators</code> facilitates neural estimation for arbitrary statistical models by having the user implicitly define their model either by providing simulated data, or by defining a function for data simulation. If the latter option is chosen, the user must provide a method <code>simulate(parameters, m)</code>, which returns simulated data from a set of <code>parameters</code>, with <code>m</code> the sample size of these simulated data.</p><p>Irrespective of their source, the simulated data must be stored as a subtype of <code>AbstractVector{AbstractArray}</code>, where each array stores <code>m</code> independent replicates simulated from one parameter vector in <code>parameters</code>, and these replicates must stored in the final dimension of each array.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate" href="#NeuralEstimators.simulate"><code>NeuralEstimators.simulate</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><p>Generic function that the user may provide methods for in order to implicitly define their statistical model.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Simulation.jl#L3-L6">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.simulate-Tuple{Any, Integer, Integer}" href="#NeuralEstimators.simulate-Tuple{Any, Integer, Integer}"><code>NeuralEstimators.simulate</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">simulate(parameters, m::Integer, J::Integer)</code></pre><p>Simulates <code>J</code> sets of <code>m</code> independent replicates for each parameter vector in <code>parameters</code> by calling <code>simulate(parameters, m)</code> a total of <code>J</code> times.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Simulation.jl#L10-L15">source</a></section></article><h2 id="Representations-for-neural-estimators"><a class="docs-heading-anchor" href="#Representations-for-neural-estimators">Representations for neural estimators</a><a id="Representations-for-neural-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Representations-for-neural-estimators" title="Permalink"></a></h2><p>Although the user is free to construct their neural estimator however they see fit, <code>NeuralEstimators</code> provides several useful representations described below. Note that if the user wishes to use an alternative representation, for compatibility with <code>NeuralEstimators</code>, simply ensure that the estimator processes data stored as subtypes of <code>AbstractVector{AbstractArray}</code>, as discussed in <a href="#NeuralEstimators.DeepSet"><code>DeepSet</code></a> (see also its source code).</p><h3 id="Deep-Set"><a class="docs-heading-anchor" href="#Deep-Set">Deep Set</a><a id="Deep-Set-1"></a><a class="docs-heading-anchor-permalink" href="#Deep-Set" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.DeepSet" href="#NeuralEstimators.DeepSet"><code>NeuralEstimators.DeepSet</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">DeepSet(œà, œï, a)
DeepSet(œà, œï; a::String = &quot;mean&quot;)</code></pre><p>A Deep Set neural estimator,</p><p class="math-container">\[Œ∏ÃÇ(ùêô) ‚â° œï(a(\{ùêô·µ¢ : i = 1, ‚Ä¶, m\})),\]</p><p>where ùêô ‚â° (ùêô‚ÇÅ&#39;, ‚Ä¶, ùêô‚Çò&#39;)&#39; are independent and identically distributed (iid) realisations from the model under a single parameter vector ùõâ, <code>œà</code> and <code>œï</code> are neural networks, and <code>a</code> is a permutation-invariant aggregation function. Note that <code>œà</code> and <code>œï</code> depend on trainable parameters, but we omit this dependence for notational convenience.</p><p>Although the above defintion of a neural estimator is with respect to a single data set ùêô, <code>DeepSet</code> estimators instead act on sets of data sets, stored as <code>Vector</code>s of <code>Array</code>s, where each array corresponds to one set of iid realisations from the model. The last dimension of each array stores the realisations; for example, if ùêô is a 3-dimensional array, then ùêô[:, :, 1] contains the first realisation, ùêô[:, :, 2] contains the second realisation, and so on.</p><p>The neural networks <code>œà</code> and <code>œï</code> and typically <code>Flux</code> neural networks. The function <code>a</code> must act on an <code>Array</code> and, since it aggregates the iid realisations, it must aggregate over the last dimension of the array.</p><p>There are two <code>DeepSet</code> constructors. The first constructor treats <code>a</code> as a keyword argument of type <code>String</code>, which can take values <code>&quot;mean&quot;</code> (default), <code>&quot;sum&quot;</code>, or <code>&quot;logsumexp&quot;</code> function. The second constructor treats <code>a</code> as a positional argument of type <code>Function</code>, and this constructor allows the user to provide a custom aggregation function.</p><p><strong>Examples</strong></p><pre><code class="language-none">n = 10 # observations in each realisation
p = 5  # number of parameters in the statistical model
w = 32 # width of each layer
œà = Chain(Dense(n, w, relu), Dense(w, w, relu));
œï = Chain(Dense(w, w, relu), Dense(w, p));
Œ∏ÃÇ = DeepSet(œà, œï)

# Apply the estimator to a single set of m=3 realisations:
Z = [rand(n, 1, 3)];
Œ∏ÃÇ(Z)

# Apply the estimator to two sets each containing m=3 realisations:
Z = [rand(n, 1, m) for m ‚àà (3, 3)];
Œ∏ÃÇ(Z)

# Apply the estimator to two sets containing m=3 and m=4 realisations, respectively:
Z = [rand(n, 1, m) for m ‚àà (3, 4)];
Œ∏ÃÇ(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/DeepSet.jl#L27-L79">source</a></section></article><h3 id="Piecewise-estimators"><a class="docs-heading-anchor" href="#Piecewise-estimators">Piecewise estimators</a><a id="Piecewise-estimators-1"></a><a class="docs-heading-anchor-permalink" href="#Piecewise-estimators" title="Permalink"></a></h3><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.PiecewiseEstimator" href="#NeuralEstimators.PiecewiseEstimator"><code>NeuralEstimators.PiecewiseEstimator</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">PiecewiseEstimator(estimators, mchange)</code></pre><p>Creates a piecewise estimator from a collection of <code>estimators</code>, based on the collection of sample-size changepoints, <code>mchange</code>, which should contain one element fewer than the number of <code>estimators</code>.</p><p><strong>Examples</strong></p><p>Suppose that we&#39;ve trained <code>Œ∏ÃÇ‚ÇÅ</code> for small sample sizes (e.g., m ‚â§ 30) and <code>Œ∏ÃÇ‚ÇÇ</code> for moderate-to-large sample sizes (e.g., m &gt; 30). Then we can construct a piecewise estimator with a sample-size changepoint of 30, which dispatches <code>Œ∏ÃÇ‚ÇÅ</code> if m ‚â§ 30 and <code>Œ∏ÃÇ‚ÇÇ</code> if m &gt; 30:</p><pre><code class="language-none">n = 2
p = 3
w = 8

œà‚ÇÅ = Chain(Dense(n, w, relu), Dense(w, w, relu));
œï‚ÇÅ = Chain(Dense(w, w, relu), Dense(w, p));
Œ∏ÃÇ‚ÇÅ = DeepSet(œà‚ÇÅ, œï‚ÇÅ)

œà‚ÇÇ = Chain(Dense(n, w, relu), Dense(w, w, relu), Dense(w, w, relu));
œï‚ÇÇ = Chain(Dense(w, w, relu), Dense(w, w, relu), Dense(w, p));
Œ∏ÃÇ‚ÇÇ = DeepSet(œà‚ÇÇ, œï‚ÇÇ)

Œ∏ÃÇ = PiecewiseEstimator([Œ∏ÃÇ‚ÇÅ, Œ∏ÃÇ‚ÇÇ], [30])
Z = [rand(n, 1, m) for m ‚àà (10, 50)]
Œ∏ÃÇ(Z)</code></pre></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/PiecewiseEstimator.jl#L14-L44">source</a></section></article><h2 id="Training"><a class="docs-heading-anchor" href="#Training">Training</a><a id="Training-1"></a><a class="docs-heading-anchor-permalink" href="#Training" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train" href="#NeuralEstimators.train"><code>NeuralEstimators.train</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><p>Generic function for training a neural estimator.</p><p>The methods are designed to cater for different forms of &quot;on-the-fly simulation&quot; (see the online documentation). In all methods, the validation data are held fixed so that the validation risk function, which is used to monitor the performance of the estimator during training, is not subject to noise.</p><p>Note that <code>train</code> is a mutating function, but the suffix <code>!</code> is omitted to avoid clashes with the <code>Flux</code> function, <code>train!</code>.</p><p><strong>Keyword arguments</strong></p><p>Arguments common to all methods:</p><ul><li><code>loss = mae</code>: the loss function, which should return the average loss when applied to multiple replicates.</li><li><code>epochs::Integer = 100</code></li><li><code>batchsize::Integer = 32</code></li><li><code>optimiser = ADAM(1e-4)</code></li><li><code>savepath::String = &quot;&quot;</code>: path to save the trained <code>Œ∏ÃÇ</code> and other information; if savepath is an empty string (default), nothing is saved.</li><li><code>stopping_epochs::Integer = 10</code>: cease training if the risk doesn&#39;t improve in <code>stopping_epochs</code> epochs.</li><li><code>use_gpu::Bool = true</code></li><li><code>verbose::Bool = true</code></li></ul><p>Arguments common to <code>train(Œ∏ÃÇ, P)</code> and <code>train(Œ∏ÃÇ, Œ∏_train, Œ∏_val)</code>:</p><ul><li><code>m</code>: sample sizes (either an <code>Integer</code> or a collection of <code>Integers</code>).</li><li><code>epochs_per_Z_refresh::Integer = 1</code>: how often to refresh the training data.</li><li><code>simulate_just_in_time::Bool = false</code>: should we simulate the data &quot;just-in-time&quot;?</li></ul><p>Arguments unique to <code>train(Œ∏ÃÇ, P)</code>:</p><ul><li><code>K::Integer = 10_000</code>: number of parameter vectors in the training set; the size of the validation set is <code>K √∑ 5</code>.</li><li><code>epochs_per_Œ∏_refresh::Integer = 1</code>: how often to refresh the training parameters; this must be a multiple of <code>epochs_per_Z_refresh</code>.</li><li><code>Œæ = nothing</code>: invariant model information; if <code>Œæ</code> is provided, the constructor <code>P</code> is called as <code>P(K, Œæ)</code>.</li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Train.jl#L1-L33">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Tuple{Any, Any}" href="#NeuralEstimators.train-Tuple{Any, Any}"><code>NeuralEstimators.train</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">train(Œ∏ÃÇ, P; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>Œ∏ÃÇ</code> by providing a constructor, <code>P</code>, where <code>P</code> is a subtype of <code>AbstractMatrix</code> or <code>ParameterConfigurations</code>, to automatically sample the sets of training and validation parameters.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Train.jl#L37-L43">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P&lt;:Union{ParameterConfigurations, AbstractMatrix}" href="#NeuralEstimators.train-Union{Tuple{P}, Tuple{Any, P, P}} where P&lt;:Union{ParameterConfigurations, AbstractMatrix}"><code>NeuralEstimators.train</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>Œ∏ÃÇ</code> by providing the training and validation parameter sets explicitly as <code>Œ∏_train</code> and <code>Œ∏_val</code>, which are both held fixed during training.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Train.jl#L148-L153">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}" href="#NeuralEstimators.train-Union{Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}}"><code>NeuralEstimators.train</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T; &lt;keyword args&gt;)</code></pre><p>Train the neural estimator <code>Œ∏ÃÇ</code> by providing the training and validation parameter sets, <code>Œ∏_train</code> and <code>Œ∏_val</code>, and the training and validation data sets, <code>Z_train</code> and <code>Z_val</code>, all of which are held fixed during training.</p><p>The sample size argument <code>m</code> is inferred from <code>Z_val</code>. The training data <code>Z_train</code> can contain <code>M</code> replicates, where <code>M</code> is a multiple of <code>m</code>; the training data will then be recycled to imitate on-the-fly simulation. For example, if <code>M = 50</code> and <code>m = 10</code>, epoch 1 uses the first 10 replicates, epoch 2 uses the next 10 replicates, and so on, until epoch 6 again uses the first 10 replicates.</p><p>Note that the elements of <code>Z_train</code> and <code>Z_val</code> should each be equally replicated; that is, the size of the last dimension in each array in <code>Z_train</code> should be constant, and similarly for <code>Z_val</code> (although these constants can differ, as discussed above).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Train.jl#L273-L289">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}" href="#NeuralEstimators.train-Union{Tuple{I}, Tuple{P}, Tuple{T}, Tuple{Any, P, P, T, T, Vector{I}}} where {T, P&lt;:Union{ParameterConfigurations, AbstractMatrix}, I&lt;:Integer}"><code>NeuralEstimators.train</code></a> ‚Äî <span class="docstring-category">Method</span></header><section><div><pre><code class="language-julia">train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T, M; &lt;keyword args&gt;)</code></pre><p>Train several neural estimators, each with the architecture given by <code>Œ∏ÃÇ</code>, using the sample sizes given by the vector of integers, <code>M</code>.</p><p>Each neural estimator is pre-trained with the neural estimator trained for the previous sample size. That is, if <code>M = [m‚ÇÅ, m‚ÇÇ]</code>, with <code>m‚ÇÇ</code> &gt; <code>m‚ÇÅ</code>, the neural estimator for sample size <code>m‚ÇÇ</code> is pre-trainined with the (trained) neural estimator for sample size <code>m‚ÇÅ</code>. By pre-training a series of neural estimators with progressively larger sample sizes, most of the learning is done with small, computationally cheap sample sizes. Hence, this approach can be beneficial even if one is only interested in estimation for a single, large sample.</p><p>This method is a wrapper for <code>train(Œ∏ÃÇ, Œ∏_train::P, Œ∏_val::P, Z_train::T, Z_val::T)</code> and, hence, it inherits its keyword arguments. Further, certain keyword arguments can be given as vectors. For instance, if we are training two neural estimators, we can use a different number of epochs by providing <code>epochs = [e‚ÇÅ, e‚ÇÇ]</code>. Other arguments that allow vectors are <code>batchsize</code>, <code>stopping_epochs</code>, and <code>optimiser</code>.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Train.jl#L389-L409">source</a></section></article><h2 id="Assessing-a-neural-estimator"><a class="docs-heading-anchor" href="#Assessing-a-neural-estimator">Assessing a neural estimator</a><a id="Assessing-a-neural-estimator-1"></a><a class="docs-heading-anchor-permalink" href="#Assessing-a-neural-estimator" title="Permalink"></a></h2><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.assess" href="#NeuralEstimators.assess"><code>NeuralEstimators.assess</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">assess(estimators, parameters, Z; &lt;keyword args&gt;)
assess(estimators, parameters; &lt;keyword args&gt;)</code></pre><p>Using a collection of <code>estimators</code>, compute estimates from data simulated from a set of <code>parameters</code>.</p><p>One may either provide simulated data <code>Z</code> as a <code>Vector{Vector{Array}}</code>, or overload <code>simulate</code> with a method <code>simulate(parameters, m::Integer)</code> and use the keyword argument <code>m</code> to specify the sample sizes to use during assessment.</p><p><strong>Keyword arguments</strong></p><ul><li><code>m::Vector{Integer}</code>: sample sizes to estimate from.</li><li><code>estimator_names::Vector{String}</code>: names of the estimators (sensible default values provided).</li><li><code>parameter_names::Vector{String}</code>: names of the parameters (sensible default values provided).</li><li><code>J::Integer = 1</code>: the number of times to replicate each parameter in <code>parameters</code>.</li><li><code>Œæ = nothing</code>: invariant model information.</li><li><code>use_Œæ = false</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators. Specifies whether or not the estimator uses the invariant model information, <code>Œæ</code>: If it does, the estimator will be applied as <code>estimator(Z, Œæ)</code>.</li><li><code>use_gpu = true</code>: a <code>Bool</code> or a collection of <code>Bool</code> objects with length equal to the number of estimators.</li><li><code>verbose::Bool = true</code></li></ul></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Assess.jl#L58-L78">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.Assessment" href="#NeuralEstimators.Assessment"><code>NeuralEstimators.Assessment</code></a> ‚Äî <span class="docstring-category">Type</span></header><section><div><pre><code class="language-julia">Assessment(Œ∏andŒ∏ÃÇ, runtime)</code></pre><p>An object for storing the result of calling <code>assess()</code>, containing a <code>DataFrame</code> of true parameters <code>Œ∏</code> and corresponding estimates <code>Œ∏ÃÇ</code>, and a <code>DataFrame</code> containing the <code>runtime</code> for each estimator.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Assess.jl#L4-L10">source</a></section></article><h2 id="Bootstrapping"><a class="docs-heading-anchor" href="#Bootstrapping">Bootstrapping</a><a id="Bootstrapping-1"></a><a class="docs-heading-anchor-permalink" href="#Bootstrapping" title="Permalink"></a></h2><p>Note that all bootstrapping functions are currently implemented for a single parameter configuration only.</p><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.parametricbootstrap" href="#NeuralEstimators.parametricbootstrap"><code>NeuralEstimators.parametricbootstrap</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">parametricbootstrap(Œ∏ÃÇ, parameters::P, m::Integer; B::Integer = 100, use_gpu::Bool = true) where {P &lt;: ParameterConfigurations}</code></pre><p>Returns <code>B</code> parameteric bootstrap samples of an estimator <code>Œ∏ÃÇ</code> as a p √ó <code>B</code> matrix, where p is the number of parameters in the statistical model, based on data sets of size <code>m</code> simulated parameter configurations, <code>parameters</code>.</p><p>This function requires the user to have defined a method <code>simulate(parameters, m::Integer</code>).</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Bootstrap.jl#L5-L13">source</a></section></article><article class="docstring"><header><a class="docstring-binding" id="NeuralEstimators.nonparametricbootstrap" href="#NeuralEstimators.nonparametricbootstrap"><code>NeuralEstimators.nonparametricbootstrap</code></a> ‚Äî <span class="docstring-category">Function</span></header><section><div><pre><code class="language-julia">nonparametricbootstrap(Œ∏ÃÇ, Z::AbstractArray{T, N}; B::Integer = 100, use_gpu::Bool = true)
nonparametricbootstrap(Œ∏ÃÇ, Z::AbstractArray{T, N}, blocks; B::Integer = 100, use_gpu::Bool = true)</code></pre><p>Returns <code>B</code> non-parametric bootstrap samples of an estimator <code>Œ∏ÃÇ</code> as a p √ó <code>B</code> matrix, where p is the number of parameters in the statistical model.</p><p>The argument <code>blocks</code> caters for block bootstrapping, and should be an integer vector specifying the block for each replicate. For example, if we have 5 replicates with the first two replicates corresponding to block 1 and the remaining replicates corresponding to block 2, then <code>blocks</code> should be [1, 1, 2, 2, 2]. The resampling algorithm tries to produce resampled data sets of a similar size to the original data, but this can only be achieved exactly if the blocks are the same length.</p></div><a class="docs-sourcelink" target="_blank" href="https://github.com/msainsburydale/NeuralEstimators.jl/blob/a05402233efd7f10629c35b2152568fd4b7fa875/src/Bootstrap.jl#L27-L41">source</a></section></article></article><nav class="docs-footer"><a class="docs-footer-prevpage" href="../../workflow/advancedusage/">¬´ Advanced usage</a><a class="docs-footer-nextpage" href="../simulation/">Model-specific functions ¬ª</a><div class="flexbox-break"></div><p class="footer-message">Powered by <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> and the <a href="https://julialang.org/">Julia Programming Language</a>.</p></nav></div><div class="modal" id="documenter-settings"><div class="modal-background"></div><div class="modal-card"><header class="modal-card-head"><p class="modal-card-title">Settings</p><button class="delete"></button></header><section class="modal-card-body"><p><label class="label">Theme</label><div class="select"><select id="documenter-themepicker"><option value="documenter-light">documenter-light</option><option value="documenter-dark">documenter-dark</option></select></div></p><hr/><p>This document was generated with <a href="https://github.com/JuliaDocs/Documenter.jl">Documenter.jl</a> on <span class="colophon-date" title="Sunday 2 October 2022 11:36">Sunday 2 October 2022</span>. Using Julia version 1.7.1.</p></section><footer class="modal-card-foot"></footer></div></div></div></body></html>
